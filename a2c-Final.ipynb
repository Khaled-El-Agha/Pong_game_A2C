{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/a2c500amsgrad/500_A2C_cnet.pt\n",
      "/kaggle/input/a2c500amsgrad/500_A2C_pnet.pt\n",
      "/kaggle/input/a2c2options/1000_A2C_cnet.pt\n",
      "/kaggle/input/a2c2options/2.0_A2C_cnet.pt\n",
      "/kaggle/input/a2c2options/2.0_A2C_pnet.pt\n",
      "/kaggle/input/a2c2options/1000_A2C_pnet.pt\n",
      "/kaggle/input/a2camsgrad/6.0_A2C_cnet.pt\n",
      "/kaggle/input/a2camsgrad/6.0_A2C_pnet.pt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari] in /opt/conda/lib/python3.7/site-packages (0.18.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (7.2.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.19.5)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.6.0)\n",
      "Requirement already satisfied: opencv-python>=3. in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (4.5.1.48)\n",
      "Collecting atari-py~=0.2.0\n",
      "  Downloading atari_py-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.18.2)\n",
      "Installing collected packages: atari-py\n",
      "Successfully installed atari-py-0.2.6\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "##import cPickle as pickle\n",
    "from torch.distributions import Categorical\n",
    "from itertools import count\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Model\n",
    "\n",
    "we will initalize 2 Sequential networks 1 for policy and the other one for critic(value), starting with cnn layers and then 1 fully connected linear (Flatten) layer.\n",
    "\n",
    "The output of the last policy layer will be the probability of taking some action that's why the final layer size will be the action_size\n",
    "\n",
    "The output of the critic layer will be just 1 final value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_shape, action_size):\n",
    "        super(PolicyNet, self).__init__()       \n",
    "       \n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            #nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            #nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            #nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            #nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        #2304\n",
    "        #print(conv_out_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_size)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    ## Getting the final output size of the convolution\n",
    "    ##to be entered as input to the last fully connected layer \n",
    "        \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1) ## Reshape the output of the \n",
    "                                                      ##convolution to be entered \n",
    "                                                      ##to the linear (like Flatten)\n",
    "        #print(conv_out.size())\n",
    "        #print(conv_out.size(0), np.prod(conv_out.size()[1:-1]))\n",
    "        #fc_in = torch.reshape(conv_out, (conv_out.size(0), np.prod(conv_out.size()[1:])))\n",
    "        nn_out = self.fc(conv_out)\n",
    "        ##return Categorical(torch.sigmoid(nn_out))\n",
    "        \n",
    "        return Categorical(torch.softmax(nn_out, dim=1))\n",
    "    \n",
    "                \n",
    "    \n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(CriticNet, self).__init__()       \n",
    "       \n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            #nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            #nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            #nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            #nn.Flatten()\n",
    "        )\n",
    "        \n",
    "         \n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        #2304\n",
    "        #print(conv_out_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "            \n",
    "        )\n",
    "    \n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        #print(conv_out.size())\n",
    "        #print(conv_out.size(0), np.prod(conv_out.size()[1:-1]))\n",
    "        #fc_in = torch.reshape(conv_out, (conv_out.size(0), np.prod(conv_out.size()[1:])))\n",
    "        nn_out = self.fc(conv_out)\n",
    "        ##return Categorical(torch.sigmoid(nn_out))\n",
    "        \n",
    "        return nn_out\n",
    "    \n",
    "    \n",
    "   \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the frame image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(I):\n",
    "  \"\"\" preprocess 210x160x3 uint8 frame into (80x80) 2D frame image \"\"\"\n",
    "\n",
    "  if I is None:\n",
    "     return torch.zeros(80,80)\n",
    "\n",
    "  I             = I[35:195]    # crop the top and buttom of the game frame\n",
    "  I             = I[::2,::2,0] # downsample by factor of 2 (to make it 1 layer instead of RGB)\n",
    "  I[I == 144]   = 0            # erase background (background type 1)\n",
    "  I[I == 109]   = 0            # erase background (background type 2)\n",
    "  I[I != 0  ]   = 1            # everything else (paddles, ball) just set to 1 \n",
    "                               #(so at the end the frame will be just zeros and ones)\n",
    "  ##return torch.from_numpy(I.astype(np.float32).ravel()).unsqueeze(0) \n",
    "  ## Flatten the image if we will use fully connected network instead of cnn \n",
    "  ## it was one of the trials when doing and learning policy gradient code and using \n",
    "  ## fully connected network instead of cnn\n",
    "    \n",
    "  return I ## Return the image without flattening (80,80) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function that takes a list of rewards and reutrn the list of returns for each step\n",
    "def discounted_returns(rewards, gamma=0.9):\n",
    "    ## Init R\n",
    "    R = 0\n",
    "    returns = list()\n",
    "    for reward in reversed(rewards):\n",
    "        if reward != 0: R = 0   # scored/lost a point in pong, so reset reward sum\n",
    "                                # This will encourge all the actions which are before the +1\n",
    "                                # and discourge all the actions which lead to -1 also \n",
    "                                # and we doing it reverse to know the action which lead to win or lose\n",
    "                                # because most of rewards are zeros in pong so we need which lead to win and which lead to lose\n",
    "        R = reward + gamma * R\n",
    "        #print(R)\n",
    "        returns.insert(0, R)\n",
    "        #returns.append(R)\n",
    "\n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    ## normalize the returns\n",
    "    ## As I read, normalizing the discounted rewards give better\n",
    "    ## results and smooth ones when doing back propagations\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-6) ## adding this term 1e-6 to prevent the den. from being zero\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing GPU or CPU and naming our saved file for the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = ''\n",
    "MODEL_NAME1 = 'A2C_pnet.pt'\n",
    "MODEL_NAME2 = 'A2C_cnet.pt'\n",
    "device = 'cuda'\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studying the environment\n",
    "\n",
    "We noticed that there is actually 6 actions for pong game but the real effective ones are 2 and 3 as 4 and 5 are just repeating the same thing as 2 and 3.\n",
    "\n",
    "Noop will do nothing and fire it is speacial for some kind of pong which you have to fire to start the game or something like that as I read from the internet\n",
    "\n",
    "So at the end of the day we just had 2 real actions for the pong game (UP,DOWN), so why just putting the sex actions as an output size to the network ?!\n",
    "\n",
    "This will lead to more computation as when we sample an action from the action space we could sample 0 or 1 which has no effect on the game also for 4 and 5 they have the same effect of 2 and 3. so this will lead to more training time\n",
    "\n",
    "I decided to choose just 2 output as the action size after undestanding the environment and I also did that after seeing the code of andrew with numpy code he just used 2 actions which are up and down and calculate the probability of them\n",
    "\n",
    "I was working by 6 first in previous versions of code but then I had this idea and it really decrease the training time a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# Here is the number of actions and the actions meaning of the environment \n",
    "env = gym.make('Pong-v0')\n",
    "\n",
    "print(env.action_space.n)\n",
    "\n",
    "print(env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initalizing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "CriticNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#initalizing the environment, network, optimizer, and some useful parameters\n",
    "\n",
    "## env = gym.make('PongNoFrameskip-v4')\n",
    "env = gym.make('Pong-v0')\n",
    "\n",
    "##input_shape = env.observation_space.shape[0]\n",
    "#action_size = env.action_space.n\n",
    "\n",
    "action_size = 2 ## As we explain above because we have only 2 real actions Up, Down\n",
    "\n",
    "input_shape = [1,80,80] # just one channel input for the convolution\n",
    "\n",
    "## initialize the net\n",
    "pnet = PolicyNet(input_shape, action_size).to(device) \n",
    "cnet = CriticNet(input_shape).to(device)\n",
    "# to device mean here to the chosen\n",
    "# device which in this case the cuda or GPU\n",
    "\n",
    "\n",
    "## initialize an optimizer\n",
    "p_optimizer = torch.optim.Adam(pnet.parameters(), lr=1e-4,eps=1e-3)\n",
    "c_optimizer = torch.optim.Adam(cnet.parameters(), lr=1e-4,eps=1e-3)\n",
    "\n",
    "## this parameter eps=1e-3 by default will be 1e-8 but we change it because \n",
    "#this parameter is in the den. and when it will be very small , \n",
    "#it will make very big weights and the training will not converge, \n",
    "#that's why we have to increase it a littile bit as also recommended from maxlapan book \n",
    "\n",
    "print(pnet)\n",
    "\n",
    "print(cnet)\n",
    "\n",
    "running_reward  = None # starting with zero runnung reward\n",
    "MEAN_REWARD_BOUND = 20 # we will try to reach this mean :D :D \n",
    "\n",
    "UP_ACTION = 2 # up action is number 2 in the action space as shown above in the explanation of the environment\n",
    "\n",
    "DOWN_ACTION = 3 # down action is number 3 in the action space as shown above in the explanation of the environment\n",
    "\n",
    "Max_ep_rewared = -21 # to track the maximum reward towards the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tLast reward: -19.00\tAverage reward: -19.00\n",
      "Episode 2\tLast reward: -21.00\tAverage reward: -19.10\n",
      "Episode 3\tLast reward: -20.00\tAverage reward: -19.14\n",
      "Episode 4\tLast reward: -21.00\tAverage reward: -19.24\n",
      "Episode 5\tLast reward: -21.00\tAverage reward: -19.33\n",
      "Episode 6\tLast reward: -21.00\tAverage reward: -19.41\n",
      "Episode 7\tLast reward: -21.00\tAverage reward: -19.49\n",
      "Episode 8\tLast reward: -21.00\tAverage reward: -19.56\n",
      "Episode 9\tLast reward: -20.00\tAverage reward: -19.59\n",
      "Episode 10\tLast reward: -21.00\tAverage reward: -19.66\n",
      "Episode 11\tLast reward: -21.00\tAverage reward: -19.72\n",
      "Episode 12\tLast reward: -21.00\tAverage reward: -19.79\n",
      "Episode 13\tLast reward: -20.00\tAverage reward: -19.80\n",
      "Episode 14\tLast reward: -20.00\tAverage reward: -19.81\n",
      "Episode 15\tLast reward: -21.00\tAverage reward: -19.87\n",
      "Episode 16\tLast reward: -21.00\tAverage reward: -19.92\n",
      "Episode 17\tLast reward: -21.00\tAverage reward: -19.98\n",
      "Episode 18\tLast reward: -20.00\tAverage reward: -19.98\n",
      "Episode 19\tLast reward: -19.00\tAverage reward: -19.93\n",
      "Episode 20\tLast reward: -21.00\tAverage reward: -19.98\n",
      "Episode 21\tLast reward: -19.00\tAverage reward: -19.93\n",
      "Episode 22\tLast reward: -20.00\tAverage reward: -19.94\n",
      "Episode 23\tLast reward: -21.00\tAverage reward: -19.99\n",
      "Episode 24\tLast reward: -19.00\tAverage reward: -19.94\n",
      "Episode 25\tLast reward: -21.00\tAverage reward: -19.99\n",
      "Episode 26\tLast reward: -21.00\tAverage reward: -20.04\n",
      "Episode 27\tLast reward: -19.00\tAverage reward: -19.99\n",
      "Episode 28\tLast reward: -20.00\tAverage reward: -19.99\n",
      "Episode 29\tLast reward: -20.00\tAverage reward: -19.99\n",
      "Episode 30\tLast reward: -21.00\tAverage reward: -20.04\n",
      "Episode 31\tLast reward: -20.00\tAverage reward: -20.04\n",
      "Episode 32\tLast reward: -21.00\tAverage reward: -20.09\n",
      "Episode 33\tLast reward: -21.00\tAverage reward: -20.13\n",
      "Episode 34\tLast reward: -20.00\tAverage reward: -20.13\n",
      "Episode 35\tLast reward: -19.00\tAverage reward: -20.07\n",
      "Episode 36\tLast reward: -20.00\tAverage reward: -20.07\n",
      "Episode 37\tLast reward: -20.00\tAverage reward: -20.06\n",
      "Episode 38\tLast reward: -21.00\tAverage reward: -20.11\n",
      "Episode 39\tLast reward: -21.00\tAverage reward: -20.16\n",
      "Episode 40\tLast reward: -21.00\tAverage reward: -20.20\n",
      "Episode 41\tLast reward: -21.00\tAverage reward: -20.24\n",
      "Episode 42\tLast reward: -18.00\tAverage reward: -20.13\n",
      "Episode 43\tLast reward: -21.00\tAverage reward: -20.17\n",
      "Episode 44\tLast reward: -19.00\tAverage reward: -20.11\n",
      "Episode 45\tLast reward: -20.00\tAverage reward: -20.11\n",
      "Episode 46\tLast reward: -18.00\tAverage reward: -20.00\n",
      "Episode 47\tLast reward: -20.00\tAverage reward: -20.00\n",
      "Episode 48\tLast reward: -20.00\tAverage reward: -20.00\n",
      "Episode 49\tLast reward: -19.00\tAverage reward: -19.95\n",
      "Episode 50\tLast reward: -19.00\tAverage reward: -19.90\n",
      "Episode 51\tLast reward: -21.00\tAverage reward: -19.96\n",
      "Episode 52\tLast reward: -21.00\tAverage reward: -20.01\n",
      "Episode 53\tLast reward: -18.00\tAverage reward: -19.91\n",
      "Episode 54\tLast reward: -20.00\tAverage reward: -19.91\n",
      "Episode 55\tLast reward: -21.00\tAverage reward: -19.97\n",
      "Episode 56\tLast reward: -21.00\tAverage reward: -20.02\n",
      "Episode 57\tLast reward: -19.00\tAverage reward: -19.97\n",
      "Episode 58\tLast reward: -20.00\tAverage reward: -19.97\n",
      "Episode 59\tLast reward: -20.00\tAverage reward: -19.97\n",
      "Episode 60\tLast reward: -19.00\tAverage reward: -19.92\n",
      "Episode 61\tLast reward: -18.00\tAverage reward: -19.83\n",
      "Episode 62\tLast reward: -20.00\tAverage reward: -19.84\n",
      "Episode 63\tLast reward: -21.00\tAverage reward: -19.89\n",
      "Episode 64\tLast reward: -19.00\tAverage reward: -19.85\n",
      "Episode 65\tLast reward: -21.00\tAverage reward: -19.91\n",
      "Episode 66\tLast reward: -18.00\tAverage reward: -19.81\n",
      "Episode 67\tLast reward: -19.00\tAverage reward: -19.77\n",
      "Episode 68\tLast reward: -18.00\tAverage reward: -19.68\n",
      "Episode 69\tLast reward: -19.00\tAverage reward: -19.65\n",
      "Episode 70\tLast reward: -20.00\tAverage reward: -19.67\n",
      "Episode 71\tLast reward: -19.00\tAverage reward: -19.63\n",
      "Episode 72\tLast reward: -21.00\tAverage reward: -19.70\n",
      "Episode 73\tLast reward: -20.00\tAverage reward: -19.72\n",
      "Episode 74\tLast reward: -21.00\tAverage reward: -19.78\n",
      "Episode 75\tLast reward: -20.00\tAverage reward: -19.79\n",
      "Episode 76\tLast reward: -19.00\tAverage reward: -19.75\n",
      "Episode 77\tLast reward: -20.00\tAverage reward: -19.76\n",
      "Episode 78\tLast reward: -19.00\tAverage reward: -19.73\n",
      "Episode 79\tLast reward: -18.00\tAverage reward: -19.64\n",
      "Episode 80\tLast reward: -20.00\tAverage reward: -19.66\n",
      "Episode 81\tLast reward: -21.00\tAverage reward: -19.72\n",
      "Episode 82\tLast reward: -21.00\tAverage reward: -19.79\n",
      "Episode 83\tLast reward: -20.00\tAverage reward: -19.80\n",
      "Episode 84\tLast reward: -15.00\tAverage reward: -19.56\n",
      "Episode 85\tLast reward: -19.00\tAverage reward: -19.53\n",
      "Episode 86\tLast reward: -21.00\tAverage reward: -19.60\n",
      "Episode 87\tLast reward: -21.00\tAverage reward: -19.67\n",
      "Episode 88\tLast reward: -21.00\tAverage reward: -19.74\n",
      "Episode 89\tLast reward: -21.00\tAverage reward: -19.80\n",
      "Episode 90\tLast reward: -21.00\tAverage reward: -19.86\n",
      "Episode 91\tLast reward: -21.00\tAverage reward: -19.92\n",
      "Episode 92\tLast reward: -20.00\tAverage reward: -19.92\n",
      "Episode 93\tLast reward: -20.00\tAverage reward: -19.93\n",
      "Episode 94\tLast reward: -17.00\tAverage reward: -19.78\n",
      "Episode 95\tLast reward: -19.00\tAverage reward: -19.74\n",
      "Episode 96\tLast reward: -21.00\tAverage reward: -19.81\n",
      "Episode 97\tLast reward: -21.00\tAverage reward: -19.87\n",
      "Episode 98\tLast reward: -19.00\tAverage reward: -19.82\n",
      "Episode 99\tLast reward: -21.00\tAverage reward: -19.88\n",
      "Episode 100\tLast reward: -21.00\tAverage reward: -19.94\n",
      "Episode 101\tLast reward: -19.00\tAverage reward: -19.89\n",
      "Episode 102\tLast reward: -20.00\tAverage reward: -19.90\n",
      "Episode 103\tLast reward: -21.00\tAverage reward: -19.95\n",
      "Episode 104\tLast reward: -21.00\tAverage reward: -20.00\n",
      "Episode 105\tLast reward: -21.00\tAverage reward: -20.05\n",
      "Episode 106\tLast reward: -21.00\tAverage reward: -20.10\n",
      "Episode 107\tLast reward: -19.00\tAverage reward: -20.05\n",
      "Episode 108\tLast reward: -19.00\tAverage reward: -19.99\n",
      "Episode 109\tLast reward: -19.00\tAverage reward: -19.94\n",
      "Episode 110\tLast reward: -19.00\tAverage reward: -19.90\n",
      "Episode 111\tLast reward: -20.00\tAverage reward: -19.90\n",
      "Episode 112\tLast reward: -17.00\tAverage reward: -19.76\n",
      "Episode 113\tLast reward: -17.00\tAverage reward: -19.62\n",
      "Episode 114\tLast reward: -20.00\tAverage reward: -19.64\n",
      "Episode 115\tLast reward: -21.00\tAverage reward: -19.71\n",
      "Episode 116\tLast reward: -18.00\tAverage reward: -19.62\n",
      "Episode 117\tLast reward: -21.00\tAverage reward: -19.69\n",
      "Episode 118\tLast reward: -19.00\tAverage reward: -19.65\n",
      "Episode 119\tLast reward: -20.00\tAverage reward: -19.67\n",
      "Episode 120\tLast reward: -19.00\tAverage reward: -19.64\n",
      "Episode 121\tLast reward: -19.00\tAverage reward: -19.61\n",
      "Episode 122\tLast reward: -21.00\tAverage reward: -19.68\n",
      "Episode 123\tLast reward: -21.00\tAverage reward: -19.74\n",
      "Episode 124\tLast reward: -19.00\tAverage reward: -19.71\n",
      "Episode 125\tLast reward: -21.00\tAverage reward: -19.77\n",
      "Episode 126\tLast reward: -19.00\tAverage reward: -19.73\n",
      "Episode 127\tLast reward: -21.00\tAverage reward: -19.80\n",
      "Episode 128\tLast reward: -17.00\tAverage reward: -19.66\n",
      "Episode 129\tLast reward: -19.00\tAverage reward: -19.62\n",
      "Episode 130\tLast reward: -19.00\tAverage reward: -19.59\n",
      "Episode 131\tLast reward: -19.00\tAverage reward: -19.56\n",
      "Episode 132\tLast reward: -21.00\tAverage reward: -19.63\n",
      "Episode 133\tLast reward: -21.00\tAverage reward: -19.70\n",
      "Episode 134\tLast reward: -21.00\tAverage reward: -19.77\n",
      "Episode 135\tLast reward: -19.00\tAverage reward: -19.73\n",
      "Episode 136\tLast reward: -19.00\tAverage reward: -19.69\n",
      "Episode 137\tLast reward: -21.00\tAverage reward: -19.76\n",
      "Episode 138\tLast reward: -21.00\tAverage reward: -19.82\n",
      "Episode 139\tLast reward: -20.00\tAverage reward: -19.83\n",
      "Episode 140\tLast reward: -21.00\tAverage reward: -19.89\n",
      "Episode 141\tLast reward: -20.00\tAverage reward: -19.89\n",
      "Episode 142\tLast reward: -19.00\tAverage reward: -19.85\n",
      "Episode 143\tLast reward: -18.00\tAverage reward: -19.76\n",
      "Episode 144\tLast reward: -21.00\tAverage reward: -19.82\n",
      "Episode 145\tLast reward: -19.00\tAverage reward: -19.78\n",
      "Episode 146\tLast reward: -19.00\tAverage reward: -19.74\n",
      "Episode 147\tLast reward: -17.00\tAverage reward: -19.60\n",
      "Episode 148\tLast reward: -21.00\tAverage reward: -19.67\n",
      "Episode 149\tLast reward: -21.00\tAverage reward: -19.74\n",
      "Episode 150\tLast reward: -21.00\tAverage reward: -19.80\n",
      "Episode 151\tLast reward: -21.00\tAverage reward: -19.86\n",
      "Episode 152\tLast reward: -13.00\tAverage reward: -19.52\n",
      "Episode 153\tLast reward: -21.00\tAverage reward: -19.59\n",
      "Episode 154\tLast reward: -17.00\tAverage reward: -19.46\n",
      "Episode 155\tLast reward: -19.00\tAverage reward: -19.44\n",
      "Episode 156\tLast reward: -20.00\tAverage reward: -19.47\n",
      "Episode 157\tLast reward: -19.00\tAverage reward: -19.44\n",
      "Episode 158\tLast reward: -21.00\tAverage reward: -19.52\n",
      "Episode 159\tLast reward: -17.00\tAverage reward: -19.40\n",
      "Episode 160\tLast reward: -21.00\tAverage reward: -19.48\n",
      "Episode 161\tLast reward: -19.00\tAverage reward: -19.45\n",
      "Episode 162\tLast reward: -15.00\tAverage reward: -19.23\n",
      "Episode 163\tLast reward: -21.00\tAverage reward: -19.32\n",
      "Episode 164\tLast reward: -21.00\tAverage reward: -19.40\n",
      "Episode 165\tLast reward: -21.00\tAverage reward: -19.48\n",
      "Episode 166\tLast reward: -19.00\tAverage reward: -19.46\n",
      "Episode 167\tLast reward: -17.00\tAverage reward: -19.33\n",
      "Episode 168\tLast reward: -21.00\tAverage reward: -19.42\n",
      "Episode 169\tLast reward: -19.00\tAverage reward: -19.40\n",
      "Episode 170\tLast reward: -18.00\tAverage reward: -19.33\n",
      "Episode 171\tLast reward: -19.00\tAverage reward: -19.31\n",
      "Episode 172\tLast reward: -21.00\tAverage reward: -19.40\n",
      "Episode 173\tLast reward: -19.00\tAverage reward: -19.38\n",
      "Episode 174\tLast reward: -21.00\tAverage reward: -19.46\n",
      "Episode 175\tLast reward: -19.00\tAverage reward: -19.43\n",
      "Episode 176\tLast reward: -19.00\tAverage reward: -19.41\n",
      "Episode 177\tLast reward: -13.00\tAverage reward: -19.09\n",
      "Episode 178\tLast reward: -19.00\tAverage reward: -19.09\n",
      "Episode 179\tLast reward: -19.00\tAverage reward: -19.08\n",
      "Episode 180\tLast reward: -16.00\tAverage reward: -18.93\n",
      "Episode 181\tLast reward: -15.00\tAverage reward: -18.73\n",
      "Episode 182\tLast reward: -19.00\tAverage reward: -18.75\n",
      "Episode 183\tLast reward: -19.00\tAverage reward: -18.76\n",
      "Episode 184\tLast reward: -19.00\tAverage reward: -18.77\n",
      "Episode 185\tLast reward: -17.00\tAverage reward: -18.68\n",
      "Episode 186\tLast reward: -21.00\tAverage reward: -18.80\n",
      "Episode 187\tLast reward: -21.00\tAverage reward: -18.91\n",
      "Episode 188\tLast reward: -17.00\tAverage reward: -18.81\n",
      "Episode 189\tLast reward: -17.00\tAverage reward: -18.72\n",
      "Episode 190\tLast reward: -17.00\tAverage reward: -18.64\n",
      "Episode 191\tLast reward: -19.00\tAverage reward: -18.65\n",
      "Episode 192\tLast reward: -19.00\tAverage reward: -18.67\n",
      "Episode 193\tLast reward: -18.00\tAverage reward: -18.64\n",
      "Episode 194\tLast reward: -17.00\tAverage reward: -18.56\n",
      "Episode 195\tLast reward: -17.00\tAverage reward: -18.48\n",
      "Episode 196\tLast reward: -19.00\tAverage reward: -18.50\n",
      "Episode 197\tLast reward: -21.00\tAverage reward: -18.63\n",
      "Episode 198\tLast reward: -17.00\tAverage reward: -18.55\n",
      "Episode 199\tLast reward: -16.00\tAverage reward: -18.42\n",
      "Episode 200\tLast reward: -21.00\tAverage reward: -18.55\n",
      "Episode 201\tLast reward: -17.00\tAverage reward: -18.47\n",
      "Episode 202\tLast reward: -18.00\tAverage reward: -18.45\n",
      "Episode 203\tLast reward: -14.00\tAverage reward: -18.23\n",
      "Episode 204\tLast reward: -21.00\tAverage reward: -18.36\n",
      "Episode 205\tLast reward: -19.00\tAverage reward: -18.40\n",
      "Episode 206\tLast reward: -12.00\tAverage reward: -18.08\n",
      "Episode 207\tLast reward: -14.00\tAverage reward: -17.87\n",
      "Episode 208\tLast reward: -19.00\tAverage reward: -17.93\n",
      "Episode 209\tLast reward: -17.00\tAverage reward: -17.88\n",
      "Episode 210\tLast reward: -16.00\tAverage reward: -17.79\n",
      "Episode 211\tLast reward: -19.00\tAverage reward: -17.85\n",
      "Episode 212\tLast reward: -19.00\tAverage reward: -17.91\n",
      "Episode 213\tLast reward: -17.00\tAverage reward: -17.86\n",
      "Episode 214\tLast reward: -21.00\tAverage reward: -18.02\n",
      "Episode 215\tLast reward: -19.00\tAverage reward: -18.07\n",
      "Episode 216\tLast reward: -21.00\tAverage reward: -18.21\n",
      "Episode 217\tLast reward: -21.00\tAverage reward: -18.35\n",
      "Episode 218\tLast reward: -15.00\tAverage reward: -18.19\n",
      "Episode 219\tLast reward: -19.00\tAverage reward: -18.23\n",
      "Episode 220\tLast reward: -21.00\tAverage reward: -18.36\n",
      "Episode 221\tLast reward: -17.00\tAverage reward: -18.30\n",
      "Episode 222\tLast reward: -17.00\tAverage reward: -18.23\n",
      "Episode 223\tLast reward: -21.00\tAverage reward: -18.37\n",
      "Episode 224\tLast reward: -21.00\tAverage reward: -18.50\n",
      "Episode 225\tLast reward: -19.00\tAverage reward: -18.53\n",
      "Episode 226\tLast reward: -19.00\tAverage reward: -18.55\n",
      "Episode 227\tLast reward: -19.00\tAverage reward: -18.57\n",
      "Episode 228\tLast reward: -20.00\tAverage reward: -18.64\n",
      "Episode 229\tLast reward: -21.00\tAverage reward: -18.76\n",
      "Episode 230\tLast reward: -21.00\tAverage reward: -18.87\n",
      "Episode 231\tLast reward: -18.00\tAverage reward: -18.83\n",
      "Episode 232\tLast reward: -16.00\tAverage reward: -18.69\n",
      "Episode 233\tLast reward: -19.00\tAverage reward: -18.70\n",
      "Episode 234\tLast reward: -18.00\tAverage reward: -18.67\n",
      "Episode 235\tLast reward: -19.00\tAverage reward: -18.69\n",
      "Episode 236\tLast reward: -21.00\tAverage reward: -18.80\n",
      "Episode 237\tLast reward: -21.00\tAverage reward: -18.91\n",
      "Episode 238\tLast reward: -15.00\tAverage reward: -18.72\n",
      "Episode 239\tLast reward: -17.00\tAverage reward: -18.63\n",
      "Episode 240\tLast reward: -15.00\tAverage reward: -18.45\n",
      "Episode 241\tLast reward: -19.00\tAverage reward: -18.48\n",
      "Episode 242\tLast reward: -19.00\tAverage reward: -18.50\n",
      "Episode 243\tLast reward: -21.00\tAverage reward: -18.63\n",
      "Episode 244\tLast reward: -17.00\tAverage reward: -18.55\n",
      "Episode 245\tLast reward: -18.00\tAverage reward: -18.52\n",
      "Episode 246\tLast reward: -19.00\tAverage reward: -18.54\n",
      "Episode 247\tLast reward: -19.00\tAverage reward: -18.57\n",
      "Episode 248\tLast reward: -17.00\tAverage reward: -18.49\n",
      "Episode 249\tLast reward: -16.00\tAverage reward: -18.36\n",
      "Episode 250\tLast reward: -14.00\tAverage reward: -18.14\n",
      "Episode 251\tLast reward: -20.00\tAverage reward: -18.24\n",
      "Episode 252\tLast reward: -15.00\tAverage reward: -18.08\n",
      "Episode 253\tLast reward: -16.00\tAverage reward: -17.97\n",
      "Episode 254\tLast reward: -18.00\tAverage reward: -17.97\n",
      "Episode 255\tLast reward: -15.00\tAverage reward: -17.82\n",
      "Episode 256\tLast reward: -17.00\tAverage reward: -17.78\n",
      "Episode 257\tLast reward: -19.00\tAverage reward: -17.84\n",
      "Episode 258\tLast reward: -19.00\tAverage reward: -17.90\n",
      "Episode 259\tLast reward: -19.00\tAverage reward: -17.96\n",
      "Episode 260\tLast reward: -21.00\tAverage reward: -18.11\n",
      "Episode 261\tLast reward: -15.00\tAverage reward: -17.95\n",
      "Episode 262\tLast reward: -21.00\tAverage reward: -18.11\n",
      "Episode 263\tLast reward: -21.00\tAverage reward: -18.25\n",
      "Episode 264\tLast reward: -19.00\tAverage reward: -18.29\n",
      "Episode 265\tLast reward: -16.00\tAverage reward: -18.17\n",
      "Episode 266\tLast reward: -19.00\tAverage reward: -18.21\n",
      "Episode 267\tLast reward: -17.00\tAverage reward: -18.15\n",
      "Episode 268\tLast reward: -17.00\tAverage reward: -18.10\n",
      "Episode 269\tLast reward: -17.00\tAverage reward: -18.04\n",
      "Episode 270\tLast reward: -21.00\tAverage reward: -18.19\n",
      "Episode 271\tLast reward: -19.00\tAverage reward: -18.23\n",
      "Episode 272\tLast reward: -21.00\tAverage reward: -18.37\n",
      "Episode 273\tLast reward: -12.00\tAverage reward: -18.05\n",
      "Episode 274\tLast reward: -17.00\tAverage reward: -18.00\n",
      "Episode 275\tLast reward: -21.00\tAverage reward: -18.15\n",
      "Episode 276\tLast reward: -19.00\tAverage reward: -18.19\n",
      "Episode 277\tLast reward: -17.00\tAverage reward: -18.13\n",
      "Episode 278\tLast reward: -18.00\tAverage reward: -18.12\n",
      "Episode 279\tLast reward: -17.00\tAverage reward: -18.07\n",
      "Episode 280\tLast reward: -17.00\tAverage reward: -18.01\n",
      "Episode 281\tLast reward: -21.00\tAverage reward: -18.16\n",
      "Episode 282\tLast reward: -21.00\tAverage reward: -18.31\n",
      "Episode 283\tLast reward: -21.00\tAverage reward: -18.44\n",
      "Episode 284\tLast reward: -16.00\tAverage reward: -18.32\n",
      "Episode 285\tLast reward: -19.00\tAverage reward: -18.35\n",
      "Episode 286\tLast reward: -18.00\tAverage reward: -18.33\n",
      "Episode 287\tLast reward: -17.00\tAverage reward: -18.27\n",
      "Episode 288\tLast reward: -19.00\tAverage reward: -18.30\n",
      "Episode 289\tLast reward: -21.00\tAverage reward: -18.44\n",
      "Episode 290\tLast reward: -19.00\tAverage reward: -18.47\n",
      "Episode 291\tLast reward: -18.00\tAverage reward: -18.44\n",
      "Episode 292\tLast reward: -19.00\tAverage reward: -18.47\n",
      "Episode 293\tLast reward: -17.00\tAverage reward: -18.40\n",
      "Episode 294\tLast reward: -14.00\tAverage reward: -18.18\n",
      "Episode 295\tLast reward: -19.00\tAverage reward: -18.22\n",
      "Episode 296\tLast reward: -19.00\tAverage reward: -18.26\n",
      "Episode 297\tLast reward: -16.00\tAverage reward: -18.15\n",
      "Episode 298\tLast reward: -21.00\tAverage reward: -18.29\n",
      "Episode 299\tLast reward: -17.00\tAverage reward: -18.22\n",
      "Episode 300\tLast reward: -18.00\tAverage reward: -18.21\n",
      "Episode 301\tLast reward: -17.00\tAverage reward: -18.15\n",
      "Episode 302\tLast reward: -15.00\tAverage reward: -17.99\n",
      "Episode 303\tLast reward: -13.00\tAverage reward: -17.74\n",
      "Episode 304\tLast reward: -21.00\tAverage reward: -17.91\n",
      "Episode 305\tLast reward: -19.00\tAverage reward: -17.96\n",
      "Episode 306\tLast reward: -13.00\tAverage reward: -17.71\n",
      "Episode 307\tLast reward: -17.00\tAverage reward: -17.68\n",
      "Episode 308\tLast reward: -15.00\tAverage reward: -17.54\n",
      "Episode 309\tLast reward: -16.00\tAverage reward: -17.47\n",
      "Episode 310\tLast reward: -15.00\tAverage reward: -17.34\n",
      "Episode 311\tLast reward: -18.00\tAverage reward: -17.38\n",
      "Episode 312\tLast reward: -19.00\tAverage reward: -17.46\n",
      "Episode 313\tLast reward: -19.00\tAverage reward: -17.53\n",
      "Episode 314\tLast reward: -19.00\tAverage reward: -17.61\n",
      "Episode 315\tLast reward: -17.00\tAverage reward: -17.58\n",
      "Episode 316\tLast reward: -15.00\tAverage reward: -17.45\n",
      "Episode 317\tLast reward: -19.00\tAverage reward: -17.53\n",
      "Episode 318\tLast reward: -17.00\tAverage reward: -17.50\n",
      "Episode 319\tLast reward: -16.00\tAverage reward: -17.43\n",
      "Episode 320\tLast reward: -17.00\tAverage reward: -17.40\n",
      "Episode 321\tLast reward: -19.00\tAverage reward: -17.48\n",
      "Episode 322\tLast reward: -19.00\tAverage reward: -17.56\n",
      "Episode 323\tLast reward: -18.00\tAverage reward: -17.58\n",
      "Episode 324\tLast reward: -20.00\tAverage reward: -17.70\n",
      "Episode 325\tLast reward: -19.00\tAverage reward: -17.77\n",
      "Episode 326\tLast reward: -17.00\tAverage reward: -17.73\n",
      "Episode 327\tLast reward: -18.00\tAverage reward: -17.74\n",
      "Episode 328\tLast reward: -15.00\tAverage reward: -17.61\n",
      "Episode 329\tLast reward: -19.00\tAverage reward: -17.68\n",
      "Episode 330\tLast reward: -19.00\tAverage reward: -17.74\n",
      "Episode 331\tLast reward: -15.00\tAverage reward: -17.60\n",
      "Episode 332\tLast reward: -15.00\tAverage reward: -17.47\n",
      "Episode 333\tLast reward: -17.00\tAverage reward: -17.45\n",
      "Episode 334\tLast reward: -15.00\tAverage reward: -17.33\n",
      "Episode 335\tLast reward: -19.00\tAverage reward: -17.41\n",
      "Episode 336\tLast reward: -17.00\tAverage reward: -17.39\n",
      "Episode 337\tLast reward: -17.00\tAverage reward: -17.37\n",
      "Episode 338\tLast reward: -19.00\tAverage reward: -17.45\n",
      "Episode 339\tLast reward: -17.00\tAverage reward: -17.43\n",
      "Episode 340\tLast reward: -21.00\tAverage reward: -17.61\n",
      "Episode 341\tLast reward: -13.00\tAverage reward: -17.38\n",
      "Episode 342\tLast reward: -17.00\tAverage reward: -17.36\n",
      "Episode 343\tLast reward: -17.00\tAverage reward: -17.34\n",
      "Episode 344\tLast reward: -17.00\tAverage reward: -17.32\n",
      "Episode 345\tLast reward: -14.00\tAverage reward: -17.16\n",
      "Episode 346\tLast reward: -17.00\tAverage reward: -17.15\n",
      "Episode 347\tLast reward: -19.00\tAverage reward: -17.24\n",
      "Episode 348\tLast reward: -15.00\tAverage reward: -17.13\n",
      "Episode 349\tLast reward: -19.00\tAverage reward: -17.22\n",
      "Episode 350\tLast reward: -17.00\tAverage reward: -17.21\n",
      "Episode 351\tLast reward: -11.00\tAverage reward: -16.90\n",
      "Episode 352\tLast reward: -17.00\tAverage reward: -16.91\n",
      "Episode 353\tLast reward: -19.00\tAverage reward: -17.01\n",
      "Episode 354\tLast reward: -19.00\tAverage reward: -17.11\n",
      "Episode 355\tLast reward: -21.00\tAverage reward: -17.31\n",
      "Episode 356\tLast reward: -11.00\tAverage reward: -16.99\n",
      "Episode 357\tLast reward: -14.00\tAverage reward: -16.84\n",
      "Episode 358\tLast reward: -19.00\tAverage reward: -16.95\n",
      "Episode 359\tLast reward: -15.00\tAverage reward: -16.85\n",
      "Episode 360\tLast reward: -18.00\tAverage reward: -16.91\n",
      "Episode 361\tLast reward: -18.00\tAverage reward: -16.96\n",
      "Episode 362\tLast reward: -17.00\tAverage reward: -16.97\n",
      "Episode 363\tLast reward: -18.00\tAverage reward: -17.02\n",
      "Episode 364\tLast reward: -13.00\tAverage reward: -16.82\n",
      "Episode 365\tLast reward: -20.00\tAverage reward: -16.98\n",
      "Episode 366\tLast reward: -18.00\tAverage reward: -17.03\n",
      "Episode 367\tLast reward: -17.00\tAverage reward: -17.03\n",
      "Episode 368\tLast reward: -16.00\tAverage reward: -16.97\n",
      "Episode 369\tLast reward: -21.00\tAverage reward: -17.18\n",
      "Episode 370\tLast reward: -14.00\tAverage reward: -17.02\n",
      "Episode 371\tLast reward: -17.00\tAverage reward: -17.02\n",
      "Episode 372\tLast reward: -16.00\tAverage reward: -16.96\n",
      "Episode 373\tLast reward: -19.00\tAverage reward: -17.07\n",
      "Episode 374\tLast reward: -19.00\tAverage reward: -17.16\n",
      "Episode 375\tLast reward: -13.00\tAverage reward: -16.96\n",
      "Episode 376\tLast reward: -18.00\tAverage reward: -17.01\n",
      "Episode 377\tLast reward: -21.00\tAverage reward: -17.21\n",
      "Episode 378\tLast reward: -17.00\tAverage reward: -17.20\n",
      "Episode 379\tLast reward: -19.00\tAverage reward: -17.29\n",
      "Episode 380\tLast reward: -15.00\tAverage reward: -17.17\n",
      "Episode 381\tLast reward: -18.00\tAverage reward: -17.21\n",
      "Episode 382\tLast reward: -17.00\tAverage reward: -17.20\n",
      "Episode 383\tLast reward: -19.00\tAverage reward: -17.29\n",
      "Episode 384\tLast reward: -18.00\tAverage reward: -17.33\n",
      "Episode 385\tLast reward: -19.00\tAverage reward: -17.41\n",
      "Episode 386\tLast reward: -18.00\tAverage reward: -17.44\n",
      "Episode 387\tLast reward: -15.00\tAverage reward: -17.32\n",
      "Episode 388\tLast reward: -19.00\tAverage reward: -17.40\n",
      "Episode 389\tLast reward: -15.00\tAverage reward: -17.28\n",
      "Episode 390\tLast reward: -17.00\tAverage reward: -17.27\n",
      "Episode 391\tLast reward: -17.00\tAverage reward: -17.26\n",
      "Episode 392\tLast reward: -12.00\tAverage reward: -16.99\n",
      "Episode 393\tLast reward: -18.00\tAverage reward: -17.04\n",
      "Episode 394\tLast reward: -15.00\tAverage reward: -16.94\n",
      "Episode 395\tLast reward: -13.00\tAverage reward: -16.74\n",
      "Episode 396\tLast reward: -15.00\tAverage reward: -16.66\n",
      "Episode 397\tLast reward: -19.00\tAverage reward: -16.77\n",
      "Episode 398\tLast reward: -15.00\tAverage reward: -16.69\n",
      "Episode 399\tLast reward: -17.00\tAverage reward: -16.70\n",
      "Episode 400\tLast reward: -17.00\tAverage reward: -16.72\n",
      "Episode 401\tLast reward: -13.00\tAverage reward: -16.53\n",
      "Episode 402\tLast reward: -13.00\tAverage reward: -16.35\n",
      "Episode 403\tLast reward: -15.00\tAverage reward: -16.29\n",
      "Episode 404\tLast reward: -16.00\tAverage reward: -16.27\n",
      "Episode 405\tLast reward: -19.00\tAverage reward: -16.41\n",
      "Episode 406\tLast reward: -20.00\tAverage reward: -16.59\n",
      "Episode 407\tLast reward: -13.00\tAverage reward: -16.41\n",
      "Episode 408\tLast reward: -17.00\tAverage reward: -16.44\n",
      "Episode 409\tLast reward: -17.00\tAverage reward: -16.47\n",
      "Episode 410\tLast reward: -11.00\tAverage reward: -16.19\n",
      "Episode 411\tLast reward: -13.00\tAverage reward: -16.03\n",
      "Episode 412\tLast reward: -18.00\tAverage reward: -16.13\n",
      "Episode 413\tLast reward: -17.00\tAverage reward: -16.17\n",
      "Episode 414\tLast reward: -11.00\tAverage reward: -15.92\n",
      "Episode 415\tLast reward: -15.00\tAverage reward: -15.87\n",
      "Episode 416\tLast reward: -17.00\tAverage reward: -15.93\n",
      "Episode 417\tLast reward: -18.00\tAverage reward: -16.03\n",
      "Episode 418\tLast reward: -19.00\tAverage reward: -16.18\n",
      "Episode 419\tLast reward: -19.00\tAverage reward: -16.32\n",
      "Episode 420\tLast reward: -19.00\tAverage reward: -16.45\n",
      "Episode 421\tLast reward: -13.00\tAverage reward: -16.28\n",
      "Episode 422\tLast reward: -15.00\tAverage reward: -16.22\n",
      "Episode 423\tLast reward: -13.00\tAverage reward: -16.06\n",
      "Episode 424\tLast reward: -13.00\tAverage reward: -15.90\n",
      "Episode 425\tLast reward: -19.00\tAverage reward: -16.06\n",
      "Episode 426\tLast reward: -17.00\tAverage reward: -16.11\n",
      "Episode 427\tLast reward: -17.00\tAverage reward: -16.15\n",
      "Episode 428\tLast reward: -15.00\tAverage reward: -16.09\n",
      "Episode 429\tLast reward: -15.00\tAverage reward: -16.04\n",
      "Episode 430\tLast reward: -15.00\tAverage reward: -15.99\n",
      "Episode 431\tLast reward: -18.00\tAverage reward: -16.09\n",
      "Episode 432\tLast reward: -16.00\tAverage reward: -16.08\n",
      "Episode 433\tLast reward: -16.00\tAverage reward: -16.08\n",
      "Episode 434\tLast reward: -15.00\tAverage reward: -16.02\n",
      "Episode 435\tLast reward: -19.00\tAverage reward: -16.17\n",
      "Episode 436\tLast reward: -13.00\tAverage reward: -16.01\n",
      "Episode 437\tLast reward: -11.00\tAverage reward: -15.76\n",
      "Episode 438\tLast reward: -19.00\tAverage reward: -15.93\n",
      "Episode 439\tLast reward: -19.00\tAverage reward: -16.08\n",
      "Episode 440\tLast reward: -16.00\tAverage reward: -16.08\n",
      "Episode 441\tLast reward: -15.00\tAverage reward: -16.02\n",
      "Episode 442\tLast reward: -15.00\tAverage reward: -15.97\n",
      "Episode 443\tLast reward: -18.00\tAverage reward: -16.07\n",
      "Episode 444\tLast reward: -16.00\tAverage reward: -16.07\n",
      "Episode 445\tLast reward: -18.00\tAverage reward: -16.16\n",
      "Episode 446\tLast reward: -15.00\tAverage reward: -16.11\n",
      "Episode 447\tLast reward: -15.00\tAverage reward: -16.05\n",
      "Episode 448\tLast reward: -15.00\tAverage reward: -16.00\n",
      "Episode 449\tLast reward: -16.00\tAverage reward: -16.00\n",
      "Episode 450\tLast reward: -18.00\tAverage reward: -16.10\n",
      "Episode 451\tLast reward: -19.00\tAverage reward: -16.24\n",
      "Episode 452\tLast reward: -17.00\tAverage reward: -16.28\n",
      "Episode 453\tLast reward: -16.00\tAverage reward: -16.27\n",
      "Episode 454\tLast reward: -13.00\tAverage reward: -16.10\n",
      "Episode 455\tLast reward: -9.00\tAverage reward: -15.75\n",
      "Episode 456\tLast reward: -19.00\tAverage reward: -15.91\n",
      "Episode 457\tLast reward: -16.00\tAverage reward: -15.92\n",
      "Episode 458\tLast reward: -19.00\tAverage reward: -16.07\n",
      "Episode 459\tLast reward: -14.00\tAverage reward: -15.97\n",
      "Episode 460\tLast reward: -18.00\tAverage reward: -16.07\n",
      "Episode 461\tLast reward: -15.00\tAverage reward: -16.01\n",
      "Episode 462\tLast reward: -18.00\tAverage reward: -16.11\n",
      "Episode 463\tLast reward: -19.00\tAverage reward: -16.26\n",
      "Episode 464\tLast reward: -13.00\tAverage reward: -16.10\n",
      "Episode 465\tLast reward: -13.00\tAverage reward: -15.94\n",
      "Episode 466\tLast reward: -19.00\tAverage reward: -16.09\n",
      "Episode 467\tLast reward: -11.00\tAverage reward: -15.84\n",
      "Episode 468\tLast reward: -18.00\tAverage reward: -15.95\n",
      "Episode 469\tLast reward: -14.00\tAverage reward: -15.85\n",
      "Episode 470\tLast reward: -14.00\tAverage reward: -15.76\n",
      "Episode 471\tLast reward: -17.00\tAverage reward: -15.82\n",
      "Episode 472\tLast reward: -17.00\tAverage reward: -15.88\n",
      "Episode 473\tLast reward: -17.00\tAverage reward: -15.93\n",
      "Episode 474\tLast reward: -15.00\tAverage reward: -15.89\n",
      "Episode 475\tLast reward: -16.00\tAverage reward: -15.89\n",
      "Episode 476\tLast reward: -16.00\tAverage reward: -15.90\n",
      "Episode 477\tLast reward: -19.00\tAverage reward: -16.05\n",
      "Episode 478\tLast reward: -12.00\tAverage reward: -15.85\n",
      "Episode 479\tLast reward: -14.00\tAverage reward: -15.76\n",
      "Episode 480\tLast reward: -19.00\tAverage reward: -15.92\n",
      "Episode 481\tLast reward: -19.00\tAverage reward: -16.07\n",
      "Episode 482\tLast reward: -18.00\tAverage reward: -16.17\n",
      "Episode 483\tLast reward: -17.00\tAverage reward: -16.21\n",
      "Episode 484\tLast reward: -15.00\tAverage reward: -16.15\n",
      "Episode 485\tLast reward: -17.00\tAverage reward: -16.19\n",
      "Episode 486\tLast reward: -14.00\tAverage reward: -16.08\n",
      "Episode 487\tLast reward: -19.00\tAverage reward: -16.23\n",
      "Episode 488\tLast reward: -10.00\tAverage reward: -15.92\n",
      "Episode 489\tLast reward: -14.00\tAverage reward: -15.82\n",
      "Episode 490\tLast reward: -15.00\tAverage reward: -15.78\n",
      "Episode 491\tLast reward: -19.00\tAverage reward: -15.94\n",
      "Episode 492\tLast reward: -12.00\tAverage reward: -15.75\n",
      "Episode 493\tLast reward: -14.00\tAverage reward: -15.66\n",
      "Episode 494\tLast reward: -11.00\tAverage reward: -15.43\n",
      "Episode 495\tLast reward: -12.00\tAverage reward: -15.25\n",
      "Episode 496\tLast reward: -18.00\tAverage reward: -15.39\n",
      "Episode 497\tLast reward: -14.00\tAverage reward: -15.32\n",
      "Episode 498\tLast reward: -17.00\tAverage reward: -15.41\n",
      "Episode 499\tLast reward: -9.00\tAverage reward: -15.09\n",
      "Episode 500\tLast reward: -15.00\tAverage reward: -15.08\n",
      "Episode 501\tLast reward: -12.00\tAverage reward: -14.93\n",
      "Episode 502\tLast reward: -13.00\tAverage reward: -14.83\n",
      "Episode 503\tLast reward: -17.00\tAverage reward: -14.94\n",
      "Episode 504\tLast reward: -15.00\tAverage reward: -14.94\n",
      "Episode 505\tLast reward: -14.00\tAverage reward: -14.90\n",
      "Episode 506\tLast reward: -17.00\tAverage reward: -15.00\n",
      "Episode 507\tLast reward: -17.00\tAverage reward: -15.10\n",
      "Episode 508\tLast reward: -19.00\tAverage reward: -15.30\n",
      "Episode 509\tLast reward: -17.00\tAverage reward: -15.38\n",
      "Episode 510\tLast reward: -17.00\tAverage reward: -15.46\n",
      "Episode 511\tLast reward: -19.00\tAverage reward: -15.64\n",
      "Episode 512\tLast reward: -16.00\tAverage reward: -15.66\n",
      "Episode 513\tLast reward: -11.00\tAverage reward: -15.42\n",
      "Episode 514\tLast reward: -9.00\tAverage reward: -15.10\n",
      "Episode 515\tLast reward: -19.00\tAverage reward: -15.30\n",
      "Episode 516\tLast reward: -17.00\tAverage reward: -15.38\n",
      "Episode 517\tLast reward: -17.00\tAverage reward: -15.46\n",
      "Episode 518\tLast reward: -13.00\tAverage reward: -15.34\n",
      "Episode 519\tLast reward: -12.00\tAverage reward: -15.17\n",
      "Episode 520\tLast reward: -15.00\tAverage reward: -15.16\n",
      "Episode 521\tLast reward: -14.00\tAverage reward: -15.11\n",
      "Episode 522\tLast reward: -19.00\tAverage reward: -15.30\n",
      "Episode 523\tLast reward: -17.00\tAverage reward: -15.39\n",
      "Episode 524\tLast reward: -19.00\tAverage reward: -15.57\n",
      "Episode 525\tLast reward: -17.00\tAverage reward: -15.64\n",
      "Episode 526\tLast reward: -3.00\tAverage reward: -15.01\n",
      "Episode 527\tLast reward: -17.00\tAverage reward: -15.11\n",
      "Episode 528\tLast reward: -13.00\tAverage reward: -15.00\n",
      "Episode 529\tLast reward: -16.00\tAverage reward: -15.05\n",
      "Episode 530\tLast reward: -11.00\tAverage reward: -14.85\n",
      "Episode 531\tLast reward: -13.00\tAverage reward: -14.76\n",
      "Episode 532\tLast reward: -12.00\tAverage reward: -14.62\n",
      "Episode 533\tLast reward: -17.00\tAverage reward: -14.74\n",
      "Episode 534\tLast reward: -17.00\tAverage reward: -14.85\n",
      "Episode 535\tLast reward: -7.00\tAverage reward: -14.46\n",
      "Episode 536\tLast reward: -13.00\tAverage reward: -14.38\n",
      "Episode 537\tLast reward: -21.00\tAverage reward: -14.72\n",
      "Episode 538\tLast reward: -19.00\tAverage reward: -14.93\n",
      "Episode 539\tLast reward: -15.00\tAverage reward: -14.93\n",
      "Episode 540\tLast reward: -18.00\tAverage reward: -15.09\n",
      "Episode 541\tLast reward: -13.00\tAverage reward: -14.98\n",
      "Episode 542\tLast reward: -11.00\tAverage reward: -14.78\n",
      "Episode 543\tLast reward: -17.00\tAverage reward: -14.89\n",
      "Episode 544\tLast reward: -13.00\tAverage reward: -14.80\n",
      "Episode 545\tLast reward: -19.00\tAverage reward: -15.01\n",
      "Episode 546\tLast reward: -15.00\tAverage reward: -15.01\n",
      "Episode 547\tLast reward: -12.00\tAverage reward: -14.86\n",
      "Episode 548\tLast reward: -15.00\tAverage reward: -14.87\n",
      "Episode 549\tLast reward: -15.00\tAverage reward: -14.87\n",
      "Episode 550\tLast reward: -16.00\tAverage reward: -14.93\n",
      "Episode 551\tLast reward: -15.00\tAverage reward: -14.93\n",
      "Episode 552\tLast reward: -17.00\tAverage reward: -15.04\n",
      "Episode 553\tLast reward: -11.00\tAverage reward: -14.83\n",
      "Episode 554\tLast reward: -14.00\tAverage reward: -14.79\n",
      "Episode 555\tLast reward: -15.00\tAverage reward: -14.80\n",
      "Episode 556\tLast reward: -14.00\tAverage reward: -14.76\n",
      "Episode 557\tLast reward: -11.00\tAverage reward: -14.57\n",
      "Episode 558\tLast reward: -15.00\tAverage reward: -14.60\n",
      "Episode 559\tLast reward: -19.00\tAverage reward: -14.82\n",
      "Episode 560\tLast reward: -15.00\tAverage reward: -14.83\n",
      "Episode 561\tLast reward: -10.00\tAverage reward: -14.58\n",
      "Episode 562\tLast reward: -16.00\tAverage reward: -14.65\n",
      "Episode 563\tLast reward: -11.00\tAverage reward: -14.47\n",
      "Episode 564\tLast reward: -17.00\tAverage reward: -14.60\n",
      "Episode 565\tLast reward: -15.00\tAverage reward: -14.62\n",
      "Episode 566\tLast reward: -19.00\tAverage reward: -14.84\n",
      "Episode 567\tLast reward: -11.00\tAverage reward: -14.65\n",
      "Episode 568\tLast reward: -18.00\tAverage reward: -14.81\n",
      "Episode 569\tLast reward: -15.00\tAverage reward: -14.82\n",
      "Episode 570\tLast reward: -12.00\tAverage reward: -14.68\n",
      "Episode 571\tLast reward: -17.00\tAverage reward: -14.80\n",
      "Episode 572\tLast reward: -13.00\tAverage reward: -14.71\n",
      "Episode 573\tLast reward: -17.00\tAverage reward: -14.82\n",
      "Episode 574\tLast reward: -17.00\tAverage reward: -14.93\n",
      "Episode 575\tLast reward: -16.00\tAverage reward: -14.98\n",
      "Episode 576\tLast reward: -19.00\tAverage reward: -15.19\n",
      "Episode 577\tLast reward: -11.00\tAverage reward: -14.98\n",
      "Episode 578\tLast reward: -16.00\tAverage reward: -15.03\n",
      "Episode 579\tLast reward: -15.00\tAverage reward: -15.03\n",
      "Episode 580\tLast reward: -14.00\tAverage reward: -14.97\n",
      "Episode 581\tLast reward: -17.00\tAverage reward: -15.08\n",
      "Episode 582\tLast reward: -19.00\tAverage reward: -15.27\n",
      "Episode 583\tLast reward: -15.00\tAverage reward: -15.26\n",
      "Episode 584\tLast reward: -16.00\tAverage reward: -15.30\n",
      "Episode 585\tLast reward: -13.00\tAverage reward: -15.18\n",
      "Episode 586\tLast reward: -15.00\tAverage reward: -15.17\n",
      "Episode 587\tLast reward: -19.00\tAverage reward: -15.36\n",
      "Episode 588\tLast reward: -14.00\tAverage reward: -15.29\n",
      "Episode 589\tLast reward: -7.00\tAverage reward: -14.88\n",
      "Episode 590\tLast reward: -13.00\tAverage reward: -14.79\n",
      "Episode 591\tLast reward: -17.00\tAverage reward: -14.90\n",
      "Episode 592\tLast reward: -15.00\tAverage reward: -14.90\n",
      "Episode 593\tLast reward: -12.00\tAverage reward: -14.76\n",
      "Episode 594\tLast reward: -11.00\tAverage reward: -14.57\n",
      "Episode 595\tLast reward: -15.00\tAverage reward: -14.59\n",
      "Episode 596\tLast reward: -10.00\tAverage reward: -14.36\n",
      "Episode 597\tLast reward: -14.00\tAverage reward: -14.34\n",
      "Episode 598\tLast reward: -9.00\tAverage reward: -14.08\n",
      "Episode 599\tLast reward: -14.00\tAverage reward: -14.07\n",
      "Episode 600\tLast reward: -14.00\tAverage reward: -14.07\n",
      "Episode 601\tLast reward: -11.00\tAverage reward: -13.92\n",
      "Episode 602\tLast reward: -13.00\tAverage reward: -13.87\n",
      "Episode 603\tLast reward: -14.00\tAverage reward: -13.88\n",
      "Episode 604\tLast reward: -11.00\tAverage reward: -13.73\n",
      "Episode 605\tLast reward: -13.00\tAverage reward: -13.70\n",
      "Episode 606\tLast reward: -16.00\tAverage reward: -13.81\n",
      "Episode 607\tLast reward: -18.00\tAverage reward: -14.02\n",
      "Episode 608\tLast reward: -11.00\tAverage reward: -13.87\n",
      "Episode 609\tLast reward: -19.00\tAverage reward: -14.13\n",
      "Episode 610\tLast reward: -17.00\tAverage reward: -14.27\n",
      "Episode 611\tLast reward: -15.00\tAverage reward: -14.31\n",
      "Episode 612\tLast reward: -15.00\tAverage reward: -14.34\n",
      "Episode 613\tLast reward: -14.00\tAverage reward: -14.32\n",
      "Episode 614\tLast reward: -12.00\tAverage reward: -14.21\n",
      "Episode 615\tLast reward: -17.00\tAverage reward: -14.35\n",
      "Episode 616\tLast reward: -19.00\tAverage reward: -14.58\n",
      "Episode 617\tLast reward: -13.00\tAverage reward: -14.50\n",
      "Episode 618\tLast reward: -15.00\tAverage reward: -14.53\n",
      "Episode 619\tLast reward: -10.00\tAverage reward: -14.30\n",
      "Episode 620\tLast reward: -15.00\tAverage reward: -14.33\n",
      "Episode 621\tLast reward: -9.00\tAverage reward: -14.07\n",
      "Episode 622\tLast reward: -14.00\tAverage reward: -14.06\n",
      "Episode 623\tLast reward: -11.00\tAverage reward: -13.91\n",
      "Episode 624\tLast reward: -16.00\tAverage reward: -14.02\n",
      "Episode 625\tLast reward: -15.00\tAverage reward: -14.06\n",
      "Episode 626\tLast reward: -14.00\tAverage reward: -14.06\n",
      "Episode 627\tLast reward: -14.00\tAverage reward: -14.06\n",
      "Episode 628\tLast reward: -16.00\tAverage reward: -14.16\n",
      "Episode 629\tLast reward: -16.00\tAverage reward: -14.25\n",
      "Episode 630\tLast reward: -15.00\tAverage reward: -14.29\n",
      "Episode 631\tLast reward: -10.00\tAverage reward: -14.07\n",
      "Episode 632\tLast reward: -9.00\tAverage reward: -13.82\n",
      "Episode 633\tLast reward: -10.00\tAverage reward: -13.63\n",
      "Episode 634\tLast reward: -10.00\tAverage reward: -13.45\n",
      "Episode 635\tLast reward: -14.00\tAverage reward: -13.47\n",
      "Episode 636\tLast reward: -15.00\tAverage reward: -13.55\n",
      "Episode 637\tLast reward: -19.00\tAverage reward: -13.82\n",
      "Episode 638\tLast reward: -19.00\tAverage reward: -14.08\n",
      "Episode 639\tLast reward: -20.00\tAverage reward: -14.38\n",
      "Episode 640\tLast reward: -10.00\tAverage reward: -14.16\n",
      "Episode 641\tLast reward: -8.00\tAverage reward: -13.85\n",
      "Episode 642\tLast reward: -14.00\tAverage reward: -13.86\n",
      "Episode 643\tLast reward: -11.00\tAverage reward: -13.71\n",
      "Episode 644\tLast reward: -10.00\tAverage reward: -13.53\n",
      "Episode 645\tLast reward: -17.00\tAverage reward: -13.70\n",
      "Episode 646\tLast reward: -11.00\tAverage reward: -13.57\n",
      "Episode 647\tLast reward: -13.00\tAverage reward: -13.54\n",
      "Episode 648\tLast reward: -15.00\tAverage reward: -13.61\n",
      "Episode 649\tLast reward: -11.00\tAverage reward: -13.48\n",
      "Episode 650\tLast reward: -12.00\tAverage reward: -13.41\n",
      "Episode 651\tLast reward: -18.00\tAverage reward: -13.64\n",
      "Episode 652\tLast reward: -15.00\tAverage reward: -13.71\n",
      "Episode 653\tLast reward: -14.00\tAverage reward: -13.72\n",
      "Episode 654\tLast reward: -19.00\tAverage reward: -13.98\n",
      "Episode 655\tLast reward: -10.00\tAverage reward: -13.78\n",
      "Episode 656\tLast reward: -15.00\tAverage reward: -13.85\n",
      "Episode 657\tLast reward: -11.00\tAverage reward: -13.70\n",
      "Episode 658\tLast reward: -12.00\tAverage reward: -13.62\n",
      "Episode 659\tLast reward: -16.00\tAverage reward: -13.74\n",
      "Episode 660\tLast reward: -14.00\tAverage reward: -13.75\n",
      "Episode 661\tLast reward: -13.00\tAverage reward: -13.71\n",
      "Episode 662\tLast reward: -19.00\tAverage reward: -13.98\n",
      "Episode 663\tLast reward: -17.00\tAverage reward: -14.13\n",
      "Episode 664\tLast reward: -13.00\tAverage reward: -14.07\n",
      "Episode 665\tLast reward: -4.00\tAverage reward: -13.57\n",
      "Episode 666\tLast reward: -11.00\tAverage reward: -13.44\n",
      "Episode 667\tLast reward: -10.00\tAverage reward: -13.27\n",
      "Episode 668\tLast reward: -21.00\tAverage reward: -13.65\n",
      "Episode 669\tLast reward: -11.00\tAverage reward: -13.52\n",
      "Episode 670\tLast reward: -2.00\tAverage reward: -12.95\n",
      "Episode 671\tLast reward: -10.00\tAverage reward: -12.80\n",
      "Episode 672\tLast reward: -12.00\tAverage reward: -12.76\n",
      "Episode 673\tLast reward: -9.00\tAverage reward: -12.57\n",
      "Episode 674\tLast reward: -11.00\tAverage reward: -12.49\n",
      "Episode 675\tLast reward: -20.00\tAverage reward: -12.87\n",
      "Episode 676\tLast reward: -19.00\tAverage reward: -13.17\n",
      "Episode 677\tLast reward: -11.00\tAverage reward: -13.07\n",
      "Episode 678\tLast reward: -14.00\tAverage reward: -13.11\n",
      "Episode 679\tLast reward: -14.00\tAverage reward: -13.16\n",
      "Episode 680\tLast reward: -11.00\tAverage reward: -13.05\n",
      "Episode 681\tLast reward: -12.00\tAverage reward: -13.00\n",
      "Episode 682\tLast reward: -17.00\tAverage reward: -13.20\n",
      "Episode 683\tLast reward: -15.00\tAverage reward: -13.29\n",
      "Episode 684\tLast reward: -15.00\tAverage reward: -13.37\n",
      "Episode 685\tLast reward: -13.00\tAverage reward: -13.35\n",
      "Episode 686\tLast reward: -17.00\tAverage reward: -13.54\n",
      "Episode 687\tLast reward: -14.00\tAverage reward: -13.56\n",
      "Episode 688\tLast reward: -12.00\tAverage reward: -13.48\n",
      "Episode 689\tLast reward: -13.00\tAverage reward: -13.46\n",
      "Episode 690\tLast reward: -17.00\tAverage reward: -13.63\n",
      "Episode 691\tLast reward: -13.00\tAverage reward: -13.60\n",
      "Episode 692\tLast reward: -5.00\tAverage reward: -13.17\n",
      "Episode 693\tLast reward: -12.00\tAverage reward: -13.11\n",
      "Episode 694\tLast reward: -17.00\tAverage reward: -13.31\n",
      "Episode 695\tLast reward: -7.00\tAverage reward: -12.99\n",
      "Episode 696\tLast reward: -15.00\tAverage reward: -13.09\n",
      "Episode 697\tLast reward: -11.00\tAverage reward: -12.99\n",
      "Episode 698\tLast reward: -9.00\tAverage reward: -12.79\n",
      "Episode 699\tLast reward: -15.00\tAverage reward: -12.90\n",
      "Episode 700\tLast reward: -14.00\tAverage reward: -12.95\n",
      "Episode 701\tLast reward: -8.00\tAverage reward: -12.71\n",
      "Episode 702\tLast reward: -11.00\tAverage reward: -12.62\n",
      "Episode 703\tLast reward: -12.00\tAverage reward: -12.59\n",
      "Episode 704\tLast reward: -7.00\tAverage reward: -12.31\n",
      "Episode 705\tLast reward: -16.00\tAverage reward: -12.50\n",
      "Episode 706\tLast reward: -15.00\tAverage reward: -12.62\n",
      "Episode 707\tLast reward: -13.00\tAverage reward: -12.64\n",
      "Episode 708\tLast reward: -19.00\tAverage reward: -12.96\n",
      "Episode 709\tLast reward: -9.00\tAverage reward: -12.76\n",
      "Episode 710\tLast reward: -17.00\tAverage reward: -12.97\n",
      "Episode 711\tLast reward: -5.00\tAverage reward: -12.57\n",
      "Episode 712\tLast reward: -10.00\tAverage reward: -12.44\n",
      "Episode 713\tLast reward: -15.00\tAverage reward: -12.57\n",
      "Episode 714\tLast reward: -11.00\tAverage reward: -12.49\n",
      "Episode 715\tLast reward: -15.00\tAverage reward: -12.62\n",
      "Episode 716\tLast reward: -15.00\tAverage reward: -12.74\n",
      "Episode 717\tLast reward: -16.00\tAverage reward: -12.90\n",
      "Episode 718\tLast reward: -17.00\tAverage reward: -13.11\n",
      "Episode 719\tLast reward: -13.00\tAverage reward: -13.10\n",
      "Episode 720\tLast reward: -14.00\tAverage reward: -13.15\n",
      "Episode 721\tLast reward: -10.00\tAverage reward: -12.99\n",
      "Episode 722\tLast reward: -9.00\tAverage reward: -12.79\n",
      "Episode 723\tLast reward: -17.00\tAverage reward: -13.00\n",
      "Episode 724\tLast reward: -15.00\tAverage reward: -13.10\n",
      "Episode 725\tLast reward: -13.00\tAverage reward: -13.09\n",
      "Episode 726\tLast reward: -15.00\tAverage reward: -13.19\n",
      "Episode 727\tLast reward: -19.00\tAverage reward: -13.48\n",
      "Episode 728\tLast reward: -17.00\tAverage reward: -13.66\n",
      "Episode 729\tLast reward: -19.00\tAverage reward: -13.92\n",
      "Episode 730\tLast reward: -16.00\tAverage reward: -14.03\n",
      "Episode 731\tLast reward: -9.00\tAverage reward: -13.78\n",
      "Episode 732\tLast reward: -17.00\tAverage reward: -13.94\n",
      "Episode 733\tLast reward: -15.00\tAverage reward: -13.99\n",
      "Episode 734\tLast reward: -11.00\tAverage reward: -13.84\n",
      "Episode 735\tLast reward: -12.00\tAverage reward: -13.75\n",
      "Episode 736\tLast reward: -7.00\tAverage reward: -13.41\n",
      "Episode 737\tLast reward: -7.00\tAverage reward: -13.09\n",
      "Episode 738\tLast reward: -15.00\tAverage reward: -13.19\n",
      "Episode 739\tLast reward: -17.00\tAverage reward: -13.38\n",
      "Episode 740\tLast reward: -11.00\tAverage reward: -13.26\n",
      "Episode 741\tLast reward: -18.00\tAverage reward: -13.50\n",
      "Episode 742\tLast reward: -13.00\tAverage reward: -13.47\n",
      "Episode 743\tLast reward: -9.00\tAverage reward: -13.25\n",
      "Episode 744\tLast reward: -5.00\tAverage reward: -12.83\n",
      "Episode 745\tLast reward: -19.00\tAverage reward: -13.14\n",
      "Episode 746\tLast reward: -12.00\tAverage reward: -13.09\n",
      "Episode 747\tLast reward: -15.00\tAverage reward: -13.18\n",
      "Episode 748\tLast reward: -10.00\tAverage reward: -13.02\n",
      "Episode 749\tLast reward: -13.00\tAverage reward: -13.02\n",
      "Episode 750\tLast reward: -14.00\tAverage reward: -13.07\n",
      "Episode 751\tLast reward: -14.00\tAverage reward: -13.12\n",
      "Episode 752\tLast reward: -16.00\tAverage reward: -13.26\n",
      "Episode 753\tLast reward: -14.00\tAverage reward: -13.30\n",
      "Episode 754\tLast reward: -15.00\tAverage reward: -13.38\n",
      "Episode 755\tLast reward: -10.00\tAverage reward: -13.21\n",
      "Episode 756\tLast reward: -17.00\tAverage reward: -13.40\n",
      "Episode 757\tLast reward: -13.00\tAverage reward: -13.38\n",
      "Episode 758\tLast reward: -21.00\tAverage reward: -13.76\n",
      "Episode 759\tLast reward: -15.00\tAverage reward: -13.83\n",
      "Episode 760\tLast reward: -20.00\tAverage reward: -14.13\n",
      "Episode 761\tLast reward: -15.00\tAverage reward: -14.18\n",
      "Episode 762\tLast reward: -9.00\tAverage reward: -13.92\n",
      "Episode 763\tLast reward: -9.00\tAverage reward: -13.67\n",
      "Episode 764\tLast reward: -11.00\tAverage reward: -13.54\n",
      "Episode 765\tLast reward: -8.00\tAverage reward: -13.26\n",
      "Episode 766\tLast reward: -16.00\tAverage reward: -13.40\n",
      "Episode 767\tLast reward: -13.00\tAverage reward: -13.38\n",
      "Episode 768\tLast reward: -15.00\tAverage reward: -13.46\n",
      "Episode 769\tLast reward: -11.00\tAverage reward: -13.34\n",
      "Episode 770\tLast reward: -17.00\tAverage reward: -13.52\n",
      "Episode 771\tLast reward: -3.00\tAverage reward: -12.99\n",
      "Episode 772\tLast reward: -10.00\tAverage reward: -12.84\n",
      "Episode 773\tLast reward: -21.00\tAverage reward: -13.25\n",
      "Episode 774\tLast reward: -11.00\tAverage reward: -13.14\n",
      "Episode 775\tLast reward: -5.00\tAverage reward: -12.73\n",
      "Episode 776\tLast reward: -9.00\tAverage reward: -12.55\n",
      "Episode 777\tLast reward: -16.00\tAverage reward: -12.72\n",
      "Episode 778\tLast reward: -13.00\tAverage reward: -12.73\n",
      "Episode 779\tLast reward: -11.00\tAverage reward: -12.65\n",
      "Episode 780\tLast reward: -10.00\tAverage reward: -12.51\n",
      "Episode 781\tLast reward: -13.00\tAverage reward: -12.54\n",
      "Episode 782\tLast reward: -13.00\tAverage reward: -12.56\n",
      "Episode 783\tLast reward: -12.00\tAverage reward: -12.53\n",
      "Episode 784\tLast reward: -17.00\tAverage reward: -12.76\n",
      "Episode 785\tLast reward: -11.00\tAverage reward: -12.67\n",
      "Episode 786\tLast reward: -11.00\tAverage reward: -12.59\n",
      "Episode 787\tLast reward: -12.00\tAverage reward: -12.56\n",
      "Episode 788\tLast reward: -12.00\tAverage reward: -12.53\n",
      "Episode 789\tLast reward: -13.00\tAverage reward: -12.55\n",
      "Episode 790\tLast reward: -16.00\tAverage reward: -12.72\n",
      "Episode 791\tLast reward: -14.00\tAverage reward: -12.79\n",
      "Episode 792\tLast reward: -8.00\tAverage reward: -12.55\n",
      "Episode 793\tLast reward: -15.00\tAverage reward: -12.67\n",
      "Episode 794\tLast reward: -9.00\tAverage reward: -12.49\n",
      "Episode 795\tLast reward: -14.00\tAverage reward: -12.56\n",
      "Episode 796\tLast reward: -16.00\tAverage reward: -12.74\n",
      "Episode 797\tLast reward: -15.00\tAverage reward: -12.85\n",
      "Episode 798\tLast reward: -8.00\tAverage reward: -12.61\n",
      "Episode 799\tLast reward: -17.00\tAverage reward: -12.83\n",
      "Episode 800\tLast reward: -14.00\tAverage reward: -12.88\n",
      "Episode 801\tLast reward: -12.00\tAverage reward: -12.84\n",
      "Episode 802\tLast reward: -14.00\tAverage reward: -12.90\n",
      "Episode 803\tLast reward: -15.00\tAverage reward: -13.00\n",
      "Episode 804\tLast reward: -17.00\tAverage reward: -13.20\n",
      "Episode 805\tLast reward: -10.00\tAverage reward: -13.04\n",
      "Episode 806\tLast reward: -9.00\tAverage reward: -12.84\n",
      "Episode 807\tLast reward: -5.00\tAverage reward: -12.45\n",
      "Episode 808\tLast reward: -17.00\tAverage reward: -12.68\n",
      "Episode 809\tLast reward: -19.00\tAverage reward: -12.99\n",
      "Episode 810\tLast reward: -14.00\tAverage reward: -13.04\n",
      "Episode 811\tLast reward: -16.00\tAverage reward: -13.19\n",
      "Episode 812\tLast reward: -12.00\tAverage reward: -13.13\n",
      "Episode 813\tLast reward: -14.00\tAverage reward: -13.17\n",
      "Episode 814\tLast reward: -7.00\tAverage reward: -12.87\n",
      "Episode 815\tLast reward: -6.00\tAverage reward: -12.52\n",
      "Episode 816\tLast reward: -10.00\tAverage reward: -12.40\n",
      "Episode 817\tLast reward: 1.00\tAverage reward: -11.73\n",
      "Episode 818\tLast reward: -15.00\tAverage reward: -11.89\n",
      "Episode 819\tLast reward: -16.00\tAverage reward: -12.10\n",
      "Episode 820\tLast reward: -12.00\tAverage reward: -12.09\n",
      "Episode 821\tLast reward: -17.00\tAverage reward: -12.34\n",
      "Episode 822\tLast reward: -13.00\tAverage reward: -12.37\n",
      "Episode 823\tLast reward: -10.00\tAverage reward: -12.25\n",
      "Episode 824\tLast reward: -20.00\tAverage reward: -12.64\n",
      "Episode 825\tLast reward: -15.00\tAverage reward: -12.76\n",
      "Episode 826\tLast reward: -14.00\tAverage reward: -12.82\n",
      "Episode 827\tLast reward: -13.00\tAverage reward: -12.83\n",
      "Episode 828\tLast reward: -12.00\tAverage reward: -12.79\n",
      "Episode 829\tLast reward: -16.00\tAverage reward: -12.95\n",
      "Episode 830\tLast reward: -17.00\tAverage reward: -13.15\n",
      "Episode 831\tLast reward: -7.00\tAverage reward: -12.84\n",
      "Episode 832\tLast reward: -14.00\tAverage reward: -12.90\n",
      "Episode 833\tLast reward: -13.00\tAverage reward: -12.91\n",
      "Episode 834\tLast reward: -15.00\tAverage reward: -13.01\n",
      "Episode 835\tLast reward: -14.00\tAverage reward: -13.06\n",
      "Episode 836\tLast reward: -11.00\tAverage reward: -12.96\n",
      "Episode 837\tLast reward: -12.00\tAverage reward: -12.91\n",
      "Episode 838\tLast reward: -9.00\tAverage reward: -12.71\n",
      "Episode 839\tLast reward: -18.00\tAverage reward: -12.98\n",
      "Episode 840\tLast reward: -13.00\tAverage reward: -12.98\n",
      "Episode 841\tLast reward: -9.00\tAverage reward: -12.78\n",
      "Episode 842\tLast reward: -17.00\tAverage reward: -12.99\n",
      "Episode 843\tLast reward: -6.00\tAverage reward: -12.64\n",
      "Episode 844\tLast reward: -11.00\tAverage reward: -12.56\n",
      "Episode 845\tLast reward: -5.00\tAverage reward: -12.18\n",
      "Episode 846\tLast reward: -15.00\tAverage reward: -12.32\n",
      "Episode 847\tLast reward: -19.00\tAverage reward: -12.66\n",
      "Episode 848\tLast reward: -14.00\tAverage reward: -12.72\n",
      "Episode 849\tLast reward: -7.00\tAverage reward: -12.44\n",
      "Episode 850\tLast reward: -19.00\tAverage reward: -12.77\n",
      "Episode 851\tLast reward: -12.00\tAverage reward: -12.73\n",
      "Episode 852\tLast reward: -14.00\tAverage reward: -12.79\n",
      "Episode 853\tLast reward: -14.00\tAverage reward: -12.85\n",
      "Episode 854\tLast reward: -14.00\tAverage reward: -12.91\n",
      "Episode 855\tLast reward: -10.00\tAverage reward: -12.76\n",
      "Episode 856\tLast reward: -14.00\tAverage reward: -12.82\n",
      "Episode 857\tLast reward: -19.00\tAverage reward: -13.13\n",
      "Episode 858\tLast reward: -7.00\tAverage reward: -12.83\n",
      "Episode 859\tLast reward: -15.00\tAverage reward: -12.94\n",
      "Episode 860\tLast reward: -12.00\tAverage reward: -12.89\n",
      "Episode 861\tLast reward: -13.00\tAverage reward: -12.89\n",
      "Episode 862\tLast reward: -17.00\tAverage reward: -13.10\n",
      "Episode 863\tLast reward: -15.00\tAverage reward: -13.19\n",
      "Episode 864\tLast reward: -16.00\tAverage reward: -13.33\n",
      "Episode 865\tLast reward: -18.00\tAverage reward: -13.57\n",
      "Episode 866\tLast reward: -11.00\tAverage reward: -13.44\n",
      "Episode 867\tLast reward: -11.00\tAverage reward: -13.32\n",
      "Episode 868\tLast reward: -16.00\tAverage reward: -13.45\n",
      "Episode 869\tLast reward: -13.00\tAverage reward: -13.43\n",
      "Episode 870\tLast reward: -12.00\tAverage reward: -13.36\n",
      "Episode 871\tLast reward: -6.00\tAverage reward: -12.99\n",
      "Episode 872\tLast reward: -13.00\tAverage reward: -12.99\n",
      "Episode 873\tLast reward: -12.00\tAverage reward: -12.94\n",
      "Episode 874\tLast reward: -17.00\tAverage reward: -13.14\n",
      "Episode 875\tLast reward: -4.00\tAverage reward: -12.69\n",
      "Episode 876\tLast reward: -7.00\tAverage reward: -12.40\n",
      "Episode 877\tLast reward: -14.00\tAverage reward: -12.48\n",
      "Episode 878\tLast reward: -16.00\tAverage reward: -12.66\n",
      "Episode 879\tLast reward: -11.00\tAverage reward: -12.58\n",
      "Episode 880\tLast reward: -11.00\tAverage reward: -12.50\n",
      "Episode 881\tLast reward: -13.00\tAverage reward: -12.52\n",
      "Episode 882\tLast reward: -15.00\tAverage reward: -12.65\n",
      "Episode 883\tLast reward: -12.00\tAverage reward: -12.61\n",
      "Episode 884\tLast reward: -12.00\tAverage reward: -12.58\n",
      "Episode 885\tLast reward: -10.00\tAverage reward: -12.45\n",
      "Episode 886\tLast reward: -19.00\tAverage reward: -12.78\n",
      "Episode 887\tLast reward: -16.00\tAverage reward: -12.94\n",
      "Episode 888\tLast reward: -15.00\tAverage reward: -13.04\n",
      "Episode 889\tLast reward: -13.00\tAverage reward: -13.04\n",
      "Episode 890\tLast reward: 1.00\tAverage reward: -12.34\n",
      "Episode 891\tLast reward: -15.00\tAverage reward: -12.47\n",
      "Episode 892\tLast reward: -12.00\tAverage reward: -12.45\n",
      "Episode 893\tLast reward: -14.00\tAverage reward: -12.53\n",
      "Episode 894\tLast reward: -13.00\tAverage reward: -12.55\n",
      "Episode 895\tLast reward: -11.00\tAverage reward: -12.47\n",
      "Episode 896\tLast reward: -13.00\tAverage reward: -12.50\n",
      "Episode 897\tLast reward: -11.00\tAverage reward: -12.42\n",
      "Episode 898\tLast reward: -10.00\tAverage reward: -12.30\n",
      "Episode 899\tLast reward: -11.00\tAverage reward: -12.24\n",
      "Episode 900\tLast reward: -15.00\tAverage reward: -12.38\n",
      "Episode 901\tLast reward: -18.00\tAverage reward: -12.66\n",
      "Episode 902\tLast reward: -17.00\tAverage reward: -12.87\n",
      "Episode 903\tLast reward: -8.00\tAverage reward: -12.63\n",
      "Episode 904\tLast reward: -15.00\tAverage reward: -12.75\n",
      "Episode 905\tLast reward: -8.00\tAverage reward: -12.51\n",
      "Episode 906\tLast reward: -12.00\tAverage reward: -12.49\n",
      "Episode 907\tLast reward: -11.00\tAverage reward: -12.41\n",
      "Episode 908\tLast reward: -14.00\tAverage reward: -12.49\n",
      "Episode 909\tLast reward: -12.00\tAverage reward: -12.47\n",
      "Episode 910\tLast reward: -14.00\tAverage reward: -12.54\n",
      "Episode 911\tLast reward: -13.00\tAverage reward: -12.57\n",
      "Episode 912\tLast reward: -17.00\tAverage reward: -12.79\n",
      "Episode 913\tLast reward: -13.00\tAverage reward: -12.80\n",
      "Episode 914\tLast reward: -7.00\tAverage reward: -12.51\n",
      "Episode 915\tLast reward: -17.00\tAverage reward: -12.73\n",
      "Episode 916\tLast reward: -12.00\tAverage reward: -12.70\n",
      "Episode 917\tLast reward: -11.00\tAverage reward: -12.61\n",
      "Episode 918\tLast reward: -13.00\tAverage reward: -12.63\n",
      "Episode 919\tLast reward: -5.00\tAverage reward: -12.25\n",
      "Episode 920\tLast reward: -13.00\tAverage reward: -12.29\n",
      "Episode 921\tLast reward: -8.00\tAverage reward: -12.07\n",
      "Episode 922\tLast reward: -13.00\tAverage reward: -12.12\n",
      "Episode 923\tLast reward: -11.00\tAverage reward: -12.06\n",
      "Episode 924\tLast reward: -11.00\tAverage reward: -12.01\n",
      "Episode 925\tLast reward: -9.00\tAverage reward: -11.86\n",
      "Episode 926\tLast reward: -12.00\tAverage reward: -11.87\n",
      "Episode 927\tLast reward: -12.00\tAverage reward: -11.87\n",
      "Episode 928\tLast reward: -11.00\tAverage reward: -11.83\n",
      "Episode 929\tLast reward: -15.00\tAverage reward: -11.99\n",
      "Episode 930\tLast reward: -13.00\tAverage reward: -12.04\n",
      "Episode 931\tLast reward: -11.00\tAverage reward: -11.99\n",
      "Episode 932\tLast reward: -17.00\tAverage reward: -12.24\n",
      "Episode 933\tLast reward: -18.00\tAverage reward: -12.53\n",
      "Episode 934\tLast reward: -16.00\tAverage reward: -12.70\n",
      "Episode 935\tLast reward: -12.00\tAverage reward: -12.66\n",
      "Episode 936\tLast reward: -11.00\tAverage reward: -12.58\n",
      "Episode 937\tLast reward: -12.00\tAverage reward: -12.55\n",
      "Episode 938\tLast reward: -15.00\tAverage reward: -12.67\n",
      "Episode 939\tLast reward: -15.00\tAverage reward: -12.79\n",
      "Episode 940\tLast reward: -19.00\tAverage reward: -13.10\n",
      "Episode 941\tLast reward: -13.00\tAverage reward: -13.10\n",
      "Episode 942\tLast reward: -14.00\tAverage reward: -13.14\n",
      "Episode 943\tLast reward: -12.00\tAverage reward: -13.08\n",
      "Episode 944\tLast reward: -17.00\tAverage reward: -13.28\n",
      "Episode 945\tLast reward: -13.00\tAverage reward: -13.27\n",
      "Episode 946\tLast reward: -15.00\tAverage reward: -13.35\n",
      "Episode 947\tLast reward: -10.00\tAverage reward: -13.19\n",
      "Episode 948\tLast reward: -12.00\tAverage reward: -13.13\n",
      "Episode 949\tLast reward: -10.00\tAverage reward: -12.97\n",
      "Episode 950\tLast reward: -13.00\tAverage reward: -12.97\n",
      "Episode 951\tLast reward: -14.00\tAverage reward: -13.02\n",
      "Episode 952\tLast reward: -16.00\tAverage reward: -13.17\n",
      "Episode 953\tLast reward: -15.00\tAverage reward: -13.26\n",
      "Episode 954\tLast reward: -16.00\tAverage reward: -13.40\n",
      "Episode 955\tLast reward: -15.00\tAverage reward: -13.48\n",
      "Episode 956\tLast reward: -19.00\tAverage reward: -13.76\n",
      "Episode 957\tLast reward: -14.00\tAverage reward: -13.77\n",
      "Episode 958\tLast reward: -15.00\tAverage reward: -13.83\n",
      "Episode 959\tLast reward: -10.00\tAverage reward: -13.64\n",
      "Episode 960\tLast reward: -13.00\tAverage reward: -13.61\n",
      "Episode 961\tLast reward: -14.00\tAverage reward: -13.63\n",
      "Episode 962\tLast reward: -11.00\tAverage reward: -13.49\n",
      "Episode 963\tLast reward: -15.00\tAverage reward: -13.57\n",
      "Episode 964\tLast reward: -11.00\tAverage reward: -13.44\n",
      "Episode 965\tLast reward: -13.00\tAverage reward: -13.42\n",
      "Episode 966\tLast reward: -11.00\tAverage reward: -13.30\n",
      "Episode 967\tLast reward: -11.00\tAverage reward: -13.18\n",
      "Episode 968\tLast reward: -13.00\tAverage reward: -13.17\n",
      "Episode 969\tLast reward: -12.00\tAverage reward: -13.12\n",
      "Episode 970\tLast reward: -7.00\tAverage reward: -12.81\n",
      "Episode 971\tLast reward: -15.00\tAverage reward: -12.92\n",
      "Episode 972\tLast reward: -11.00\tAverage reward: -12.82\n",
      "Episode 973\tLast reward: -7.00\tAverage reward: -12.53\n",
      "Episode 974\tLast reward: -11.00\tAverage reward: -12.46\n",
      "Episode 975\tLast reward: -13.00\tAverage reward: -12.48\n",
      "Episode 976\tLast reward: -7.00\tAverage reward: -12.21\n",
      "Episode 977\tLast reward: -11.00\tAverage reward: -12.15\n",
      "Episode 978\tLast reward: -16.00\tAverage reward: -12.34\n",
      "Episode 979\tLast reward: -5.00\tAverage reward: -11.97\n",
      "Episode 980\tLast reward: -13.00\tAverage reward: -12.03\n",
      "Episode 981\tLast reward: -16.00\tAverage reward: -12.22\n",
      "Episode 982\tLast reward: -11.00\tAverage reward: -12.16\n",
      "Episode 983\tLast reward: -6.00\tAverage reward: -11.85\n",
      "Episode 984\tLast reward: -15.00\tAverage reward: -12.01\n",
      "Episode 985\tLast reward: -21.00\tAverage reward: -12.46\n",
      "Episode 986\tLast reward: -5.00\tAverage reward: -12.09\n",
      "Episode 987\tLast reward: -13.00\tAverage reward: -12.13\n",
      "Episode 988\tLast reward: -5.00\tAverage reward: -11.78\n",
      "Episode 989\tLast reward: -13.00\tAverage reward: -11.84\n",
      "Episode 990\tLast reward: -13.00\tAverage reward: -11.90\n",
      "Episode 991\tLast reward: -13.00\tAverage reward: -11.95\n",
      "Episode 992\tLast reward: -14.00\tAverage reward: -12.05\n",
      "Episode 993\tLast reward: -7.00\tAverage reward: -11.80\n",
      "Episode 994\tLast reward: -7.00\tAverage reward: -11.56\n",
      "Episode 995\tLast reward: -7.00\tAverage reward: -11.33\n",
      "Episode 996\tLast reward: -12.00\tAverage reward: -11.37\n",
      "Episode 997\tLast reward: -7.00\tAverage reward: -11.15\n",
      "Episode 998\tLast reward: -3.00\tAverage reward: -10.74\n",
      "Episode 999\tLast reward: -9.00\tAverage reward: -10.65\n",
      "Episode 1000\tLast reward: -11.00\tAverage reward: -10.67\n",
      "Episode 1001\tLast reward: -14.00\tAverage reward: -10.84\n",
      "Episode 1002\tLast reward: -8.00\tAverage reward: -10.70\n",
      "Episode 1003\tLast reward: -11.00\tAverage reward: -10.71\n",
      "Episode 1004\tLast reward: -13.00\tAverage reward: -10.83\n",
      "Episode 1005\tLast reward: -12.00\tAverage reward: -10.88\n",
      "Episode 1006\tLast reward: -16.00\tAverage reward: -11.14\n",
      "Episode 1007\tLast reward: -11.00\tAverage reward: -11.13\n",
      "Episode 1008\tLast reward: -13.00\tAverage reward: -11.23\n",
      "Episode 1009\tLast reward: -8.00\tAverage reward: -11.06\n",
      "Episode 1010\tLast reward: -13.00\tAverage reward: -11.16\n",
      "Episode 1011\tLast reward: -19.00\tAverage reward: -11.55\n",
      "Episode 1012\tLast reward: -14.00\tAverage reward: -11.68\n",
      "Episode 1013\tLast reward: -16.00\tAverage reward: -11.89\n",
      "Episode 1014\tLast reward: -6.00\tAverage reward: -11.60\n",
      "Episode 1015\tLast reward: -15.00\tAverage reward: -11.77\n",
      "Episode 1016\tLast reward: -7.00\tAverage reward: -11.53\n",
      "Episode 1017\tLast reward: -8.00\tAverage reward: -11.35\n",
      "Episode 1018\tLast reward: -9.00\tAverage reward: -11.24\n",
      "Episode 1019\tLast reward: -9.00\tAverage reward: -11.12\n",
      "Episode 1020\tLast reward: -12.00\tAverage reward: -11.17\n",
      "Episode 1021\tLast reward: -13.00\tAverage reward: -11.26\n",
      "Episode 1022\tLast reward: -17.00\tAverage reward: -11.55\n",
      "Episode 1023\tLast reward: -14.00\tAverage reward: -11.67\n",
      "Episode 1024\tLast reward: -14.00\tAverage reward: -11.79\n",
      "Episode 1025\tLast reward: -15.00\tAverage reward: -11.95\n",
      "Episode 1026\tLast reward: -13.00\tAverage reward: -12.00\n",
      "Episode 1027\tLast reward: -13.00\tAverage reward: -12.05\n",
      "Episode 1028\tLast reward: -12.00\tAverage reward: -12.05\n",
      "Episode 1029\tLast reward: -19.00\tAverage reward: -12.39\n",
      "Episode 1030\tLast reward: -9.00\tAverage reward: -12.22\n",
      "Episode 1031\tLast reward: -17.00\tAverage reward: -12.46\n",
      "Episode 1032\tLast reward: -7.00\tAverage reward: -12.19\n",
      "Episode 1033\tLast reward: -12.00\tAverage reward: -12.18\n",
      "Episode 1034\tLast reward: -13.00\tAverage reward: -12.22\n",
      "Episode 1035\tLast reward: -17.00\tAverage reward: -12.46\n",
      "Episode 1036\tLast reward: -3.00\tAverage reward: -11.99\n",
      "Episode 1037\tLast reward: -12.00\tAverage reward: -11.99\n",
      "Episode 1038\tLast reward: -17.00\tAverage reward: -12.24\n",
      "Episode 1039\tLast reward: -11.00\tAverage reward: -12.18\n",
      "Episode 1040\tLast reward: -11.00\tAverage reward: -12.12\n",
      "Episode 1041\tLast reward: -17.00\tAverage reward: -12.36\n",
      "Episode 1042\tLast reward: -19.00\tAverage reward: -12.69\n",
      "Episode 1043\tLast reward: -11.00\tAverage reward: -12.61\n",
      "Episode 1044\tLast reward: -15.00\tAverage reward: -12.73\n",
      "Episode 1045\tLast reward: -12.00\tAverage reward: -12.69\n",
      "Episode 1046\tLast reward: -9.00\tAverage reward: -12.51\n",
      "Episode 1047\tLast reward: -11.00\tAverage reward: -12.43\n",
      "Episode 1048\tLast reward: -11.00\tAverage reward: -12.36\n",
      "Episode 1049\tLast reward: -12.00\tAverage reward: -12.34\n",
      "Episode 1050\tLast reward: -9.00\tAverage reward: -12.18\n",
      "Episode 1051\tLast reward: -11.00\tAverage reward: -12.12\n",
      "Episode 1052\tLast reward: -12.00\tAverage reward: -12.11\n",
      "Episode 1053\tLast reward: -15.00\tAverage reward: -12.26\n",
      "Episode 1054\tLast reward: -13.00\tAverage reward: -12.29\n",
      "Episode 1055\tLast reward: -9.00\tAverage reward: -12.13\n",
      "Episode 1056\tLast reward: -6.00\tAverage reward: -11.82\n",
      "Episode 1057\tLast reward: -11.00\tAverage reward: -11.78\n",
      "Episode 1058\tLast reward: -13.00\tAverage reward: -11.84\n",
      "Episode 1059\tLast reward: -15.00\tAverage reward: -12.00\n",
      "Episode 1060\tLast reward: -11.00\tAverage reward: -11.95\n",
      "Episode 1061\tLast reward: -17.00\tAverage reward: -12.20\n",
      "Episode 1062\tLast reward: -3.00\tAverage reward: -11.74\n",
      "Episode 1063\tLast reward: -9.00\tAverage reward: -11.60\n",
      "Episode 1064\tLast reward: -7.00\tAverage reward: -11.37\n",
      "Episode 1065\tLast reward: -12.00\tAverage reward: -11.41\n",
      "Episode 1066\tLast reward: -1.00\tAverage reward: -10.89\n",
      "Episode 1067\tLast reward: -9.00\tAverage reward: -10.79\n",
      "Episode 1068\tLast reward: -15.00\tAverage reward: -11.00\n",
      "Episode 1069\tLast reward: -11.00\tAverage reward: -11.00\n",
      "Episode 1070\tLast reward: -17.00\tAverage reward: -11.30\n",
      "Episode 1071\tLast reward: -6.00\tAverage reward: -11.04\n",
      "Episode 1072\tLast reward: -9.00\tAverage reward: -10.93\n",
      "Episode 1073\tLast reward: -7.00\tAverage reward: -10.74\n",
      "Episode 1074\tLast reward: -11.00\tAverage reward: -10.75\n",
      "Episode 1075\tLast reward: -3.00\tAverage reward: -10.36\n",
      "Episode 1076\tLast reward: -15.00\tAverage reward: -10.60\n",
      "Episode 1077\tLast reward: -8.00\tAverage reward: -10.47\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4b8c4974979a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#print(entropy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m#print(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in count(1):\n",
    "    ## Initalizing lists to put the values in them after each episode\n",
    "    action_log_probs = list()\n",
    "    rewards = list()\n",
    "    values = list()\n",
    "    state = env.reset()  # First state (frame) from the env.\n",
    "    ##test = net(torch.FloatTensor(state).unsqueeze(0))\n",
    "    ##print(test)\n",
    "    prev_x = None      # previous frame is none at the beginning of the episode \n",
    "    #print(state.shape)\n",
    "    ##counter = 0\n",
    "    for t in range(100000):\n",
    "    #while True:\n",
    "        #env.render()  ## enable to watch the game but not working with kaggle or colab\n",
    "        \n",
    "        ## Subtracting each frame from the previous one and put them in (state variable) \n",
    "        ## and then set previous one to be the current one for the next loop\n",
    "        ## this will give information on the velocity of the ball when getting the difference between the 2 frames\n",
    "        \n",
    "        cur_x  = preprocess(state)\n",
    "        #print(\"cur_x = \", cur_x.shape)\n",
    "        state  = cur_x - prev_x if prev_x is not None else preprocess(prev_x)##np.zeros(input_shape)\n",
    "        #print(\"state = \", state.shape)\n",
    "        prev_x = cur_x\n",
    "        \n",
    "        ## so the tensor given to the model should be of shape \n",
    "        ## [batch_size, No. of channel, height, width].\n",
    "        ## to be sent to cnn \n",
    "        \n",
    "        state = state.reshape(1, 1, 80, 80)\n",
    "        \n",
    "        ## evaluating the action probabilites from the policy net\n",
    "        action_prob = pnet(torch.FloatTensor(state).to(device))\n",
    "        action = action_prob.sample() ## take an action sampled from a categorical distribution given the state\n",
    "        action_log_probs.append(action_prob.log_prob(action)) # take the log of this action or log prob of this sample\n",
    "        \n",
    "        ## As we only have 2 actions we will map them to the real actions in the action space\n",
    "       \n",
    "        \n",
    "        if action.item() == 0:\n",
    "            action = UP_ACTION   ## the action that will be passed to the env. in the next step is = 2\n",
    "        else: ## if action = 1, 2 possibilites only \n",
    "            action = DOWN_ACTION ## the action that will be passed to the env. in the next step is = 3\n",
    "        \n",
    "        ## evaluating the value from the critic net\n",
    "        \n",
    "        value = cnet(torch.FloatTensor(state).to(device))\n",
    "        values.append(value[0])\n",
    "        \n",
    "        # take a random action depends on the random sampled action \n",
    "        next_state, reward, is_done, _ = env.step(action) ## action = up,down (2,3) \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        ## current state is next state now\n",
    "        state = next_state \n",
    "\n",
    "        if is_done:\n",
    "            #print(rewards)\n",
    "            #print(values)\n",
    "            break\n",
    "            \n",
    "       \n",
    "    ## Calculating the discounted rewards        \n",
    "    returns = discounted_returns(rewards)\n",
    "    \n",
    "    ## Now we have the discounted reward + log_probs of the actions\n",
    "    action_losses = list()\n",
    "    critic_losses = list()\n",
    "    ## collect the action losses and critic losses to a list and calculating our advantage\n",
    "    for ret, l_prob, v in zip(returns, action_log_probs, values):\n",
    "        advantage = ret - v ## advantage here depends on the discounted reward and the value coming from the critic net \n",
    "        #print(advantage)\n",
    "        #print(-l_prob * ret)\n",
    "        action_losses.append(-l_prob * advantage.detach())##calculating the policy -log_prob*Adv.\n",
    "        critic_losses.append(advantage.pow(2)) ## calculating critic loss and we can use also MSELoss to calculate it \n",
    "    \n",
    "    \n",
    "    \n",
    "    p_optimizer.zero_grad() ## make all gradient zeros before taking any step and before making backward to make sure that all gradients are zeros before each step \n",
    "    ## accumulate the action losses\n",
    "    action_loss = torch.cat(action_losses).sum()\n",
    "    # Backward step, calculating gradients\n",
    "    action_loss.backward()\n",
    "    ## step the optimizer, updates the weights of the net \n",
    "    p_optimizer.step()\n",
    "\n",
    "    #repeat the same for the critic net \n",
    "    \n",
    "    c_optimizer.zero_grad()\n",
    "    critic_loss = torch.cat(critic_losses).mean()\n",
    "    critic_loss.backward()\n",
    "    c_optimizer.step()\n",
    "    \n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "    # saving after each 100 epoch    \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "    # save this model when getting a max reward\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "      \n",
    "    if running_reward >= MEAN_REWARD_BOUND:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n",
    "    \n",
    "env.close()\n",
    "\n",
    "\n",
    "            \n",
    "##:.2f\n",
    "##'Episode {}\\t Last reward: {}\\t Average reward: {}'.format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for the last part and introducing the next one \n",
    "\n",
    "As shown in the previous results, the model is fluctuating a lot in the interval of [-10,12]\n",
    "\n",
    "So we start to think how to improve this result also the session of kaggle was endded after 9 hours and we need to load our model starting episode 1000 as shown below \n",
    "\n",
    "We will change the learning rate to be 1e-5 and see what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test the environment\n",
    "## env = gym.make('PongNoFrameskip-v4')\n",
    "env = gym.make('Pong-v0')\n",
    "##input_shape = env.observation_space.shape[0]\n",
    "#action_size = env.action_space.n\n",
    "\n",
    "action_size = 2\n",
    "\n",
    "##print(\"Env reward threshold: {}\".format(env.spec.reward_threshold))\n",
    "reward_list = list()\n",
    "\n",
    "input_shape = [1,80,80]\n",
    "\n",
    "##input_shape = [STACK_SIZE, 84, 84]\n",
    "\n",
    "\n",
    "pnet = torch.load('../input/a2c2options/1000_A2C_pnet.pt').to(device)##,map_location=torch.device('cpu'))\n",
    "pnet.eval()\n",
    "\n",
    "cnet = torch.load('../input/a2c2options/1000_A2C_cnet.pt').to(device)##,map_location=torch.device('cpu'))\n",
    "cnet.eval()\n",
    "\n",
    "\n",
    "## initialize an optimizer\n",
    "p_optimizer = torch.optim.Adam(pnet.parameters(), lr=1e-5,eps=1e-3)\n",
    "c_optimizer = torch.optim.Adam(cnet.parameters(), lr=1e-5,eps=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "running_reward  = None\n",
    "MEAN_REWARD_BOUND = 20\n",
    "\n",
    "UP_ACTION = 2\n",
    "\n",
    "DOWN_ACTION = 3\n",
    "Max_ep_rewared = -21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tLast reward: -13.00\tAverage reward: -13.00\n",
      "Episode 2\tLast reward: -13.00\tAverage reward: -13.00\n",
      "Episode 3\tLast reward: -15.00\tAverage reward: -13.10\n",
      "Episode 4\tLast reward: -15.00\tAverage reward: -13.19\n",
      "Episode 5\tLast reward: -10.00\tAverage reward: -13.04\n",
      "Episode 6\tLast reward: -2.00\tAverage reward: -12.48\n",
      "Episode 7\tLast reward: -10.00\tAverage reward: -12.36\n",
      "Episode 8\tLast reward: -15.00\tAverage reward: -12.49\n",
      "Episode 9\tLast reward: -12.00\tAverage reward: -12.47\n",
      "Episode 10\tLast reward: -7.00\tAverage reward: -12.19\n",
      "Episode 11\tLast reward: -13.00\tAverage reward: -12.23\n",
      "Episode 12\tLast reward: -15.00\tAverage reward: -12.37\n",
      "Episode 13\tLast reward: -14.00\tAverage reward: -12.45\n",
      "Episode 14\tLast reward: -11.00\tAverage reward: -12.38\n",
      "Episode 15\tLast reward: -11.00\tAverage reward: -12.31\n",
      "Episode 16\tLast reward: -14.00\tAverage reward: -12.40\n",
      "Episode 17\tLast reward: -10.00\tAverage reward: -12.28\n",
      "Episode 18\tLast reward: -12.00\tAverage reward: -12.26\n",
      "Episode 19\tLast reward: -17.00\tAverage reward: -12.50\n",
      "Episode 20\tLast reward: -15.00\tAverage reward: -12.62\n",
      "Episode 21\tLast reward: -7.00\tAverage reward: -12.34\n",
      "Episode 22\tLast reward: -11.00\tAverage reward: -12.28\n",
      "Episode 23\tLast reward: -13.00\tAverage reward: -12.31\n",
      "Episode 24\tLast reward: -4.00\tAverage reward: -11.90\n",
      "Episode 25\tLast reward: -9.00\tAverage reward: -11.75\n",
      "Episode 26\tLast reward: -10.00\tAverage reward: -11.66\n",
      "Episode 27\tLast reward: -11.00\tAverage reward: -11.63\n",
      "Episode 28\tLast reward: -10.00\tAverage reward: -11.55\n",
      "Episode 29\tLast reward: -11.00\tAverage reward: -11.52\n",
      "Episode 30\tLast reward: -14.00\tAverage reward: -11.65\n",
      "Episode 31\tLast reward: -3.00\tAverage reward: -11.21\n",
      "Episode 32\tLast reward: -11.00\tAverage reward: -11.20\n",
      "Episode 33\tLast reward: -11.00\tAverage reward: -11.19\n",
      "Episode 34\tLast reward: -15.00\tAverage reward: -11.38\n",
      "Episode 35\tLast reward: -16.00\tAverage reward: -11.61\n",
      "Episode 36\tLast reward: -17.00\tAverage reward: -11.88\n",
      "Episode 37\tLast reward: -10.00\tAverage reward: -11.79\n",
      "Episode 38\tLast reward: -7.00\tAverage reward: -11.55\n",
      "Episode 39\tLast reward: -14.00\tAverage reward: -11.67\n",
      "Episode 40\tLast reward: -13.00\tAverage reward: -11.74\n",
      "Episode 41\tLast reward: -15.00\tAverage reward: -11.90\n",
      "Episode 42\tLast reward: -16.00\tAverage reward: -12.11\n",
      "Episode 43\tLast reward: -5.00\tAverage reward: -11.75\n",
      "Episode 44\tLast reward: -13.00\tAverage reward: -11.81\n",
      "Episode 45\tLast reward: -15.00\tAverage reward: -11.97\n",
      "Episode 46\tLast reward: -7.00\tAverage reward: -11.72\n",
      "Episode 47\tLast reward: -10.00\tAverage reward: -11.64\n",
      "Episode 48\tLast reward: -12.00\tAverage reward: -11.66\n",
      "Episode 49\tLast reward: -16.00\tAverage reward: -11.87\n",
      "Episode 50\tLast reward: -10.00\tAverage reward: -11.78\n",
      "Episode 51\tLast reward: -17.00\tAverage reward: -12.04\n",
      "Episode 52\tLast reward: -11.00\tAverage reward: -11.99\n",
      "Episode 53\tLast reward: -14.00\tAverage reward: -12.09\n",
      "Episode 54\tLast reward: -11.00\tAverage reward: -12.03\n",
      "Episode 55\tLast reward: -18.00\tAverage reward: -12.33\n",
      "Episode 56\tLast reward: -17.00\tAverage reward: -12.57\n",
      "Episode 57\tLast reward: -13.00\tAverage reward: -12.59\n",
      "Episode 58\tLast reward: -6.00\tAverage reward: -12.26\n",
      "Episode 59\tLast reward: -9.00\tAverage reward: -12.10\n",
      "Episode 60\tLast reward: -18.00\tAverage reward: -12.39\n",
      "Episode 61\tLast reward: -9.00\tAverage reward: -12.22\n",
      "Episode 62\tLast reward: -8.00\tAverage reward: -12.01\n",
      "Episode 63\tLast reward: -17.00\tAverage reward: -12.26\n",
      "Episode 64\tLast reward: -11.00\tAverage reward: -12.20\n",
      "Episode 65\tLast reward: -7.00\tAverage reward: -11.94\n",
      "Episode 66\tLast reward: -17.00\tAverage reward: -12.19\n",
      "Episode 67\tLast reward: -12.00\tAverage reward: -12.18\n",
      "Episode 68\tLast reward: -1.00\tAverage reward: -11.62\n",
      "Episode 69\tLast reward: -13.00\tAverage reward: -11.69\n",
      "Episode 70\tLast reward: -15.00\tAverage reward: -11.86\n",
      "Episode 71\tLast reward: -19.00\tAverage reward: -12.21\n",
      "Episode 72\tLast reward: -14.00\tAverage reward: -12.30\n",
      "Episode 73\tLast reward: -9.00\tAverage reward: -12.14\n",
      "Episode 74\tLast reward: -7.00\tAverage reward: -11.88\n",
      "Episode 75\tLast reward: -9.00\tAverage reward: -11.74\n",
      "Episode 76\tLast reward: -17.00\tAverage reward: -12.00\n",
      "Episode 77\tLast reward: -16.00\tAverage reward: -12.20\n",
      "Episode 78\tLast reward: -15.00\tAverage reward: -12.34\n",
      "Episode 79\tLast reward: -13.00\tAverage reward: -12.37\n",
      "Episode 80\tLast reward: -14.00\tAverage reward: -12.45\n",
      "Episode 81\tLast reward: -11.00\tAverage reward: -12.38\n",
      "Episode 82\tLast reward: -11.00\tAverage reward: -12.31\n",
      "Episode 83\tLast reward: -17.00\tAverage reward: -12.55\n",
      "Episode 84\tLast reward: -14.00\tAverage reward: -12.62\n",
      "Episode 85\tLast reward: -18.00\tAverage reward: -12.89\n",
      "Episode 86\tLast reward: -13.00\tAverage reward: -12.89\n",
      "Episode 87\tLast reward: -15.00\tAverage reward: -13.00\n",
      "Episode 88\tLast reward: -12.00\tAverage reward: -12.95\n",
      "Episode 89\tLast reward: -19.00\tAverage reward: -13.25\n",
      "Episode 90\tLast reward: -12.00\tAverage reward: -13.19\n",
      "Episode 91\tLast reward: -13.00\tAverage reward: -13.18\n",
      "Episode 92\tLast reward: -12.00\tAverage reward: -13.12\n",
      "Episode 93\tLast reward: -9.00\tAverage reward: -12.91\n",
      "Episode 94\tLast reward: -13.00\tAverage reward: -12.92\n",
      "Episode 95\tLast reward: -12.00\tAverage reward: -12.87\n",
      "Episode 96\tLast reward: -14.00\tAverage reward: -12.93\n",
      "Episode 97\tLast reward: -14.00\tAverage reward: -12.98\n",
      "Episode 98\tLast reward: -10.00\tAverage reward: -12.83\n",
      "Episode 99\tLast reward: -9.00\tAverage reward: -12.64\n",
      "Episode 100\tLast reward: -14.00\tAverage reward: -12.71\n",
      "Episode 101\tLast reward: -10.00\tAverage reward: -12.57\n",
      "Episode 102\tLast reward: -13.00\tAverage reward: -12.60\n",
      "Episode 103\tLast reward: -13.00\tAverage reward: -12.62\n",
      "Episode 104\tLast reward: -10.00\tAverage reward: -12.49\n",
      "Episode 105\tLast reward: -8.00\tAverage reward: -12.26\n",
      "Episode 106\tLast reward: -11.00\tAverage reward: -12.20\n",
      "Episode 107\tLast reward: -14.00\tAverage reward: -12.29\n",
      "Episode 108\tLast reward: -14.00\tAverage reward: -12.37\n",
      "Episode 109\tLast reward: -17.00\tAverage reward: -12.60\n",
      "Episode 110\tLast reward: -9.00\tAverage reward: -12.42\n",
      "Episode 111\tLast reward: -15.00\tAverage reward: -12.55\n",
      "Episode 112\tLast reward: -15.00\tAverage reward: -12.68\n",
      "Episode 113\tLast reward: -9.00\tAverage reward: -12.49\n",
      "Episode 114\tLast reward: -15.00\tAverage reward: -12.62\n",
      "Episode 115\tLast reward: -14.00\tAverage reward: -12.69\n",
      "Episode 116\tLast reward: -11.00\tAverage reward: -12.60\n",
      "Episode 117\tLast reward: -13.00\tAverage reward: -12.62\n",
      "Episode 118\tLast reward: -11.00\tAverage reward: -12.54\n",
      "Episode 119\tLast reward: -18.00\tAverage reward: -12.81\n",
      "Episode 120\tLast reward: -9.00\tAverage reward: -12.62\n",
      "Episode 121\tLast reward: -14.00\tAverage reward: -12.69\n",
      "Episode 122\tLast reward: -15.00\tAverage reward: -12.81\n",
      "Episode 123\tLast reward: -13.00\tAverage reward: -12.82\n",
      "Episode 124\tLast reward: -15.00\tAverage reward: -12.93\n",
      "Episode 125\tLast reward: -14.00\tAverage reward: -12.98\n",
      "Episode 126\tLast reward: -5.00\tAverage reward: -12.58\n",
      "Episode 127\tLast reward: -16.00\tAverage reward: -12.75\n",
      "Episode 128\tLast reward: -13.00\tAverage reward: -12.76\n",
      "Episode 129\tLast reward: -17.00\tAverage reward: -12.98\n",
      "Episode 130\tLast reward: -16.00\tAverage reward: -13.13\n",
      "Episode 131\tLast reward: -15.00\tAverage reward: -13.22\n",
      "Episode 132\tLast reward: -15.00\tAverage reward: -13.31\n",
      "Episode 133\tLast reward: -10.00\tAverage reward: -13.14\n",
      "Episode 134\tLast reward: -9.00\tAverage reward: -12.94\n",
      "Episode 135\tLast reward: -13.00\tAverage reward: -12.94\n",
      "Episode 136\tLast reward: -8.00\tAverage reward: -12.69\n",
      "Episode 137\tLast reward: -8.00\tAverage reward: -12.46\n",
      "Episode 138\tLast reward: -11.00\tAverage reward: -12.39\n",
      "Episode 139\tLast reward: -14.00\tAverage reward: -12.47\n",
      "Episode 140\tLast reward: -11.00\tAverage reward: -12.39\n",
      "Episode 141\tLast reward: -5.00\tAverage reward: -12.02\n",
      "Episode 142\tLast reward: -17.00\tAverage reward: -12.27\n",
      "Episode 143\tLast reward: -14.00\tAverage reward: -12.36\n",
      "Episode 144\tLast reward: -6.00\tAverage reward: -12.04\n",
      "Episode 145\tLast reward: -6.00\tAverage reward: -11.74\n",
      "Episode 146\tLast reward: -12.00\tAverage reward: -11.75\n",
      "Episode 147\tLast reward: -9.00\tAverage reward: -11.61\n",
      "Episode 148\tLast reward: -9.00\tAverage reward: -11.48\n",
      "Episode 149\tLast reward: -17.00\tAverage reward: -11.76\n",
      "Episode 150\tLast reward: -17.00\tAverage reward: -12.02\n",
      "Episode 151\tLast reward: -18.00\tAverage reward: -12.32\n",
      "Episode 152\tLast reward: -6.00\tAverage reward: -12.00\n",
      "Episode 153\tLast reward: -14.00\tAverage reward: -12.10\n",
      "Episode 154\tLast reward: -11.00\tAverage reward: -12.05\n",
      "Episode 155\tLast reward: -11.00\tAverage reward: -12.00\n",
      "Episode 156\tLast reward: -8.00\tAverage reward: -11.80\n",
      "Episode 157\tLast reward: -9.00\tAverage reward: -11.66\n",
      "Episode 158\tLast reward: -7.00\tAverage reward: -11.42\n",
      "Episode 159\tLast reward: -4.00\tAverage reward: -11.05\n",
      "Episode 160\tLast reward: -13.00\tAverage reward: -11.15\n",
      "Episode 161\tLast reward: -5.00\tAverage reward: -10.84\n",
      "Episode 162\tLast reward: 5.00\tAverage reward: -10.05\n",
      "Episode 163\tLast reward: -19.00\tAverage reward: -10.50\n",
      "Episode 164\tLast reward: -14.00\tAverage reward: -10.67\n",
      "Episode 165\tLast reward: -3.00\tAverage reward: -10.29\n",
      "Episode 166\tLast reward: -14.00\tAverage reward: -10.47\n",
      "Episode 167\tLast reward: -15.00\tAverage reward: -10.70\n",
      "Episode 168\tLast reward: -12.00\tAverage reward: -10.77\n",
      "Episode 169\tLast reward: -10.00\tAverage reward: -10.73\n",
      "Episode 170\tLast reward: -13.00\tAverage reward: -10.84\n",
      "Episode 171\tLast reward: -18.00\tAverage reward: -11.20\n",
      "Episode 172\tLast reward: -11.00\tAverage reward: -11.19\n",
      "Episode 173\tLast reward: -13.00\tAverage reward: -11.28\n",
      "Episode 174\tLast reward: -13.00\tAverage reward: -11.37\n",
      "Episode 175\tLast reward: -13.00\tAverage reward: -11.45\n",
      "Episode 176\tLast reward: -11.00\tAverage reward: -11.43\n",
      "Episode 177\tLast reward: -8.00\tAverage reward: -11.25\n",
      "Episode 178\tLast reward: -13.00\tAverage reward: -11.34\n",
      "Episode 179\tLast reward: -14.00\tAverage reward: -11.47\n",
      "Episode 180\tLast reward: -14.00\tAverage reward: -11.60\n",
      "Episode 181\tLast reward: -16.00\tAverage reward: -11.82\n",
      "Episode 182\tLast reward: -16.00\tAverage reward: -12.03\n",
      "Episode 183\tLast reward: -9.00\tAverage reward: -11.88\n",
      "Episode 184\tLast reward: -13.00\tAverage reward: -11.93\n",
      "Episode 185\tLast reward: -15.00\tAverage reward: -12.09\n",
      "Episode 186\tLast reward: -11.00\tAverage reward: -12.03\n",
      "Episode 187\tLast reward: -13.00\tAverage reward: -12.08\n",
      "Episode 188\tLast reward: -5.00\tAverage reward: -11.73\n",
      "Episode 189\tLast reward: -13.00\tAverage reward: -11.79\n",
      "Episode 190\tLast reward: -7.00\tAverage reward: -11.55\n",
      "Episode 191\tLast reward: -16.00\tAverage reward: -11.77\n",
      "Episode 192\tLast reward: -15.00\tAverage reward: -11.94\n",
      "Episode 193\tLast reward: -12.00\tAverage reward: -11.94\n",
      "Episode 194\tLast reward: -20.00\tAverage reward: -12.34\n",
      "Episode 195\tLast reward: -14.00\tAverage reward: -12.42\n",
      "Episode 196\tLast reward: -13.00\tAverage reward: -12.45\n",
      "Episode 197\tLast reward: -13.00\tAverage reward: -12.48\n",
      "Episode 198\tLast reward: -10.00\tAverage reward: -12.36\n",
      "Episode 199\tLast reward: -5.00\tAverage reward: -11.99\n",
      "Episode 200\tLast reward: -18.00\tAverage reward: -12.29\n",
      "Episode 201\tLast reward: -10.00\tAverage reward: -12.17\n",
      "Episode 202\tLast reward: -18.00\tAverage reward: -12.47\n",
      "Episode 203\tLast reward: -13.00\tAverage reward: -12.49\n",
      "Episode 204\tLast reward: -13.00\tAverage reward: -12.52\n",
      "Episode 205\tLast reward: -11.00\tAverage reward: -12.44\n",
      "Episode 206\tLast reward: -13.00\tAverage reward: -12.47\n",
      "Episode 207\tLast reward: -13.00\tAverage reward: -12.50\n",
      "Episode 208\tLast reward: -15.00\tAverage reward: -12.62\n",
      "Episode 209\tLast reward: -6.00\tAverage reward: -12.29\n",
      "Episode 210\tLast reward: -15.00\tAverage reward: -12.43\n",
      "Episode 211\tLast reward: -18.00\tAverage reward: -12.70\n",
      "Episode 212\tLast reward: -21.00\tAverage reward: -13.12\n",
      "Episode 213\tLast reward: -19.00\tAverage reward: -13.41\n",
      "Episode 214\tLast reward: -6.00\tAverage reward: -13.04\n",
      "Episode 215\tLast reward: -3.00\tAverage reward: -12.54\n",
      "Episode 216\tLast reward: -16.00\tAverage reward: -12.71\n",
      "Episode 217\tLast reward: -9.00\tAverage reward: -12.53\n",
      "Episode 218\tLast reward: -19.00\tAverage reward: -12.85\n",
      "Episode 219\tLast reward: -11.00\tAverage reward: -12.76\n",
      "Episode 220\tLast reward: -9.00\tAverage reward: -12.57\n",
      "Episode 221\tLast reward: -10.00\tAverage reward: -12.44\n",
      "Episode 222\tLast reward: -9.00\tAverage reward: -12.27\n",
      "Episode 223\tLast reward: -17.00\tAverage reward: -12.51\n",
      "Episode 224\tLast reward: -15.00\tAverage reward: -12.63\n",
      "Episode 225\tLast reward: -8.00\tAverage reward: -12.40\n",
      "Episode 226\tLast reward: -9.00\tAverage reward: -12.23\n",
      "Episode 227\tLast reward: -12.00\tAverage reward: -12.22\n",
      "Episode 228\tLast reward: -10.00\tAverage reward: -12.11\n",
      "Episode 229\tLast reward: -11.00\tAverage reward: -12.05\n",
      "Episode 230\tLast reward: -11.00\tAverage reward: -12.00\n",
      "Episode 231\tLast reward: -19.00\tAverage reward: -12.35\n",
      "Episode 232\tLast reward: -14.00\tAverage reward: -12.43\n",
      "Episode 233\tLast reward: -18.00\tAverage reward: -12.71\n",
      "Episode 234\tLast reward: -7.00\tAverage reward: -12.42\n",
      "Episode 235\tLast reward: -6.00\tAverage reward: -12.10\n",
      "Episode 236\tLast reward: -15.00\tAverage reward: -12.25\n",
      "Episode 237\tLast reward: -15.00\tAverage reward: -12.39\n",
      "Episode 238\tLast reward: -9.00\tAverage reward: -12.22\n",
      "Episode 239\tLast reward: -13.00\tAverage reward: -12.26\n",
      "Episode 240\tLast reward: -16.00\tAverage reward: -12.44\n",
      "Episode 241\tLast reward: -9.00\tAverage reward: -12.27\n",
      "Episode 242\tLast reward: -14.00\tAverage reward: -12.36\n",
      "Episode 243\tLast reward: -8.00\tAverage reward: -12.14\n",
      "Episode 244\tLast reward: -15.00\tAverage reward: -12.28\n",
      "Episode 245\tLast reward: -13.00\tAverage reward: -12.32\n",
      "Episode 246\tLast reward: -9.00\tAverage reward: -12.15\n",
      "Episode 247\tLast reward: -13.00\tAverage reward: -12.19\n",
      "Episode 248\tLast reward: -11.00\tAverage reward: -12.14\n",
      "Episode 249\tLast reward: -2.00\tAverage reward: -11.63\n",
      "Episode 250\tLast reward: 1.00\tAverage reward: -11.00\n",
      "Episode 251\tLast reward: -11.00\tAverage reward: -11.00\n",
      "Episode 252\tLast reward: 1.00\tAverage reward: -10.40\n",
      "Episode 253\tLast reward: -15.00\tAverage reward: -10.63\n",
      "Episode 254\tLast reward: -17.00\tAverage reward: -10.95\n",
      "Episode 255\tLast reward: -18.00\tAverage reward: -11.30\n",
      "Episode 256\tLast reward: -15.00\tAverage reward: -11.48\n",
      "Episode 257\tLast reward: -12.00\tAverage reward: -11.51\n",
      "Episode 258\tLast reward: -15.00\tAverage reward: -11.68\n",
      "Episode 259\tLast reward: -13.00\tAverage reward: -11.75\n",
      "Episode 260\tLast reward: -6.00\tAverage reward: -11.46\n",
      "Episode 261\tLast reward: -11.00\tAverage reward: -11.44\n",
      "Episode 262\tLast reward: -18.00\tAverage reward: -11.77\n",
      "Episode 263\tLast reward: -7.00\tAverage reward: -11.53\n",
      "Episode 264\tLast reward: -3.00\tAverage reward: -11.10\n",
      "Episode 265\tLast reward: 6.00\tAverage reward: -10.25\n",
      "Episode 266\tLast reward: -9.00\tAverage reward: -10.19\n",
      "Episode 267\tLast reward: -18.00\tAverage reward: -10.58\n",
      "Episode 268\tLast reward: -14.00\tAverage reward: -10.75\n",
      "Episode 269\tLast reward: -15.00\tAverage reward: -10.96\n",
      "Episode 270\tLast reward: -11.00\tAverage reward: -10.96\n",
      "Episode 271\tLast reward: -15.00\tAverage reward: -11.16\n",
      "Episode 272\tLast reward: -8.00\tAverage reward: -11.01\n",
      "Episode 273\tLast reward: -10.00\tAverage reward: -10.96\n",
      "Episode 274\tLast reward: -17.00\tAverage reward: -11.26\n",
      "Episode 275\tLast reward: -10.00\tAverage reward: -11.19\n",
      "Episode 276\tLast reward: -14.00\tAverage reward: -11.33\n",
      "Episode 277\tLast reward: -7.00\tAverage reward: -11.12\n",
      "Episode 278\tLast reward: -13.00\tAverage reward: -11.21\n",
      "Episode 279\tLast reward: -17.00\tAverage reward: -11.50\n",
      "Episode 280\tLast reward: -17.00\tAverage reward: -11.78\n",
      "Episode 281\tLast reward: -8.00\tAverage reward: -11.59\n",
      "Episode 282\tLast reward: -2.00\tAverage reward: -11.11\n",
      "Episode 283\tLast reward: -15.00\tAverage reward: -11.30\n",
      "Episode 284\tLast reward: -14.00\tAverage reward: -11.44\n",
      "Episode 285\tLast reward: -3.00\tAverage reward: -11.02\n",
      "Episode 286\tLast reward: -1.00\tAverage reward: -10.52\n",
      "Episode 287\tLast reward: -19.00\tAverage reward: -10.94\n",
      "Episode 288\tLast reward: -8.00\tAverage reward: -10.79\n",
      "Episode 289\tLast reward: -18.00\tAverage reward: -11.15\n",
      "Episode 290\tLast reward: -11.00\tAverage reward: -11.15\n",
      "Episode 291\tLast reward: -17.00\tAverage reward: -11.44\n",
      "Episode 292\tLast reward: -15.00\tAverage reward: -11.62\n",
      "Episode 293\tLast reward: -16.00\tAverage reward: -11.84\n",
      "Episode 294\tLast reward: -19.00\tAverage reward: -12.19\n",
      "Episode 295\tLast reward: -15.00\tAverage reward: -12.33\n",
      "Episode 296\tLast reward: -9.00\tAverage reward: -12.17\n",
      "Episode 297\tLast reward: -15.00\tAverage reward: -12.31\n",
      "Episode 298\tLast reward: -15.00\tAverage reward: -12.44\n",
      "Episode 299\tLast reward: -15.00\tAverage reward: -12.57\n",
      "Episode 300\tLast reward: -11.00\tAverage reward: -12.49\n",
      "Episode 301\tLast reward: -9.00\tAverage reward: -12.32\n",
      "Episode 302\tLast reward: -13.00\tAverage reward: -12.35\n",
      "Episode 303\tLast reward: -17.00\tAverage reward: -12.58\n",
      "Episode 304\tLast reward: -10.00\tAverage reward: -12.46\n",
      "Episode 305\tLast reward: -14.00\tAverage reward: -12.53\n",
      "Episode 306\tLast reward: -6.00\tAverage reward: -12.21\n",
      "Episode 307\tLast reward: -14.00\tAverage reward: -12.30\n",
      "Episode 308\tLast reward: -13.00\tAverage reward: -12.33\n",
      "Episode 309\tLast reward: -5.00\tAverage reward: -11.96\n",
      "Episode 310\tLast reward: -4.00\tAverage reward: -11.57\n",
      "Episode 311\tLast reward: -13.00\tAverage reward: -11.64\n",
      "Episode 312\tLast reward: -10.00\tAverage reward: -11.56\n",
      "Episode 313\tLast reward: -10.00\tAverage reward: -11.48\n",
      "Episode 314\tLast reward: -7.00\tAverage reward: -11.25\n",
      "Episode 315\tLast reward: -11.00\tAverage reward: -11.24\n",
      "Episode 316\tLast reward: -11.00\tAverage reward: -11.23\n",
      "Episode 317\tLast reward: -13.00\tAverage reward: -11.32\n",
      "Episode 318\tLast reward: -8.00\tAverage reward: -11.15\n",
      "Episode 319\tLast reward: -7.00\tAverage reward: -10.94\n",
      "Episode 320\tLast reward: -8.00\tAverage reward: -10.80\n",
      "Episode 321\tLast reward: -15.00\tAverage reward: -11.01\n",
      "Episode 322\tLast reward: -11.00\tAverage reward: -11.01\n",
      "Episode 323\tLast reward: -7.00\tAverage reward: -10.81\n",
      "Episode 324\tLast reward: -15.00\tAverage reward: -11.02\n",
      "Episode 325\tLast reward: -7.00\tAverage reward: -10.82\n",
      "Episode 326\tLast reward: -5.00\tAverage reward: -10.52\n",
      "Episode 327\tLast reward: -6.00\tAverage reward: -10.30\n",
      "Episode 328\tLast reward: -11.00\tAverage reward: -10.33\n",
      "Episode 329\tLast reward: -15.00\tAverage reward: -10.57\n",
      "Episode 330\tLast reward: -13.00\tAverage reward: -10.69\n",
      "Episode 331\tLast reward: -13.00\tAverage reward: -10.80\n",
      "Episode 332\tLast reward: -15.00\tAverage reward: -11.01\n",
      "Episode 333\tLast reward: -8.00\tAverage reward: -10.86\n",
      "Episode 334\tLast reward: -11.00\tAverage reward: -10.87\n",
      "Episode 335\tLast reward: -5.00\tAverage reward: -10.58\n",
      "Episode 336\tLast reward: -17.00\tAverage reward: -10.90\n",
      "Episode 337\tLast reward: -15.00\tAverage reward: -11.10\n",
      "Episode 338\tLast reward: -14.00\tAverage reward: -11.25\n",
      "Episode 339\tLast reward: -12.00\tAverage reward: -11.29\n",
      "Episode 340\tLast reward: -11.00\tAverage reward: -11.27\n",
      "Episode 341\tLast reward: -12.00\tAverage reward: -11.31\n",
      "Episode 342\tLast reward: -7.00\tAverage reward: -11.09\n",
      "Episode 343\tLast reward: -3.00\tAverage reward: -10.69\n",
      "Episode 344\tLast reward: -12.00\tAverage reward: -10.75\n",
      "Episode 345\tLast reward: -15.00\tAverage reward: -10.97\n",
      "Episode 346\tLast reward: -1.00\tAverage reward: -10.47\n",
      "Episode 347\tLast reward: -15.00\tAverage reward: -10.69\n",
      "Episode 348\tLast reward: -17.00\tAverage reward: -11.01\n",
      "Episode 349\tLast reward: -10.00\tAverage reward: -10.96\n",
      "Episode 350\tLast reward: -9.00\tAverage reward: -10.86\n",
      "Episode 351\tLast reward: -15.00\tAverage reward: -11.07\n",
      "Episode 352\tLast reward: -16.00\tAverage reward: -11.31\n",
      "Episode 353\tLast reward: -11.00\tAverage reward: -11.30\n",
      "Episode 354\tLast reward: -9.00\tAverage reward: -11.18\n",
      "Episode 355\tLast reward: -11.00\tAverage reward: -11.17\n",
      "Episode 356\tLast reward: -4.00\tAverage reward: -10.82\n",
      "Episode 357\tLast reward: -15.00\tAverage reward: -11.02\n",
      "Episode 358\tLast reward: -11.00\tAverage reward: -11.02\n",
      "Episode 359\tLast reward: -13.00\tAverage reward: -11.12\n",
      "Episode 360\tLast reward: -14.00\tAverage reward: -11.27\n",
      "Episode 361\tLast reward: -15.00\tAverage reward: -11.45\n",
      "Episode 362\tLast reward: -13.00\tAverage reward: -11.53\n",
      "Episode 363\tLast reward: -11.00\tAverage reward: -11.50\n",
      "Episode 364\tLast reward: -7.00\tAverage reward: -11.28\n",
      "Episode 365\tLast reward: -5.00\tAverage reward: -10.96\n",
      "Episode 366\tLast reward: -17.00\tAverage reward: -11.27\n",
      "Episode 367\tLast reward: -14.00\tAverage reward: -11.40\n",
      "Episode 368\tLast reward: -12.00\tAverage reward: -11.43\n",
      "Episode 369\tLast reward: -13.00\tAverage reward: -11.51\n",
      "Episode 370\tLast reward: -16.00\tAverage reward: -11.74\n",
      "Episode 371\tLast reward: -16.00\tAverage reward: -11.95\n",
      "Episode 372\tLast reward: -9.00\tAverage reward: -11.80\n",
      "Episode 373\tLast reward: -9.00\tAverage reward: -11.66\n",
      "Episode 374\tLast reward: -11.00\tAverage reward: -11.63\n",
      "Episode 375\tLast reward: -9.00\tAverage reward: -11.50\n",
      "Episode 376\tLast reward: -12.00\tAverage reward: -11.52\n",
      "Episode 377\tLast reward: 2.00\tAverage reward: -10.85\n",
      "Episode 378\tLast reward: -13.00\tAverage reward: -10.95\n",
      "Episode 379\tLast reward: -7.00\tAverage reward: -10.76\n",
      "Episode 380\tLast reward: -10.00\tAverage reward: -10.72\n",
      "Episode 381\tLast reward: -14.00\tAverage reward: -10.88\n",
      "Episode 382\tLast reward: -17.00\tAverage reward: -11.19\n",
      "Episode 383\tLast reward: -19.00\tAverage reward: -11.58\n",
      "Episode 384\tLast reward: -14.00\tAverage reward: -11.70\n",
      "Episode 385\tLast reward: -10.00\tAverage reward: -11.61\n",
      "Episode 386\tLast reward: -14.00\tAverage reward: -11.73\n",
      "Episode 387\tLast reward: -13.00\tAverage reward: -11.80\n",
      "Episode 388\tLast reward: -9.00\tAverage reward: -11.66\n",
      "Episode 389\tLast reward: -5.00\tAverage reward: -11.32\n",
      "Episode 390\tLast reward: -15.00\tAverage reward: -11.51\n",
      "Episode 391\tLast reward: -13.00\tAverage reward: -11.58\n",
      "Episode 392\tLast reward: -15.00\tAverage reward: -11.75\n",
      "Episode 393\tLast reward: -15.00\tAverage reward: -11.92\n",
      "Episode 394\tLast reward: -5.00\tAverage reward: -11.57\n",
      "Episode 395\tLast reward: -11.00\tAverage reward: -11.54\n",
      "Episode 396\tLast reward: -12.00\tAverage reward: -11.56\n",
      "Episode 397\tLast reward: -11.00\tAverage reward: -11.54\n",
      "Episode 398\tLast reward: -10.00\tAverage reward: -11.46\n",
      "Episode 399\tLast reward: -15.00\tAverage reward: -11.64\n",
      "Episode 400\tLast reward: -11.00\tAverage reward: -11.60\n",
      "Episode 401\tLast reward: -13.00\tAverage reward: -11.67\n",
      "Episode 402\tLast reward: -13.00\tAverage reward: -11.74\n",
      "Episode 403\tLast reward: -14.00\tAverage reward: -11.85\n",
      "Episode 404\tLast reward: -15.00\tAverage reward: -12.01\n",
      "Episode 405\tLast reward: -7.00\tAverage reward: -11.76\n",
      "Episode 406\tLast reward: -13.00\tAverage reward: -11.82\n",
      "Episode 407\tLast reward: -15.00\tAverage reward: -11.98\n",
      "Episode 408\tLast reward: -8.00\tAverage reward: -11.78\n",
      "Episode 409\tLast reward: -9.00\tAverage reward: -11.64\n",
      "Episode 410\tLast reward: -9.00\tAverage reward: -11.51\n",
      "Episode 411\tLast reward: -4.00\tAverage reward: -11.14\n",
      "Episode 412\tLast reward: -11.00\tAverage reward: -11.13\n",
      "Episode 413\tLast reward: -15.00\tAverage reward: -11.32\n",
      "Episode 414\tLast reward: -7.00\tAverage reward: -11.11\n",
      "Episode 415\tLast reward: -15.00\tAverage reward: -11.30\n",
      "Episode 416\tLast reward: -2.00\tAverage reward: -10.84\n",
      "Episode 417\tLast reward: -12.00\tAverage reward: -10.89\n",
      "Episode 418\tLast reward: -8.00\tAverage reward: -10.75\n",
      "Episode 419\tLast reward: -7.00\tAverage reward: -10.56\n",
      "Episode 420\tLast reward: -13.00\tAverage reward: -10.68\n",
      "Episode 421\tLast reward: -13.00\tAverage reward: -10.80\n",
      "Episode 422\tLast reward: -13.00\tAverage reward: -10.91\n",
      "Episode 423\tLast reward: -12.00\tAverage reward: -10.96\n",
      "Episode 424\tLast reward: -13.00\tAverage reward: -11.07\n",
      "Episode 425\tLast reward: -10.00\tAverage reward: -11.01\n",
      "Episode 426\tLast reward: -10.00\tAverage reward: -10.96\n",
      "Episode 427\tLast reward: -1.00\tAverage reward: -10.46\n",
      "Episode 428\tLast reward: -5.00\tAverage reward: -10.19\n",
      "Episode 429\tLast reward: -10.00\tAverage reward: -10.18\n",
      "Episode 430\tLast reward: -13.00\tAverage reward: -10.32\n",
      "Episode 431\tLast reward: -5.00\tAverage reward: -10.06\n",
      "Episode 432\tLast reward: -12.00\tAverage reward: -10.15\n",
      "Episode 433\tLast reward: -13.00\tAverage reward: -10.30\n",
      "Episode 434\tLast reward: -11.00\tAverage reward: -10.33\n",
      "Episode 435\tLast reward: -11.00\tAverage reward: -10.36\n",
      "Episode 436\tLast reward: -7.00\tAverage reward: -10.20\n",
      "Episode 437\tLast reward: -16.00\tAverage reward: -10.49\n",
      "Episode 438\tLast reward: -10.00\tAverage reward: -10.46\n",
      "Episode 439\tLast reward: -13.00\tAverage reward: -10.59\n",
      "Episode 440\tLast reward: -7.00\tAverage reward: -10.41\n",
      "Episode 441\tLast reward: -15.00\tAverage reward: -10.64\n",
      "Episode 442\tLast reward: -5.00\tAverage reward: -10.36\n",
      "Episode 443\tLast reward: -15.00\tAverage reward: -10.59\n",
      "Episode 444\tLast reward: -9.00\tAverage reward: -10.51\n",
      "Episode 445\tLast reward: -12.00\tAverage reward: -10.58\n",
      "Episode 446\tLast reward: -17.00\tAverage reward: -10.90\n",
      "Episode 447\tLast reward: -11.00\tAverage reward: -10.91\n",
      "Episode 448\tLast reward: -7.00\tAverage reward: -10.71\n",
      "Episode 449\tLast reward: -9.00\tAverage reward: -10.63\n",
      "Episode 450\tLast reward: -9.00\tAverage reward: -10.55\n",
      "Episode 451\tLast reward: -10.00\tAverage reward: -10.52\n",
      "Episode 452\tLast reward: -13.00\tAverage reward: -10.64\n",
      "Episode 453\tLast reward: -6.00\tAverage reward: -10.41\n",
      "Episode 454\tLast reward: -12.00\tAverage reward: -10.49\n",
      "Episode 455\tLast reward: -5.00\tAverage reward: -10.22\n",
      "Episode 456\tLast reward: -13.00\tAverage reward: -10.36\n",
      "Episode 457\tLast reward: -10.00\tAverage reward: -10.34\n",
      "Episode 458\tLast reward: -15.00\tAverage reward: -10.57\n",
      "Episode 459\tLast reward: -17.00\tAverage reward: -10.89\n",
      "Episode 460\tLast reward: -5.00\tAverage reward: -10.60\n",
      "Episode 461\tLast reward: -13.00\tAverage reward: -10.72\n",
      "Episode 462\tLast reward: -12.00\tAverage reward: -10.78\n",
      "Episode 463\tLast reward: -12.00\tAverage reward: -10.84\n",
      "Episode 464\tLast reward: -16.00\tAverage reward: -11.10\n",
      "Episode 465\tLast reward: -15.00\tAverage reward: -11.30\n",
      "Episode 466\tLast reward: -17.00\tAverage reward: -11.58\n",
      "Episode 467\tLast reward: -10.00\tAverage reward: -11.50\n",
      "Episode 468\tLast reward: -11.00\tAverage reward: -11.48\n",
      "Episode 469\tLast reward: -9.00\tAverage reward: -11.35\n",
      "Episode 470\tLast reward: -10.00\tAverage reward: -11.29\n",
      "Episode 471\tLast reward: -15.00\tAverage reward: -11.47\n",
      "Episode 472\tLast reward: -11.00\tAverage reward: -11.45\n",
      "Episode 473\tLast reward: -8.00\tAverage reward: -11.28\n",
      "Episode 474\tLast reward: -17.00\tAverage reward: -11.56\n",
      "Episode 475\tLast reward: -11.00\tAverage reward: -11.53\n",
      "Episode 476\tLast reward: -11.00\tAverage reward: -11.51\n",
      "Episode 477\tLast reward: -7.00\tAverage reward: -11.28\n",
      "Episode 478\tLast reward: -14.00\tAverage reward: -11.42\n",
      "Episode 479\tLast reward: -6.00\tAverage reward: -11.15\n",
      "Episode 480\tLast reward: -13.00\tAverage reward: -11.24\n",
      "Episode 481\tLast reward: -10.00\tAverage reward: -11.18\n",
      "Episode 482\tLast reward: -9.00\tAverage reward: -11.07\n",
      "Episode 483\tLast reward: -18.00\tAverage reward: -11.41\n",
      "Episode 484\tLast reward: -15.00\tAverage reward: -11.59\n",
      "Episode 485\tLast reward: 5.00\tAverage reward: -10.76\n",
      "Episode 486\tLast reward: -11.00\tAverage reward: -10.78\n",
      "Episode 487\tLast reward: -15.00\tAverage reward: -10.99\n",
      "Episode 488\tLast reward: -5.00\tAverage reward: -10.69\n",
      "Episode 489\tLast reward: -9.00\tAverage reward: -10.60\n",
      "Episode 490\tLast reward: -5.00\tAverage reward: -10.32\n",
      "Episode 491\tLast reward: -10.00\tAverage reward: -10.31\n",
      "Episode 492\tLast reward: -13.00\tAverage reward: -10.44\n",
      "Episode 493\tLast reward: -9.00\tAverage reward: -10.37\n",
      "Episode 494\tLast reward: -13.00\tAverage reward: -10.50\n",
      "Episode 495\tLast reward: -2.00\tAverage reward: -10.08\n",
      "Episode 496\tLast reward: -7.00\tAverage reward: -9.92\n",
      "Episode 497\tLast reward: -11.00\tAverage reward: -9.98\n",
      "Episode 498\tLast reward: -13.00\tAverage reward: -10.13\n",
      "Episode 499\tLast reward: -10.00\tAverage reward: -10.12\n",
      "Episode 500\tLast reward: -12.00\tAverage reward: -10.22\n",
      "Episode 501\tLast reward: -3.00\tAverage reward: -9.85\n",
      "Episode 502\tLast reward: -8.00\tAverage reward: -9.76\n",
      "Episode 503\tLast reward: -9.00\tAverage reward: -9.72\n",
      "Episode 504\tLast reward: -9.00\tAverage reward: -9.69\n",
      "Episode 505\tLast reward: -15.00\tAverage reward: -9.95\n",
      "Episode 506\tLast reward: -18.00\tAverage reward: -10.36\n",
      "Episode 507\tLast reward: -12.00\tAverage reward: -10.44\n",
      "Episode 508\tLast reward: -5.00\tAverage reward: -10.17\n",
      "Episode 509\tLast reward: -9.00\tAverage reward: -10.11\n",
      "Episode 510\tLast reward: -11.00\tAverage reward: -10.15\n",
      "Episode 511\tLast reward: -12.00\tAverage reward: -10.24\n",
      "Episode 512\tLast reward: 3.00\tAverage reward: -9.58\n",
      "Episode 513\tLast reward: -13.00\tAverage reward: -9.75\n",
      "Episode 514\tLast reward: -15.00\tAverage reward: -10.02\n",
      "Episode 515\tLast reward: -10.00\tAverage reward: -10.01\n",
      "Episode 516\tLast reward: -15.00\tAverage reward: -10.26\n",
      "Episode 517\tLast reward: 1.00\tAverage reward: -9.70\n",
      "Episode 518\tLast reward: -16.00\tAverage reward: -10.02\n",
      "Episode 519\tLast reward: -11.00\tAverage reward: -10.06\n",
      "Episode 520\tLast reward: -17.00\tAverage reward: -10.41\n",
      "Episode 521\tLast reward: -16.00\tAverage reward: -10.69\n",
      "Episode 522\tLast reward: -11.00\tAverage reward: -10.71\n",
      "Episode 523\tLast reward: -18.00\tAverage reward: -11.07\n",
      "Episode 524\tLast reward: -14.00\tAverage reward: -11.22\n",
      "Episode 525\tLast reward: -8.00\tAverage reward: -11.06\n",
      "Episode 526\tLast reward: -12.00\tAverage reward: -11.10\n",
      "Episode 527\tLast reward: -13.00\tAverage reward: -11.20\n",
      "Episode 528\tLast reward: -5.00\tAverage reward: -10.89\n",
      "Episode 529\tLast reward: -11.00\tAverage reward: -10.89\n",
      "Episode 530\tLast reward: -3.00\tAverage reward: -10.50\n",
      "Episode 531\tLast reward: -19.00\tAverage reward: -10.92\n",
      "Episode 532\tLast reward: -18.00\tAverage reward: -11.28\n",
      "Episode 533\tLast reward: -13.00\tAverage reward: -11.36\n",
      "Episode 534\tLast reward: -12.00\tAverage reward: -11.40\n",
      "Episode 535\tLast reward: -6.00\tAverage reward: -11.13\n",
      "Episode 536\tLast reward: -11.00\tAverage reward: -11.12\n",
      "Episode 537\tLast reward: -11.00\tAverage reward: -11.11\n",
      "Episode 538\tLast reward: -8.00\tAverage reward: -10.96\n",
      "Episode 539\tLast reward: -16.00\tAverage reward: -11.21\n",
      "Episode 540\tLast reward: -15.00\tAverage reward: -11.40\n",
      "Episode 541\tLast reward: -6.00\tAverage reward: -11.13\n",
      "Episode 542\tLast reward: -9.00\tAverage reward: -11.02\n",
      "Episode 543\tLast reward: -6.00\tAverage reward: -10.77\n",
      "Episode 544\tLast reward: -14.00\tAverage reward: -10.93\n",
      "Episode 545\tLast reward: -14.00\tAverage reward: -11.09\n",
      "Episode 546\tLast reward: -11.00\tAverage reward: -11.08\n",
      "Episode 547\tLast reward: -15.00\tAverage reward: -11.28\n",
      "Episode 548\tLast reward: -18.00\tAverage reward: -11.61\n",
      "Episode 549\tLast reward: -5.00\tAverage reward: -11.28\n",
      "Episode 550\tLast reward: -13.00\tAverage reward: -11.37\n",
      "Episode 551\tLast reward: -9.00\tAverage reward: -11.25\n",
      "Episode 552\tLast reward: -9.00\tAverage reward: -11.14\n",
      "Episode 553\tLast reward: -8.00\tAverage reward: -10.98\n",
      "Episode 554\tLast reward: -17.00\tAverage reward: -11.28\n",
      "Episode 555\tLast reward: -13.00\tAverage reward: -11.37\n",
      "Episode 556\tLast reward: -2.00\tAverage reward: -10.90\n",
      "Episode 557\tLast reward: -11.00\tAverage reward: -10.91\n",
      "Episode 558\tLast reward: -12.00\tAverage reward: -10.96\n",
      "Episode 559\tLast reward: -12.00\tAverage reward: -11.01\n",
      "Episode 560\tLast reward: -9.00\tAverage reward: -10.91\n",
      "Episode 561\tLast reward: -9.00\tAverage reward: -10.82\n",
      "Episode 562\tLast reward: -11.00\tAverage reward: -10.82\n",
      "Episode 563\tLast reward: -6.00\tAverage reward: -10.58\n",
      "Episode 564\tLast reward: -19.00\tAverage reward: -11.00\n",
      "Episode 565\tLast reward: -12.00\tAverage reward: -11.05\n",
      "Episode 566\tLast reward: -14.00\tAverage reward: -11.20\n",
      "Episode 567\tLast reward: -11.00\tAverage reward: -11.19\n",
      "Episode 568\tLast reward: -14.00\tAverage reward: -11.33\n",
      "Episode 569\tLast reward: -10.00\tAverage reward: -11.27\n",
      "Episode 570\tLast reward: -16.00\tAverage reward: -11.50\n",
      "Episode 571\tLast reward: -15.00\tAverage reward: -11.68\n",
      "Episode 572\tLast reward: -16.00\tAverage reward: -11.89\n",
      "Episode 573\tLast reward: -17.00\tAverage reward: -12.15\n",
      "Episode 574\tLast reward: -12.00\tAverage reward: -12.14\n",
      "Episode 575\tLast reward: -12.00\tAverage reward: -12.13\n",
      "Episode 576\tLast reward: -15.00\tAverage reward: -12.28\n",
      "Episode 577\tLast reward: -10.00\tAverage reward: -12.16\n",
      "Episode 578\tLast reward: -13.00\tAverage reward: -12.21\n",
      "Episode 579\tLast reward: -15.00\tAverage reward: -12.34\n",
      "Episode 580\tLast reward: -19.00\tAverage reward: -12.68\n",
      "Episode 581\tLast reward: -11.00\tAverage reward: -12.59\n",
      "Episode 582\tLast reward: -19.00\tAverage reward: -12.91\n",
      "Episode 583\tLast reward: -16.00\tAverage reward: -13.07\n",
      "Episode 584\tLast reward: -7.00\tAverage reward: -12.77\n",
      "Episode 585\tLast reward: -14.00\tAverage reward: -12.83\n",
      "Episode 586\tLast reward: -12.00\tAverage reward: -12.79\n",
      "Episode 587\tLast reward: -17.00\tAverage reward: -13.00\n",
      "Episode 588\tLast reward: -15.00\tAverage reward: -13.10\n",
      "Episode 589\tLast reward: -5.00\tAverage reward: -12.69\n",
      "Episode 590\tLast reward: -4.00\tAverage reward: -12.26\n",
      "Episode 591\tLast reward: -17.00\tAverage reward: -12.49\n",
      "Episode 592\tLast reward: -8.00\tAverage reward: -12.27\n",
      "Episode 593\tLast reward: -11.00\tAverage reward: -12.21\n",
      "Episode 594\tLast reward: -11.00\tAverage reward: -12.15\n",
      "Episode 595\tLast reward: -13.00\tAverage reward: -12.19\n",
      "Episode 596\tLast reward: -12.00\tAverage reward: -12.18\n",
      "Episode 597\tLast reward: -3.00\tAverage reward: -11.72\n",
      "Episode 598\tLast reward: -9.00\tAverage reward: -11.58\n",
      "Episode 599\tLast reward: -12.00\tAverage reward: -11.60\n",
      "Episode 600\tLast reward: -11.00\tAverage reward: -11.57\n",
      "Episode 601\tLast reward: -11.00\tAverage reward: -11.55\n",
      "Episode 602\tLast reward: -12.00\tAverage reward: -11.57\n",
      "Episode 603\tLast reward: -14.00\tAverage reward: -11.69\n",
      "Episode 604\tLast reward: -12.00\tAverage reward: -11.71\n",
      "Episode 605\tLast reward: -17.00\tAverage reward: -11.97\n",
      "Episode 606\tLast reward: -7.00\tAverage reward: -11.72\n",
      "Episode 607\tLast reward: -8.00\tAverage reward: -11.54\n",
      "Episode 608\tLast reward: -15.00\tAverage reward: -11.71\n",
      "Episode 609\tLast reward: -12.00\tAverage reward: -11.72\n",
      "Episode 610\tLast reward: -6.00\tAverage reward: -11.44\n",
      "Episode 611\tLast reward: -7.00\tAverage reward: -11.22\n",
      "Episode 612\tLast reward: -11.00\tAverage reward: -11.20\n",
      "Episode 613\tLast reward: -16.00\tAverage reward: -11.44\n",
      "Episode 614\tLast reward: -15.00\tAverage reward: -11.62\n",
      "Episode 615\tLast reward: -19.00\tAverage reward: -11.99\n",
      "Episode 616\tLast reward: -13.00\tAverage reward: -12.04\n",
      "Episode 617\tLast reward: -11.00\tAverage reward: -11.99\n",
      "Episode 618\tLast reward: -15.00\tAverage reward: -12.14\n",
      "Episode 619\tLast reward: -7.00\tAverage reward: -11.88\n",
      "Episode 620\tLast reward: -5.00\tAverage reward: -11.54\n",
      "Episode 621\tLast reward: -14.00\tAverage reward: -11.66\n",
      "Episode 622\tLast reward: -11.00\tAverage reward: -11.63\n",
      "Episode 623\tLast reward: -9.00\tAverage reward: -11.50\n",
      "Episode 624\tLast reward: -11.00\tAverage reward: -11.47\n",
      "Episode 625\tLast reward: -13.00\tAverage reward: -11.55\n",
      "Episode 626\tLast reward: -12.00\tAverage reward: -11.57\n",
      "Episode 627\tLast reward: -9.00\tAverage reward: -11.44\n",
      "Episode 628\tLast reward: -13.00\tAverage reward: -11.52\n",
      "Episode 629\tLast reward: -8.00\tAverage reward: -11.34\n",
      "Episode 630\tLast reward: -18.00\tAverage reward: -11.68\n",
      "Episode 631\tLast reward: -16.00\tAverage reward: -11.89\n",
      "Episode 632\tLast reward: 0.00\tAverage reward: -11.30\n",
      "Episode 633\tLast reward: -10.00\tAverage reward: -11.23\n",
      "Episode 634\tLast reward: -16.00\tAverage reward: -11.47\n",
      "Episode 635\tLast reward: -12.00\tAverage reward: -11.50\n",
      "Episode 636\tLast reward: -15.00\tAverage reward: -11.67\n",
      "Episode 637\tLast reward: -14.00\tAverage reward: -11.79\n",
      "Episode 638\tLast reward: -11.00\tAverage reward: -11.75\n",
      "Episode 639\tLast reward: -16.00\tAverage reward: -11.96\n",
      "Episode 640\tLast reward: -14.00\tAverage reward: -12.06\n",
      "Episode 641\tLast reward: -15.00\tAverage reward: -12.21\n",
      "Episode 642\tLast reward: -13.00\tAverage reward: -12.25\n",
      "Episode 643\tLast reward: -13.00\tAverage reward: -12.29\n",
      "Episode 644\tLast reward: -16.00\tAverage reward: -12.47\n",
      "Episode 645\tLast reward: -10.00\tAverage reward: -12.35\n",
      "Episode 646\tLast reward: -13.00\tAverage reward: -12.38\n",
      "Episode 647\tLast reward: -12.00\tAverage reward: -12.36\n",
      "Episode 648\tLast reward: -11.00\tAverage reward: -12.30\n",
      "Episode 649\tLast reward: -10.00\tAverage reward: -12.18\n",
      "Episode 650\tLast reward: -15.00\tAverage reward: -12.32\n",
      "Episode 651\tLast reward: -15.00\tAverage reward: -12.46\n",
      "Episode 652\tLast reward: -11.00\tAverage reward: -12.38\n",
      "Episode 653\tLast reward: -14.00\tAverage reward: -12.46\n",
      "Episode 654\tLast reward: -9.00\tAverage reward: -12.29\n",
      "Episode 655\tLast reward: -13.00\tAverage reward: -12.33\n",
      "Episode 656\tLast reward: -14.00\tAverage reward: -12.41\n",
      "Episode 657\tLast reward: -6.00\tAverage reward: -12.09\n",
      "Episode 658\tLast reward: -13.00\tAverage reward: -12.13\n",
      "Episode 659\tLast reward: -9.00\tAverage reward: -11.98\n",
      "Episode 660\tLast reward: -18.00\tAverage reward: -12.28\n",
      "Episode 661\tLast reward: -14.00\tAverage reward: -12.37\n",
      "Episode 662\tLast reward: -7.00\tAverage reward: -12.10\n",
      "Episode 663\tLast reward: -12.00\tAverage reward: -12.09\n",
      "Episode 664\tLast reward: -15.00\tAverage reward: -12.24\n",
      "Episode 665\tLast reward: -15.00\tAverage reward: -12.38\n",
      "Episode 666\tLast reward: -14.00\tAverage reward: -12.46\n",
      "Episode 667\tLast reward: -5.00\tAverage reward: -12.08\n",
      "Episode 668\tLast reward: -3.00\tAverage reward: -11.63\n",
      "Episode 669\tLast reward: -15.00\tAverage reward: -11.80\n",
      "Episode 670\tLast reward: -13.00\tAverage reward: -11.86\n",
      "Episode 671\tLast reward: -9.00\tAverage reward: -11.72\n",
      "Episode 672\tLast reward: -14.00\tAverage reward: -11.83\n",
      "Episode 673\tLast reward: -13.00\tAverage reward: -11.89\n",
      "Episode 674\tLast reward: -11.00\tAverage reward: -11.84\n",
      "Episode 675\tLast reward: -10.00\tAverage reward: -11.75\n",
      "Episode 676\tLast reward: -11.00\tAverage reward: -11.71\n",
      "Episode 677\tLast reward: -13.00\tAverage reward: -11.78\n",
      "Episode 678\tLast reward: -5.00\tAverage reward: -11.44\n",
      "Episode 679\tLast reward: -9.00\tAverage reward: -11.32\n",
      "Episode 680\tLast reward: -15.00\tAverage reward: -11.50\n",
      "Episode 681\tLast reward: -16.00\tAverage reward: -11.73\n",
      "Episode 682\tLast reward: -13.00\tAverage reward: -11.79\n",
      "Episode 683\tLast reward: -15.00\tAverage reward: -11.95\n",
      "Episode 684\tLast reward: -10.00\tAverage reward: -11.85\n",
      "Episode 685\tLast reward: -8.00\tAverage reward: -11.66\n",
      "Episode 686\tLast reward: -10.00\tAverage reward: -11.58\n",
      "Episode 687\tLast reward: -19.00\tAverage reward: -11.95\n",
      "Episode 688\tLast reward: -17.00\tAverage reward: -12.20\n",
      "Episode 689\tLast reward: -14.00\tAverage reward: -12.29\n",
      "Episode 690\tLast reward: -10.00\tAverage reward: -12.18\n",
      "Episode 691\tLast reward: -7.00\tAverage reward: -11.92\n",
      "Episode 692\tLast reward: -7.00\tAverage reward: -11.67\n",
      "Episode 693\tLast reward: -6.00\tAverage reward: -11.39\n",
      "Episode 694\tLast reward: -10.00\tAverage reward: -11.32\n",
      "Episode 695\tLast reward: -11.00\tAverage reward: -11.30\n",
      "Episode 696\tLast reward: -13.00\tAverage reward: -11.39\n",
      "Episode 697\tLast reward: -14.00\tAverage reward: -11.52\n",
      "Episode 698\tLast reward: -17.00\tAverage reward: -11.79\n",
      "Episode 699\tLast reward: -13.00\tAverage reward: -11.85\n",
      "Episode 700\tLast reward: -13.00\tAverage reward: -11.91\n",
      "Episode 701\tLast reward: -11.00\tAverage reward: -11.86\n",
      "Episode 702\tLast reward: -9.00\tAverage reward: -11.72\n",
      "Episode 703\tLast reward: -11.00\tAverage reward: -11.69\n",
      "Episode 704\tLast reward: -13.00\tAverage reward: -11.75\n",
      "Episode 705\tLast reward: -13.00\tAverage reward: -11.81\n",
      "Episode 706\tLast reward: -10.00\tAverage reward: -11.72\n",
      "Episode 707\tLast reward: -17.00\tAverage reward: -11.99\n",
      "Episode 708\tLast reward: -16.00\tAverage reward: -12.19\n",
      "Episode 709\tLast reward: -6.00\tAverage reward: -11.88\n",
      "Episode 710\tLast reward: -16.00\tAverage reward: -12.08\n",
      "Episode 711\tLast reward: -17.00\tAverage reward: -12.33\n",
      "Episode 712\tLast reward: -12.00\tAverage reward: -12.31\n",
      "Episode 713\tLast reward: -12.00\tAverage reward: -12.30\n",
      "Episode 714\tLast reward: -17.00\tAverage reward: -12.53\n",
      "Episode 715\tLast reward: -14.00\tAverage reward: -12.61\n",
      "Episode 716\tLast reward: -11.00\tAverage reward: -12.53\n",
      "Episode 717\tLast reward: -11.00\tAverage reward: -12.45\n",
      "Episode 718\tLast reward: -19.00\tAverage reward: -12.78\n",
      "Episode 719\tLast reward: -17.00\tAverage reward: -12.99\n",
      "Episode 720\tLast reward: -8.00\tAverage reward: -12.74\n",
      "Episode 721\tLast reward: -16.00\tAverage reward: -12.90\n",
      "Episode 722\tLast reward: -11.00\tAverage reward: -12.81\n",
      "Episode 723\tLast reward: -13.00\tAverage reward: -12.82\n",
      "Episode 724\tLast reward: -16.00\tAverage reward: -12.98\n",
      "Episode 725\tLast reward: -13.00\tAverage reward: -12.98\n",
      "Episode 726\tLast reward: -19.00\tAverage reward: -13.28\n",
      "Episode 727\tLast reward: -6.00\tAverage reward: -12.91\n",
      "Episode 728\tLast reward: -7.00\tAverage reward: -12.62\n",
      "Episode 729\tLast reward: -7.00\tAverage reward: -12.34\n",
      "Episode 730\tLast reward: -9.00\tAverage reward: -12.17\n",
      "Episode 731\tLast reward: -7.00\tAverage reward: -11.91\n",
      "Episode 732\tLast reward: -11.00\tAverage reward: -11.87\n",
      "Episode 733\tLast reward: -11.00\tAverage reward: -11.82\n",
      "Episode 734\tLast reward: -5.00\tAverage reward: -11.48\n",
      "Episode 735\tLast reward: -9.00\tAverage reward: -11.36\n",
      "Episode 736\tLast reward: -13.00\tAverage reward: -11.44\n",
      "Episode 737\tLast reward: -13.00\tAverage reward: -11.52\n",
      "Episode 738\tLast reward: -3.00\tAverage reward: -11.09\n",
      "Episode 739\tLast reward: -14.00\tAverage reward: -11.24\n",
      "Episode 740\tLast reward: -17.00\tAverage reward: -11.53\n",
      "Episode 741\tLast reward: -6.00\tAverage reward: -11.25\n",
      "Episode 742\tLast reward: -6.00\tAverage reward: -10.99\n",
      "Episode 743\tLast reward: -7.00\tAverage reward: -10.79\n",
      "Episode 744\tLast reward: -16.00\tAverage reward: -11.05\n",
      "Episode 745\tLast reward: -13.00\tAverage reward: -11.15\n",
      "Episode 746\tLast reward: -7.00\tAverage reward: -10.94\n",
      "Episode 747\tLast reward: -12.00\tAverage reward: -10.99\n",
      "Episode 748\tLast reward: -8.00\tAverage reward: -10.84\n",
      "Episode 749\tLast reward: -5.00\tAverage reward: -10.55\n",
      "Episode 750\tLast reward: -2.00\tAverage reward: -10.12\n",
      "Episode 751\tLast reward: -12.00\tAverage reward: -10.22\n",
      "Episode 752\tLast reward: -13.00\tAverage reward: -10.36\n",
      "Episode 753\tLast reward: -14.00\tAverage reward: -10.54\n",
      "Episode 754\tLast reward: -8.00\tAverage reward: -10.41\n",
      "Episode 755\tLast reward: -14.00\tAverage reward: -10.59\n",
      "Episode 756\tLast reward: -7.00\tAverage reward: -10.41\n",
      "Episode 757\tLast reward: -16.00\tAverage reward: -10.69\n",
      "Episode 758\tLast reward: -12.00\tAverage reward: -10.76\n",
      "Episode 759\tLast reward: -17.00\tAverage reward: -11.07\n",
      "Episode 760\tLast reward: -14.00\tAverage reward: -11.21\n",
      "Episode 761\tLast reward: -11.00\tAverage reward: -11.20\n",
      "Episode 762\tLast reward: -11.00\tAverage reward: -11.19\n",
      "Episode 763\tLast reward: -12.00\tAverage reward: -11.23\n",
      "Episode 764\tLast reward: -13.00\tAverage reward: -11.32\n",
      "Episode 765\tLast reward: -13.00\tAverage reward: -11.41\n",
      "Episode 766\tLast reward: -12.00\tAverage reward: -11.44\n",
      "Episode 767\tLast reward: -8.00\tAverage reward: -11.26\n",
      "Episode 768\tLast reward: -15.00\tAverage reward: -11.45\n",
      "Episode 769\tLast reward: -9.00\tAverage reward: -11.33\n",
      "Episode 770\tLast reward: -5.00\tAverage reward: -11.01\n",
      "Episode 771\tLast reward: -21.00\tAverage reward: -11.51\n",
      "Episode 772\tLast reward: -18.00\tAverage reward: -11.84\n",
      "Episode 773\tLast reward: -12.00\tAverage reward: -11.84\n",
      "Episode 774\tLast reward: -17.00\tAverage reward: -12.10\n",
      "Episode 775\tLast reward: -9.00\tAverage reward: -11.95\n",
      "Episode 776\tLast reward: -19.00\tAverage reward: -12.30\n",
      "Episode 777\tLast reward: -5.00\tAverage reward: -11.93\n",
      "Episode 778\tLast reward: -16.00\tAverage reward: -12.14\n",
      "Episode 779\tLast reward: -13.00\tAverage reward: -12.18\n",
      "Episode 780\tLast reward: -7.00\tAverage reward: -11.92\n",
      "Episode 781\tLast reward: -14.00\tAverage reward: -12.03\n",
      "Episode 782\tLast reward: -10.00\tAverage reward: -11.92\n",
      "Episode 783\tLast reward: -17.00\tAverage reward: -12.18\n",
      "Episode 784\tLast reward: -9.00\tAverage reward: -12.02\n",
      "Episode 785\tLast reward: -9.00\tAverage reward: -11.87\n",
      "Episode 786\tLast reward: -13.00\tAverage reward: -11.92\n",
      "Episode 787\tLast reward: -5.00\tAverage reward: -11.58\n",
      "Episode 788\tLast reward: -5.00\tAverage reward: -11.25\n",
      "Episode 789\tLast reward: -11.00\tAverage reward: -11.24\n",
      "Episode 790\tLast reward: -13.00\tAverage reward: -11.33\n",
      "Episode 791\tLast reward: -9.00\tAverage reward: -11.21\n",
      "Episode 792\tLast reward: -18.00\tAverage reward: -11.55\n",
      "Episode 793\tLast reward: -12.00\tAverage reward: -11.57\n",
      "Episode 794\tLast reward: -14.00\tAverage reward: -11.69\n",
      "Episode 795\tLast reward: 1.00\tAverage reward: -11.06\n",
      "Episode 796\tLast reward: -13.00\tAverage reward: -11.16\n",
      "Episode 797\tLast reward: -14.00\tAverage reward: -11.30\n",
      "Episode 798\tLast reward: -13.00\tAverage reward: -11.38\n",
      "Episode 799\tLast reward: -12.00\tAverage reward: -11.41\n",
      "Episode 800\tLast reward: -7.00\tAverage reward: -11.19\n",
      "Episode 801\tLast reward: -8.00\tAverage reward: -11.03\n",
      "Episode 802\tLast reward: -13.00\tAverage reward: -11.13\n",
      "Episode 803\tLast reward: -17.00\tAverage reward: -11.42\n",
      "Episode 804\tLast reward: 4.00\tAverage reward: -10.65\n",
      "Episode 805\tLast reward: -13.00\tAverage reward: -10.77\n",
      "Episode 806\tLast reward: -15.00\tAverage reward: -10.98\n",
      "Episode 807\tLast reward: -15.00\tAverage reward: -11.18\n",
      "Episode 808\tLast reward: -18.00\tAverage reward: -11.52\n",
      "Episode 809\tLast reward: -13.00\tAverage reward: -11.60\n",
      "Episode 810\tLast reward: -14.00\tAverage reward: -11.72\n",
      "Episode 811\tLast reward: -10.00\tAverage reward: -11.63\n",
      "Episode 812\tLast reward: -13.00\tAverage reward: -11.70\n",
      "Episode 813\tLast reward: -12.00\tAverage reward: -11.72\n",
      "Episode 814\tLast reward: -14.00\tAverage reward: -11.83\n",
      "Episode 815\tLast reward: -17.00\tAverage reward: -12.09\n",
      "Episode 816\tLast reward: -3.00\tAverage reward: -11.63\n",
      "Episode 817\tLast reward: -17.00\tAverage reward: -11.90\n",
      "Episode 818\tLast reward: -13.00\tAverage reward: -11.96\n",
      "Episode 819\tLast reward: -8.00\tAverage reward: -11.76\n",
      "Episode 820\tLast reward: -11.00\tAverage reward: -11.72\n",
      "Episode 821\tLast reward: -11.00\tAverage reward: -11.69\n",
      "Episode 822\tLast reward: -15.00\tAverage reward: -11.85\n",
      "Episode 823\tLast reward: -11.00\tAverage reward: -11.81\n",
      "Episode 824\tLast reward: -15.00\tAverage reward: -11.97\n",
      "Episode 825\tLast reward: -10.00\tAverage reward: -11.87\n",
      "Episode 826\tLast reward: -7.00\tAverage reward: -11.63\n",
      "Episode 827\tLast reward: -15.00\tAverage reward: -11.79\n",
      "Episode 828\tLast reward: -11.00\tAverage reward: -11.75\n",
      "Episode 829\tLast reward: -8.00\tAverage reward: -11.57\n",
      "Episode 830\tLast reward: -13.00\tAverage reward: -11.64\n",
      "Episode 831\tLast reward: -10.00\tAverage reward: -11.56\n",
      "Episode 832\tLast reward: -14.00\tAverage reward: -11.68\n",
      "Episode 833\tLast reward: -15.00\tAverage reward: -11.85\n",
      "Episode 834\tLast reward: -15.00\tAverage reward: -12.00\n",
      "Episode 835\tLast reward: -17.00\tAverage reward: -12.25\n",
      "Episode 836\tLast reward: 0.00\tAverage reward: -11.64\n",
      "Episode 837\tLast reward: -11.00\tAverage reward: -11.61\n",
      "Episode 838\tLast reward: -16.00\tAverage reward: -11.83\n",
      "Episode 839\tLast reward: -7.00\tAverage reward: -11.59\n",
      "Episode 840\tLast reward: -11.00\tAverage reward: -11.56\n",
      "Episode 841\tLast reward: -14.00\tAverage reward: -11.68\n",
      "Episode 842\tLast reward: -9.00\tAverage reward: -11.55\n",
      "Episode 843\tLast reward: -10.00\tAverage reward: -11.47\n",
      "Episode 844\tLast reward: -13.00\tAverage reward: -11.54\n",
      "Episode 845\tLast reward: -7.00\tAverage reward: -11.32\n",
      "Episode 846\tLast reward: -13.00\tAverage reward: -11.40\n",
      "Episode 847\tLast reward: -19.00\tAverage reward: -11.78\n",
      "Episode 848\tLast reward: -11.00\tAverage reward: -11.74\n",
      "Episode 849\tLast reward: -11.00\tAverage reward: -11.71\n",
      "Episode 850\tLast reward: -11.00\tAverage reward: -11.67\n",
      "Episode 851\tLast reward: -4.00\tAverage reward: -11.29\n",
      "Episode 852\tLast reward: -15.00\tAverage reward: -11.47\n",
      "Episode 853\tLast reward: -8.00\tAverage reward: -11.30\n",
      "Episode 854\tLast reward: -20.00\tAverage reward: -11.73\n",
      "Episode 855\tLast reward: -1.00\tAverage reward: -11.20\n",
      "Episode 856\tLast reward: -9.00\tAverage reward: -11.09\n",
      "Episode 857\tLast reward: -14.00\tAverage reward: -11.23\n",
      "Episode 858\tLast reward: -18.00\tAverage reward: -11.57\n",
      "Episode 859\tLast reward: -11.00\tAverage reward: -11.54\n",
      "Episode 860\tLast reward: -15.00\tAverage reward: -11.72\n",
      "Episode 861\tLast reward: -12.00\tAverage reward: -11.73\n",
      "Episode 862\tLast reward: -12.00\tAverage reward: -11.74\n",
      "Episode 863\tLast reward: -11.00\tAverage reward: -11.71\n",
      "Episode 864\tLast reward: -14.00\tAverage reward: -11.82\n",
      "Episode 865\tLast reward: -11.00\tAverage reward: -11.78\n",
      "Episode 866\tLast reward: -13.00\tAverage reward: -11.84\n",
      "Episode 867\tLast reward: -12.00\tAverage reward: -11.85\n",
      "Episode 868\tLast reward: -11.00\tAverage reward: -11.81\n",
      "Episode 869\tLast reward: -17.00\tAverage reward: -12.07\n",
      "Episode 870\tLast reward: -6.00\tAverage reward: -11.76\n",
      "Episode 871\tLast reward: -13.00\tAverage reward: -11.82\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9753b40e3b8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m## take an action sampled from a categorical distribution given the state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0maction_log_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-49cafd6be726>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m##return Categorical(torch.sigmoid(nn_out))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`probs` parameter must be at least one-dimensional.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in count(1):\n",
    "    action_log_probs = list()\n",
    "    rewards = list()\n",
    "    values = list()\n",
    "    state = env.reset()\n",
    "    ##test = net(torch.FloatTensor(state).unsqueeze(0))\n",
    "    ##print(test)\n",
    "    prev_x = None\n",
    "    #print(state.shape)\n",
    "    ##counter = 0\n",
    "    for t in range(100000):\n",
    "    #while True:\n",
    "        #env.render()\n",
    "        \n",
    "        cur_x  = preprocess(state)\n",
    "        #print(\"cur_x = \", cur_x.shape)\n",
    "        state  = cur_x - prev_x if prev_x is not None else preprocess(prev_x)##np.zeros(input_shape)\n",
    "        #print(\"state = \", state.shape)\n",
    "        prev_x = cur_x\n",
    "        \n",
    "        ## so the tensor given to the model should be of shape [batch_size, 1, height, width].\n",
    "        \n",
    "        \n",
    "        ##state = torch.FloatTensor(state)\n",
    "        \n",
    "        state = state.reshape(1, 1, 80, 80)\n",
    "        \n",
    "        #env.render()\n",
    "        ## take an action sampled from a categorical distribution given the state\n",
    "        action_prob = pnet(torch.FloatTensor(state).to(device))\n",
    "        action = action_prob.sample()\n",
    "        action_log_probs.append(action_prob.log_prob(action))\n",
    "        \n",
    "        if action.item() == 0:\n",
    "            action = UP_ACTION\n",
    "        else:\n",
    "            action = DOWN_ACTION\n",
    "        \n",
    "        #print(entropy)\n",
    "        value = cnet(torch.FloatTensor(state).to(device))\n",
    "        values.append(value[0])\n",
    "        #print(action)\n",
    "        next_state, reward, is_done, _ = env.step(action) # take a random action\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        ## current state is next state now\n",
    "        state = next_state\n",
    "\n",
    "        if is_done:\n",
    "            #print(rewards)\n",
    "            #print(values)\n",
    "            break\n",
    "            \n",
    "       \n",
    "            \n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "                \n",
    "    ## Now we have the discounted reward + log_probs of the actions\n",
    "    returns = discounted_returns(rewards)\n",
    "    #print(returns)\n",
    "    action_losses = list()\n",
    "    critic_losses = list()\n",
    "    ## collect the action losses to a list\n",
    "    for ret, l_prob, v in zip(returns, action_log_probs, values):\n",
    "        advantage = ret - v\n",
    "        #print(advantage)\n",
    "        #print(-l_prob * ret)\n",
    "        action_losses.append(-l_prob * advantage.detach())\n",
    "        critic_losses.append(advantage.pow(2))\n",
    "\n",
    "    p_optimizer.zero_grad()\n",
    "    ## accumulate the action losses\n",
    "    action_loss = torch.cat(action_losses).sum()\n",
    "    action_loss.backward()\n",
    "    ## step the optimizer\n",
    "    p_optimizer.step()\n",
    "\n",
    "    c_optimizer.zero_grad()\n",
    "    critic_loss = torch.cat(critic_losses).mean()\n",
    "    critic_loss.backward()\n",
    "    c_optimizer.step()\n",
    "    '''\n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "    '''\n",
    "      \n",
    "    if running_reward >= MEAN_REWARD_BOUND:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n",
    "    \n",
    "env.close()\n",
    "\n",
    "\n",
    "            \n",
    "##:.2f\n",
    "##'Episode {}\\t Last reward: {}\\t Average reward: {}'.format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for the last part and introducing the next one \n",
    "\n",
    "As shown in the previous results, the model is not changing that much but there is more -10 in the running Average but it is also fluctuating in the interval of [-10,12]\n",
    "\n",
    "So we starting to think of improving the optimizer, there is an optimizer called amsgrad Adam, they said it is much better than adam but in pytorch it is just an option to be enabled (amsgrad=True)\n",
    "\n",
    "We will enable this option and load the model from the best last reward achieved which is 6 and see the results \n",
    "\n",
    "Note: When playing with last rewards 6 it is not really 6, maybe it was a lucky game but it is not bad also we will try to provide the best agent that we achived "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the environment\n",
    "## env = gym.make('PongNoFrameskip-v4')\n",
    "env = gym.make('Pong-v0')\n",
    "##input_shape = env.observation_space.shape[0]\n",
    "#action_size = env.action_space.n\n",
    "\n",
    "action_size = 2\n",
    "\n",
    "##print(\"Env reward threshold: {}\".format(env.spec.reward_threshold))\n",
    "reward_list = list()\n",
    "\n",
    "input_shape = [1,80,80]\n",
    "\n",
    "##input_shape = [STACK_SIZE, 84, 84]\n",
    "\n",
    "\n",
    "pnet = torch.load('../input/a2camsgrad/6.0_A2C_pnet.pt').to(device)##,map_location=torch.device('cpu'))\n",
    "pnet.eval()\n",
    "\n",
    "cnet = torch.load('../input/a2camsgrad/6.0_A2C_cnet.pt').to(device)##,map_location=torch.device('cpu'))\n",
    "cnet.eval()\n",
    "\n",
    "\n",
    "## initialize an optimizer with enabling the AMSGrad\n",
    "p_optimizer = torch.optim.Adam(pnet.parameters(), lr=1e-5,eps=1e-3,amsgrad=True)\n",
    "c_optimizer = torch.optim.Adam(cnet.parameters(), lr=1e-5,eps=1e-3,amsgrad=True)\n",
    "\n",
    "\n",
    "\n",
    "running_reward  = None\n",
    "MEAN_REWARD_BOUND = 20\n",
    "\n",
    "UP_ACTION = 2\n",
    "\n",
    "DOWN_ACTION = 3\n",
    "Max_ep_rewared = -21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tLast reward: -15.00\tAverage reward: -15.00\n",
      "Episode 2\tLast reward: -15.00\tAverage reward: -15.00\n",
      "Episode 3\tLast reward: -11.00\tAverage reward: -14.80\n",
      "Episode 4\tLast reward: -15.00\tAverage reward: -14.81\n",
      "Episode 5\tLast reward: -5.00\tAverage reward: -14.32\n",
      "Episode 6\tLast reward: -11.00\tAverage reward: -14.15\n",
      "Episode 7\tLast reward: -9.00\tAverage reward: -13.90\n",
      "Episode 8\tLast reward: -16.00\tAverage reward: -14.00\n",
      "Episode 9\tLast reward: -10.00\tAverage reward: -13.80\n",
      "Episode 10\tLast reward: -11.00\tAverage reward: -13.66\n",
      "Episode 11\tLast reward: -12.00\tAverage reward: -13.58\n",
      "Episode 12\tLast reward: -13.00\tAverage reward: -13.55\n",
      "Episode 13\tLast reward: -5.00\tAverage reward: -13.12\n",
      "Episode 14\tLast reward: -7.00\tAverage reward: -12.82\n",
      "Episode 15\tLast reward: -15.00\tAverage reward: -12.92\n",
      "Episode 16\tLast reward: -13.00\tAverage reward: -12.93\n",
      "Episode 17\tLast reward: -17.00\tAverage reward: -13.13\n",
      "Episode 18\tLast reward: -18.00\tAverage reward: -13.38\n",
      "Episode 19\tLast reward: -11.00\tAverage reward: -13.26\n",
      "Episode 20\tLast reward: -12.00\tAverage reward: -13.19\n",
      "Episode 21\tLast reward: -9.00\tAverage reward: -12.98\n",
      "Episode 22\tLast reward: -16.00\tAverage reward: -13.13\n",
      "Episode 23\tLast reward: -8.00\tAverage reward: -12.88\n",
      "Episode 24\tLast reward: -5.00\tAverage reward: -12.48\n",
      "Episode 25\tLast reward: -9.00\tAverage reward: -12.31\n",
      "Episode 26\tLast reward: -1.00\tAverage reward: -11.74\n",
      "Episode 27\tLast reward: -17.00\tAverage reward: -12.01\n",
      "Episode 28\tLast reward: -11.00\tAverage reward: -11.96\n",
      "Episode 29\tLast reward: -11.00\tAverage reward: -11.91\n",
      "Episode 30\tLast reward: -10.00\tAverage reward: -11.81\n",
      "Episode 31\tLast reward: -15.00\tAverage reward: -11.97\n",
      "Episode 32\tLast reward: -11.00\tAverage reward: -11.92\n",
      "Episode 33\tLast reward: -16.00\tAverage reward: -12.13\n",
      "Episode 34\tLast reward: -11.00\tAverage reward: -12.07\n",
      "Episode 35\tLast reward: -13.00\tAverage reward: -12.12\n",
      "Episode 36\tLast reward: -17.00\tAverage reward: -12.36\n",
      "Episode 37\tLast reward: -8.00\tAverage reward: -12.14\n",
      "Episode 38\tLast reward: -15.00\tAverage reward: -12.29\n",
      "Episode 39\tLast reward: -7.00\tAverage reward: -12.02\n",
      "Episode 40\tLast reward: -15.00\tAverage reward: -12.17\n",
      "Episode 41\tLast reward: -11.00\tAverage reward: -12.11\n",
      "Episode 42\tLast reward: -17.00\tAverage reward: -12.36\n",
      "Episode 43\tLast reward: -13.00\tAverage reward: -12.39\n",
      "Episode 44\tLast reward: -15.00\tAverage reward: -12.52\n",
      "Episode 45\tLast reward: -13.00\tAverage reward: -12.54\n",
      "Episode 46\tLast reward: -15.00\tAverage reward: -12.67\n",
      "Episode 47\tLast reward: -17.00\tAverage reward: -12.88\n",
      "Episode 48\tLast reward: -12.00\tAverage reward: -12.84\n",
      "Episode 49\tLast reward: -13.00\tAverage reward: -12.85\n",
      "Episode 50\tLast reward: -11.00\tAverage reward: -12.75\n",
      "Episode 51\tLast reward: -17.00\tAverage reward: -12.97\n",
      "Episode 52\tLast reward: -6.00\tAverage reward: -12.62\n",
      "Episode 53\tLast reward: -15.00\tAverage reward: -12.74\n",
      "Episode 54\tLast reward: -16.00\tAverage reward: -12.90\n",
      "Episode 55\tLast reward: -10.00\tAverage reward: -12.76\n",
      "Episode 56\tLast reward: -13.00\tAverage reward: -12.77\n",
      "Episode 57\tLast reward: -14.00\tAverage reward: -12.83\n",
      "Episode 58\tLast reward: -15.00\tAverage reward: -12.94\n",
      "Episode 59\tLast reward: -2.00\tAverage reward: -12.39\n",
      "Episode 60\tLast reward: -10.00\tAverage reward: -12.27\n",
      "Episode 61\tLast reward: -15.00\tAverage reward: -12.41\n",
      "Episode 62\tLast reward: -5.00\tAverage reward: -12.04\n",
      "Episode 63\tLast reward: -10.00\tAverage reward: -11.94\n",
      "Episode 64\tLast reward: -6.00\tAverage reward: -11.64\n",
      "Episode 65\tLast reward: -13.00\tAverage reward: -11.71\n",
      "Episode 66\tLast reward: -15.00\tAverage reward: -11.87\n",
      "Episode 67\tLast reward: -15.00\tAverage reward: -12.03\n",
      "Episode 68\tLast reward: -14.00\tAverage reward: -12.13\n",
      "Episode 69\tLast reward: -4.00\tAverage reward: -11.72\n",
      "Episode 70\tLast reward: -12.00\tAverage reward: -11.73\n",
      "Episode 71\tLast reward: -13.00\tAverage reward: -11.80\n",
      "Episode 72\tLast reward: -11.00\tAverage reward: -11.76\n",
      "Episode 73\tLast reward: -10.00\tAverage reward: -11.67\n",
      "Episode 74\tLast reward: -9.00\tAverage reward: -11.54\n",
      "Episode 75\tLast reward: -17.00\tAverage reward: -11.81\n",
      "Episode 76\tLast reward: -15.00\tAverage reward: -11.97\n",
      "Episode 77\tLast reward: -6.00\tAverage reward: -11.67\n",
      "Episode 78\tLast reward: -19.00\tAverage reward: -12.04\n",
      "Episode 79\tLast reward: -11.00\tAverage reward: -11.99\n",
      "Episode 80\tLast reward: -9.00\tAverage reward: -11.84\n",
      "Episode 81\tLast reward: -12.00\tAverage reward: -11.84\n",
      "Episode 82\tLast reward: -13.00\tAverage reward: -11.90\n",
      "Episode 83\tLast reward: -13.00\tAverage reward: -11.96\n",
      "Episode 84\tLast reward: -3.00\tAverage reward: -11.51\n",
      "Episode 85\tLast reward: -13.00\tAverage reward: -11.58\n",
      "Episode 86\tLast reward: -15.00\tAverage reward: -11.75\n",
      "Episode 87\tLast reward: -8.00\tAverage reward: -11.57\n",
      "Episode 88\tLast reward: -12.00\tAverage reward: -11.59\n",
      "Episode 89\tLast reward: -17.00\tAverage reward: -11.86\n",
      "Episode 90\tLast reward: -17.00\tAverage reward: -12.12\n",
      "Episode 91\tLast reward: -13.00\tAverage reward: -12.16\n",
      "Episode 92\tLast reward: -18.00\tAverage reward: -12.45\n",
      "Episode 93\tLast reward: -6.00\tAverage reward: -12.13\n",
      "Episode 94\tLast reward: -17.00\tAverage reward: -12.37\n",
      "Episode 95\tLast reward: -11.00\tAverage reward: -12.30\n",
      "Episode 96\tLast reward: -7.00\tAverage reward: -12.04\n",
      "Episode 97\tLast reward: -15.00\tAverage reward: -12.19\n",
      "Episode 98\tLast reward: -11.00\tAverage reward: -12.13\n",
      "Episode 99\tLast reward: -8.00\tAverage reward: -11.92\n",
      "Episode 100\tLast reward: -15.00\tAverage reward: -12.08\n",
      "Episode 101\tLast reward: -11.00\tAverage reward: -12.02\n",
      "Episode 102\tLast reward: -6.00\tAverage reward: -11.72\n",
      "Episode 103\tLast reward: -3.00\tAverage reward: -11.28\n",
      "Episode 104\tLast reward: -7.00\tAverage reward: -11.07\n",
      "Episode 105\tLast reward: -16.00\tAverage reward: -11.32\n",
      "Episode 106\tLast reward: -13.00\tAverage reward: -11.40\n",
      "Episode 107\tLast reward: -15.00\tAverage reward: -11.58\n",
      "Episode 108\tLast reward: -13.00\tAverage reward: -11.65\n",
      "Episode 109\tLast reward: -17.00\tAverage reward: -11.92\n",
      "Episode 110\tLast reward: -13.00\tAverage reward: -11.97\n",
      "Episode 111\tLast reward: -14.00\tAverage reward: -12.07\n",
      "Episode 112\tLast reward: -9.00\tAverage reward: -11.92\n",
      "Episode 113\tLast reward: -12.00\tAverage reward: -11.92\n",
      "Episode 114\tLast reward: -13.00\tAverage reward: -11.98\n",
      "Episode 115\tLast reward: -8.00\tAverage reward: -11.78\n",
      "Episode 116\tLast reward: -11.00\tAverage reward: -11.74\n",
      "Episode 117\tLast reward: -16.00\tAverage reward: -11.95\n",
      "Episode 118\tLast reward: -14.00\tAverage reward: -12.06\n",
      "Episode 119\tLast reward: -15.00\tAverage reward: -12.20\n",
      "Episode 120\tLast reward: -7.00\tAverage reward: -11.94\n",
      "Episode 121\tLast reward: -19.00\tAverage reward: -12.30\n",
      "Episode 122\tLast reward: -13.00\tAverage reward: -12.33\n",
      "Episode 123\tLast reward: -13.00\tAverage reward: -12.36\n",
      "Episode 124\tLast reward: -13.00\tAverage reward: -12.40\n",
      "Episode 125\tLast reward: -11.00\tAverage reward: -12.33\n",
      "Episode 126\tLast reward: -17.00\tAverage reward: -12.56\n",
      "Episode 127\tLast reward: -12.00\tAverage reward: -12.53\n",
      "Episode 128\tLast reward: -12.00\tAverage reward: -12.51\n",
      "Episode 129\tLast reward: -13.00\tAverage reward: -12.53\n",
      "Episode 130\tLast reward: -13.00\tAverage reward: -12.55\n",
      "Episode 131\tLast reward: -18.00\tAverage reward: -12.83\n",
      "Episode 132\tLast reward: -15.00\tAverage reward: -12.93\n",
      "Episode 133\tLast reward: -9.00\tAverage reward: -12.74\n",
      "Episode 134\tLast reward: -10.00\tAverage reward: -12.60\n",
      "Episode 135\tLast reward: -11.00\tAverage reward: -12.52\n",
      "Episode 136\tLast reward: -17.00\tAverage reward: -12.75\n",
      "Episode 137\tLast reward: -4.00\tAverage reward: -12.31\n",
      "Episode 138\tLast reward: -12.00\tAverage reward: -12.29\n",
      "Episode 139\tLast reward: -11.00\tAverage reward: -12.23\n",
      "Episode 140\tLast reward: -13.00\tAverage reward: -12.27\n",
      "Episode 141\tLast reward: -12.00\tAverage reward: -12.25\n",
      "Episode 142\tLast reward: -12.00\tAverage reward: -12.24\n",
      "Episode 143\tLast reward: -14.00\tAverage reward: -12.33\n",
      "Episode 144\tLast reward: -17.00\tAverage reward: -12.56\n",
      "Episode 145\tLast reward: -8.00\tAverage reward: -12.33\n",
      "Episode 146\tLast reward: -8.00\tAverage reward: -12.12\n",
      "Episode 147\tLast reward: -7.00\tAverage reward: -11.86\n",
      "Episode 148\tLast reward: -14.00\tAverage reward: -11.97\n",
      "Episode 149\tLast reward: -9.00\tAverage reward: -11.82\n",
      "Episode 150\tLast reward: -13.00\tAverage reward: -11.88\n",
      "Episode 151\tLast reward: -11.00\tAverage reward: -11.83\n",
      "Episode 152\tLast reward: -12.00\tAverage reward: -11.84\n",
      "Episode 153\tLast reward: -14.00\tAverage reward: -11.95\n",
      "Episode 154\tLast reward: -11.00\tAverage reward: -11.90\n",
      "Episode 155\tLast reward: -12.00\tAverage reward: -11.91\n",
      "Episode 156\tLast reward: -11.00\tAverage reward: -11.86\n",
      "Episode 157\tLast reward: -15.00\tAverage reward: -12.02\n",
      "Episode 158\tLast reward: -12.00\tAverage reward: -12.02\n",
      "Episode 159\tLast reward: -9.00\tAverage reward: -11.87\n",
      "Episode 160\tLast reward: -1.00\tAverage reward: -11.32\n",
      "Episode 161\tLast reward: -7.00\tAverage reward: -11.11\n",
      "Episode 162\tLast reward: -9.00\tAverage reward: -11.00\n",
      "Episode 163\tLast reward: -17.00\tAverage reward: -11.30\n",
      "Episode 164\tLast reward: -17.00\tAverage reward: -11.59\n",
      "Episode 165\tLast reward: -16.00\tAverage reward: -11.81\n",
      "Episode 166\tLast reward: -17.00\tAverage reward: -12.07\n",
      "Episode 167\tLast reward: -15.00\tAverage reward: -12.21\n",
      "Episode 168\tLast reward: -19.00\tAverage reward: -12.55\n",
      "Episode 169\tLast reward: -16.00\tAverage reward: -12.73\n",
      "Episode 170\tLast reward: -13.00\tAverage reward: -12.74\n",
      "Episode 171\tLast reward: -5.00\tAverage reward: -12.35\n",
      "Episode 172\tLast reward: -13.00\tAverage reward: -12.39\n",
      "Episode 173\tLast reward: -10.00\tAverage reward: -12.27\n",
      "Episode 174\tLast reward: -11.00\tAverage reward: -12.20\n",
      "Episode 175\tLast reward: -13.00\tAverage reward: -12.24\n",
      "Episode 176\tLast reward: -15.00\tAverage reward: -12.38\n",
      "Episode 177\tLast reward: -13.00\tAverage reward: -12.41\n",
      "Episode 178\tLast reward: -10.00\tAverage reward: -12.29\n",
      "Episode 179\tLast reward: -17.00\tAverage reward: -12.53\n",
      "Episode 180\tLast reward: -16.00\tAverage reward: -12.70\n",
      "Episode 181\tLast reward: -15.00\tAverage reward: -12.81\n",
      "Episode 182\tLast reward: -7.00\tAverage reward: -12.52\n",
      "Episode 183\tLast reward: -13.00\tAverage reward: -12.55\n",
      "Episode 184\tLast reward: -10.00\tAverage reward: -12.42\n",
      "Episode 185\tLast reward: -14.00\tAverage reward: -12.50\n",
      "Episode 186\tLast reward: -11.00\tAverage reward: -12.42\n",
      "Episode 187\tLast reward: -8.00\tAverage reward: -12.20\n",
      "Episode 188\tLast reward: -10.00\tAverage reward: -12.09\n",
      "Episode 189\tLast reward: -9.00\tAverage reward: -11.94\n",
      "Episode 190\tLast reward: -15.00\tAverage reward: -12.09\n",
      "Episode 191\tLast reward: -13.00\tAverage reward: -12.14\n",
      "Episode 192\tLast reward: -14.00\tAverage reward: -12.23\n",
      "Episode 193\tLast reward: -13.00\tAverage reward: -12.27\n",
      "Episode 194\tLast reward: -8.00\tAverage reward: -12.06\n",
      "Episode 195\tLast reward: -10.00\tAverage reward: -11.95\n",
      "Episode 196\tLast reward: -10.00\tAverage reward: -11.85\n",
      "Episode 197\tLast reward: -7.00\tAverage reward: -11.61\n",
      "Episode 198\tLast reward: -11.00\tAverage reward: -11.58\n",
      "Episode 199\tLast reward: -17.00\tAverage reward: -11.85\n",
      "Episode 200\tLast reward: -19.00\tAverage reward: -12.21\n",
      "Episode 201\tLast reward: -13.00\tAverage reward: -12.25\n",
      "Episode 202\tLast reward: -10.00\tAverage reward: -12.14\n",
      "Episode 203\tLast reward: -10.00\tAverage reward: -12.03\n",
      "Episode 204\tLast reward: -13.00\tAverage reward: -12.08\n",
      "Episode 205\tLast reward: -13.00\tAverage reward: -12.12\n",
      "Episode 206\tLast reward: -8.00\tAverage reward: -11.92\n",
      "Episode 207\tLast reward: -10.00\tAverage reward: -11.82\n",
      "Episode 208\tLast reward: -8.00\tAverage reward: -11.63\n",
      "Episode 209\tLast reward: -10.00\tAverage reward: -11.55\n",
      "Episode 210\tLast reward: -14.00\tAverage reward: -11.67\n",
      "Episode 211\tLast reward: -13.00\tAverage reward: -11.74\n",
      "Episode 212\tLast reward: -13.00\tAverage reward: -11.80\n",
      "Episode 213\tLast reward: -10.00\tAverage reward: -11.71\n",
      "Episode 214\tLast reward: -13.00\tAverage reward: -11.78\n",
      "Episode 215\tLast reward: -15.00\tAverage reward: -11.94\n",
      "Episode 216\tLast reward: -6.00\tAverage reward: -11.64\n",
      "Episode 217\tLast reward: -5.00\tAverage reward: -11.31\n",
      "Episode 218\tLast reward: -9.00\tAverage reward: -11.19\n",
      "Episode 219\tLast reward: -14.00\tAverage reward: -11.33\n",
      "Episode 220\tLast reward: -14.00\tAverage reward: -11.47\n",
      "Episode 221\tLast reward: -14.00\tAverage reward: -11.59\n",
      "Episode 222\tLast reward: -15.00\tAverage reward: -11.76\n",
      "Episode 223\tLast reward: -8.00\tAverage reward: -11.58\n",
      "Episode 224\tLast reward: -13.00\tAverage reward: -11.65\n",
      "Episode 225\tLast reward: -11.00\tAverage reward: -11.61\n",
      "Episode 226\tLast reward: -15.00\tAverage reward: -11.78\n",
      "Episode 227\tLast reward: -7.00\tAverage reward: -11.54\n",
      "Episode 228\tLast reward: -11.00\tAverage reward: -11.52\n",
      "Episode 229\tLast reward: -17.00\tAverage reward: -11.79\n",
      "Episode 230\tLast reward: -13.00\tAverage reward: -11.85\n",
      "Episode 231\tLast reward: -17.00\tAverage reward: -12.11\n",
      "Episode 232\tLast reward: -9.00\tAverage reward: -11.95\n",
      "Episode 233\tLast reward: -4.00\tAverage reward: -11.56\n",
      "Episode 234\tLast reward: -15.00\tAverage reward: -11.73\n",
      "Episode 235\tLast reward: -12.00\tAverage reward: -11.74\n",
      "Episode 236\tLast reward: -8.00\tAverage reward: -11.55\n",
      "Episode 237\tLast reward: -7.00\tAverage reward: -11.33\n",
      "Episode 238\tLast reward: -9.00\tAverage reward: -11.21\n",
      "Episode 239\tLast reward: -14.00\tAverage reward: -11.35\n",
      "Episode 240\tLast reward: -14.00\tAverage reward: -11.48\n",
      "Episode 241\tLast reward: -9.00\tAverage reward: -11.36\n",
      "Episode 242\tLast reward: -13.00\tAverage reward: -11.44\n",
      "Episode 243\tLast reward: -15.00\tAverage reward: -11.62\n",
      "Episode 244\tLast reward: -16.00\tAverage reward: -11.84\n",
      "Episode 245\tLast reward: -7.00\tAverage reward: -11.60\n",
      "Episode 246\tLast reward: -6.00\tAverage reward: -11.32\n",
      "Episode 247\tLast reward: -9.00\tAverage reward: -11.20\n",
      "Episode 248\tLast reward: -17.00\tAverage reward: -11.49\n",
      "Episode 249\tLast reward: -11.00\tAverage reward: -11.47\n",
      "Episode 250\tLast reward: -11.00\tAverage reward: -11.44\n",
      "Episode 251\tLast reward: -3.00\tAverage reward: -11.02\n",
      "Episode 252\tLast reward: -17.00\tAverage reward: -11.32\n",
      "Episode 253\tLast reward: -16.00\tAverage reward: -11.55\n",
      "Episode 254\tLast reward: -12.00\tAverage reward: -11.58\n",
      "Episode 255\tLast reward: -11.00\tAverage reward: -11.55\n",
      "Episode 256\tLast reward: -15.00\tAverage reward: -11.72\n",
      "Episode 257\tLast reward: -14.00\tAverage reward: -11.83\n",
      "Episode 258\tLast reward: -15.00\tAverage reward: -11.99\n",
      "Episode 259\tLast reward: -6.00\tAverage reward: -11.69\n",
      "Episode 260\tLast reward: -13.00\tAverage reward: -11.76\n",
      "Episode 261\tLast reward: -14.00\tAverage reward: -11.87\n",
      "Episode 262\tLast reward: -11.00\tAverage reward: -11.83\n",
      "Episode 263\tLast reward: -1.00\tAverage reward: -11.28\n",
      "Episode 264\tLast reward: -4.00\tAverage reward: -10.92\n",
      "Episode 265\tLast reward: -13.00\tAverage reward: -11.02\n",
      "Episode 266\tLast reward: -8.00\tAverage reward: -10.87\n",
      "Episode 267\tLast reward: -8.00\tAverage reward: -10.73\n",
      "Episode 268\tLast reward: -15.00\tAverage reward: -10.94\n",
      "Episode 269\tLast reward: -9.00\tAverage reward: -10.85\n",
      "Episode 270\tLast reward: -8.00\tAverage reward: -10.70\n",
      "Episode 271\tLast reward: -17.00\tAverage reward: -11.02\n",
      "Episode 272\tLast reward: -11.00\tAverage reward: -11.02\n",
      "Episode 273\tLast reward: -12.00\tAverage reward: -11.07\n",
      "Episode 274\tLast reward: -8.00\tAverage reward: -10.91\n",
      "Episode 275\tLast reward: -10.00\tAverage reward: -10.87\n",
      "Episode 276\tLast reward: -13.00\tAverage reward: -10.97\n",
      "Episode 277\tLast reward: -14.00\tAverage reward: -11.13\n",
      "Episode 278\tLast reward: -15.00\tAverage reward: -11.32\n",
      "Episode 279\tLast reward: -10.00\tAverage reward: -11.25\n",
      "Episode 280\tLast reward: -17.00\tAverage reward: -11.54\n",
      "Episode 281\tLast reward: -10.00\tAverage reward: -11.46\n",
      "Episode 282\tLast reward: -14.00\tAverage reward: -11.59\n",
      "Episode 283\tLast reward: -10.00\tAverage reward: -11.51\n",
      "Episode 284\tLast reward: -12.00\tAverage reward: -11.54\n",
      "Episode 285\tLast reward: -15.00\tAverage reward: -11.71\n",
      "Episode 286\tLast reward: -6.00\tAverage reward: -11.42\n",
      "Episode 287\tLast reward: -15.00\tAverage reward: -11.60\n",
      "Episode 288\tLast reward: -15.00\tAverage reward: -11.77\n",
      "Episode 289\tLast reward: -6.00\tAverage reward: -11.48\n",
      "Episode 290\tLast reward: -15.00\tAverage reward: -11.66\n",
      "Episode 291\tLast reward: -13.00\tAverage reward: -11.73\n",
      "Episode 292\tLast reward: -15.00\tAverage reward: -11.89\n",
      "Episode 293\tLast reward: -8.00\tAverage reward: -11.70\n",
      "Episode 294\tLast reward: -8.00\tAverage reward: -11.51\n",
      "Episode 295\tLast reward: -13.00\tAverage reward: -11.59\n",
      "Episode 296\tLast reward: -17.00\tAverage reward: -11.86\n",
      "Episode 297\tLast reward: -18.00\tAverage reward: -12.16\n",
      "Episode 298\tLast reward: -15.00\tAverage reward: -12.30\n",
      "Episode 299\tLast reward: -13.00\tAverage reward: -12.34\n",
      "Episode 300\tLast reward: -15.00\tAverage reward: -12.47\n",
      "Episode 301\tLast reward: -8.00\tAverage reward: -12.25\n",
      "Episode 302\tLast reward: -9.00\tAverage reward: -12.09\n",
      "Episode 303\tLast reward: -7.00\tAverage reward: -11.83\n",
      "Episode 304\tLast reward: -12.00\tAverage reward: -11.84\n",
      "Episode 305\tLast reward: -11.00\tAverage reward: -11.80\n",
      "Episode 306\tLast reward: -15.00\tAverage reward: -11.96\n",
      "Episode 307\tLast reward: -13.00\tAverage reward: -12.01\n",
      "Episode 308\tLast reward: -15.00\tAverage reward: -12.16\n",
      "Episode 309\tLast reward: -18.00\tAverage reward: -12.45\n",
      "Episode 310\tLast reward: -9.00\tAverage reward: -12.28\n",
      "Episode 311\tLast reward: -15.00\tAverage reward: -12.42\n",
      "Episode 312\tLast reward: -9.00\tAverage reward: -12.24\n",
      "Episode 313\tLast reward: -11.00\tAverage reward: -12.18\n",
      "Episode 314\tLast reward: -9.00\tAverage reward: -12.02\n",
      "Episode 315\tLast reward: -18.00\tAverage reward: -12.32\n",
      "Episode 316\tLast reward: -11.00\tAverage reward: -12.26\n",
      "Episode 317\tLast reward: -16.00\tAverage reward: -12.44\n",
      "Episode 318\tLast reward: -9.00\tAverage reward: -12.27\n",
      "Episode 319\tLast reward: -3.00\tAverage reward: -11.81\n",
      "Episode 320\tLast reward: -14.00\tAverage reward: -11.92\n",
      "Episode 321\tLast reward: -10.00\tAverage reward: -11.82\n",
      "Episode 322\tLast reward: -10.00\tAverage reward: -11.73\n",
      "Episode 323\tLast reward: -5.00\tAverage reward: -11.39\n",
      "Episode 324\tLast reward: -14.00\tAverage reward: -11.52\n",
      "Episode 325\tLast reward: -5.00\tAverage reward: -11.20\n",
      "Episode 326\tLast reward: -17.00\tAverage reward: -11.49\n",
      "Episode 327\tLast reward: -13.00\tAverage reward: -11.56\n",
      "Episode 328\tLast reward: -7.00\tAverage reward: -11.34\n",
      "Episode 329\tLast reward: -17.00\tAverage reward: -11.62\n",
      "Episode 330\tLast reward: -12.00\tAverage reward: -11.64\n",
      "Episode 331\tLast reward: -9.00\tAverage reward: -11.51\n",
      "Episode 332\tLast reward: -11.00\tAverage reward: -11.48\n",
      "Episode 333\tLast reward: -16.00\tAverage reward: -11.71\n",
      "Episode 334\tLast reward: -11.00\tAverage reward: -11.67\n",
      "Episode 335\tLast reward: -5.00\tAverage reward: -11.34\n",
      "Episode 336\tLast reward: -13.00\tAverage reward: -11.42\n",
      "Episode 337\tLast reward: -11.00\tAverage reward: -11.40\n",
      "Episode 338\tLast reward: -13.00\tAverage reward: -11.48\n",
      "Episode 339\tLast reward: -13.00\tAverage reward: -11.56\n",
      "Episode 340\tLast reward: -13.00\tAverage reward: -11.63\n",
      "Episode 341\tLast reward: -15.00\tAverage reward: -11.80\n",
      "Episode 342\tLast reward: -9.00\tAverage reward: -11.66\n",
      "Episode 343\tLast reward: -10.00\tAverage reward: -11.57\n",
      "Episode 344\tLast reward: -15.00\tAverage reward: -11.75\n",
      "Episode 345\tLast reward: -4.00\tAverage reward: -11.36\n",
      "Episode 346\tLast reward: -9.00\tAverage reward: -11.24\n",
      "Episode 347\tLast reward: -15.00\tAverage reward: -11.43\n",
      "Episode 348\tLast reward: -13.00\tAverage reward: -11.51\n",
      "Episode 349\tLast reward: -15.00\tAverage reward: -11.68\n",
      "Episode 350\tLast reward: -7.00\tAverage reward: -11.45\n",
      "Episode 351\tLast reward: -7.00\tAverage reward: -11.22\n",
      "Episode 352\tLast reward: -12.00\tAverage reward: -11.26\n",
      "Episode 353\tLast reward: -13.00\tAverage reward: -11.35\n",
      "Episode 354\tLast reward: -16.00\tAverage reward: -11.58\n",
      "Episode 355\tLast reward: -12.00\tAverage reward: -11.60\n",
      "Episode 356\tLast reward: -11.00\tAverage reward: -11.57\n",
      "Episode 357\tLast reward: -9.00\tAverage reward: -11.44\n",
      "Episode 358\tLast reward: -19.00\tAverage reward: -11.82\n",
      "Episode 359\tLast reward: -17.00\tAverage reward: -12.08\n",
      "Episode 360\tLast reward: -16.00\tAverage reward: -12.28\n",
      "Episode 361\tLast reward: -17.00\tAverage reward: -12.51\n",
      "Episode 362\tLast reward: -7.00\tAverage reward: -12.24\n",
      "Episode 363\tLast reward: -12.00\tAverage reward: -12.23\n",
      "Episode 364\tLast reward: -13.00\tAverage reward: -12.26\n",
      "Episode 365\tLast reward: -6.00\tAverage reward: -11.95\n",
      "Episode 366\tLast reward: -9.00\tAverage reward: -11.80\n",
      "Episode 367\tLast reward: -13.00\tAverage reward: -11.86\n",
      "Episode 368\tLast reward: -10.00\tAverage reward: -11.77\n",
      "Episode 369\tLast reward: -10.00\tAverage reward: -11.68\n",
      "Episode 370\tLast reward: -17.00\tAverage reward: -11.95\n",
      "Episode 371\tLast reward: -20.00\tAverage reward: -12.35\n",
      "Episode 372\tLast reward: -8.00\tAverage reward: -12.13\n",
      "Episode 373\tLast reward: -8.00\tAverage reward: -11.93\n",
      "Episode 374\tLast reward: -1.00\tAverage reward: -11.38\n",
      "Episode 375\tLast reward: -11.00\tAverage reward: -11.36\n",
      "Episode 376\tLast reward: -16.00\tAverage reward: -11.59\n",
      "Episode 377\tLast reward: -15.00\tAverage reward: -11.76\n",
      "Episode 378\tLast reward: -13.00\tAverage reward: -11.83\n",
      "Episode 379\tLast reward: -11.00\tAverage reward: -11.78\n",
      "Episode 380\tLast reward: -12.00\tAverage reward: -11.79\n",
      "Episode 381\tLast reward: -16.00\tAverage reward: -12.00\n",
      "Episode 382\tLast reward: -15.00\tAverage reward: -12.15\n",
      "Episode 383\tLast reward: -1.00\tAverage reward: -11.60\n",
      "Episode 384\tLast reward: -16.00\tAverage reward: -11.82\n",
      "Episode 385\tLast reward: -5.00\tAverage reward: -11.48\n",
      "Episode 386\tLast reward: -12.00\tAverage reward: -11.50\n",
      "Episode 387\tLast reward: -11.00\tAverage reward: -11.48\n",
      "Episode 388\tLast reward: -13.00\tAverage reward: -11.55\n",
      "Episode 389\tLast reward: -9.00\tAverage reward: -11.43\n",
      "Episode 390\tLast reward: 3.00\tAverage reward: -10.70\n",
      "Episode 391\tLast reward: -11.00\tAverage reward: -10.72\n",
      "Episode 392\tLast reward: -14.00\tAverage reward: -10.88\n",
      "Episode 393\tLast reward: -13.00\tAverage reward: -10.99\n",
      "Episode 394\tLast reward: -19.00\tAverage reward: -11.39\n",
      "Episode 395\tLast reward: -12.00\tAverage reward: -11.42\n",
      "Episode 396\tLast reward: -5.00\tAverage reward: -11.10\n",
      "Episode 397\tLast reward: -15.00\tAverage reward: -11.29\n",
      "Episode 398\tLast reward: -15.00\tAverage reward: -11.48\n",
      "Episode 399\tLast reward: -11.00\tAverage reward: -11.46\n",
      "Episode 400\tLast reward: -19.00\tAverage reward: -11.83\n",
      "Episode 401\tLast reward: -15.00\tAverage reward: -11.99\n",
      "Episode 402\tLast reward: -13.00\tAverage reward: -12.04\n",
      "Episode 403\tLast reward: -10.00\tAverage reward: -11.94\n",
      "Episode 404\tLast reward: -13.00\tAverage reward: -11.99\n",
      "Episode 405\tLast reward: -11.00\tAverage reward: -11.94\n",
      "Episode 406\tLast reward: -15.00\tAverage reward: -12.10\n",
      "Episode 407\tLast reward: -16.00\tAverage reward: -12.29\n",
      "Episode 408\tLast reward: -15.00\tAverage reward: -12.43\n",
      "Episode 409\tLast reward: -15.00\tAverage reward: -12.56\n",
      "Episode 410\tLast reward: -11.00\tAverage reward: -12.48\n",
      "Episode 411\tLast reward: -12.00\tAverage reward: -12.45\n",
      "Episode 412\tLast reward: -11.00\tAverage reward: -12.38\n",
      "Episode 413\tLast reward: -10.00\tAverage reward: -12.26\n",
      "Episode 414\tLast reward: -9.00\tAverage reward: -12.10\n",
      "Episode 415\tLast reward: -2.00\tAverage reward: -11.59\n",
      "Episode 416\tLast reward: -10.00\tAverage reward: -11.51\n",
      "Episode 417\tLast reward: -5.00\tAverage reward: -11.19\n",
      "Episode 418\tLast reward: -11.00\tAverage reward: -11.18\n",
      "Episode 419\tLast reward: -13.00\tAverage reward: -11.27\n",
      "Episode 420\tLast reward: -16.00\tAverage reward: -11.51\n",
      "Episode 421\tLast reward: -18.00\tAverage reward: -11.83\n",
      "Episode 422\tLast reward: -15.00\tAverage reward: -11.99\n",
      "Episode 423\tLast reward: -14.00\tAverage reward: -12.09\n",
      "Episode 424\tLast reward: -16.00\tAverage reward: -12.29\n",
      "Episode 425\tLast reward: -18.00\tAverage reward: -12.57\n",
      "Episode 426\tLast reward: -4.00\tAverage reward: -12.14\n",
      "Episode 427\tLast reward: -15.00\tAverage reward: -12.29\n",
      "Episode 428\tLast reward: -5.00\tAverage reward: -11.92\n",
      "Episode 429\tLast reward: -7.00\tAverage reward: -11.68\n",
      "Episode 430\tLast reward: -11.00\tAverage reward: -11.64\n",
      "Episode 431\tLast reward: -17.00\tAverage reward: -11.91\n",
      "Episode 432\tLast reward: -11.00\tAverage reward: -11.86\n",
      "Episode 433\tLast reward: -10.00\tAverage reward: -11.77\n",
      "Episode 434\tLast reward: -13.00\tAverage reward: -11.83\n",
      "Episode 435\tLast reward: -9.00\tAverage reward: -11.69\n",
      "Episode 436\tLast reward: -10.00\tAverage reward: -11.61\n",
      "Episode 437\tLast reward: -11.00\tAverage reward: -11.58\n",
      "Episode 438\tLast reward: -10.00\tAverage reward: -11.50\n",
      "Episode 439\tLast reward: -9.00\tAverage reward: -11.37\n",
      "Episode 440\tLast reward: -13.00\tAverage reward: -11.45\n",
      "Episode 441\tLast reward: -11.00\tAverage reward: -11.43\n",
      "Episode 442\tLast reward: -17.00\tAverage reward: -11.71\n",
      "Episode 443\tLast reward: -6.00\tAverage reward: -11.42\n",
      "Episode 444\tLast reward: -13.00\tAverage reward: -11.50\n",
      "Episode 445\tLast reward: -9.00\tAverage reward: -11.38\n",
      "Episode 446\tLast reward: -10.00\tAverage reward: -11.31\n",
      "Episode 447\tLast reward: -9.00\tAverage reward: -11.19\n",
      "Episode 448\tLast reward: -15.00\tAverage reward: -11.38\n",
      "Episode 449\tLast reward: -17.00\tAverage reward: -11.66\n",
      "Episode 450\tLast reward: -7.00\tAverage reward: -11.43\n",
      "Episode 451\tLast reward: -17.00\tAverage reward: -11.71\n",
      "Episode 452\tLast reward: -10.00\tAverage reward: -11.62\n",
      "Episode 453\tLast reward: -3.00\tAverage reward: -11.19\n",
      "Episode 454\tLast reward: -6.00\tAverage reward: -10.93\n",
      "Episode 455\tLast reward: -13.00\tAverage reward: -11.04\n",
      "Episode 456\tLast reward: -3.00\tAverage reward: -10.63\n",
      "Episode 457\tLast reward: -13.00\tAverage reward: -10.75\n",
      "Episode 458\tLast reward: -9.00\tAverage reward: -10.67\n",
      "Episode 459\tLast reward: -15.00\tAverage reward: -10.88\n",
      "Episode 460\tLast reward: -4.00\tAverage reward: -10.54\n",
      "Episode 461\tLast reward: -17.00\tAverage reward: -10.86\n",
      "Episode 462\tLast reward: -16.00\tAverage reward: -11.12\n",
      "Episode 463\tLast reward: -16.00\tAverage reward: -11.36\n",
      "Episode 464\tLast reward: -12.00\tAverage reward: -11.39\n",
      "Episode 465\tLast reward: -11.00\tAverage reward: -11.37\n",
      "Episode 466\tLast reward: -9.00\tAverage reward: -11.26\n",
      "Episode 467\tLast reward: -16.00\tAverage reward: -11.49\n",
      "Episode 468\tLast reward: -17.00\tAverage reward: -11.77\n",
      "Episode 469\tLast reward: -16.00\tAverage reward: -11.98\n",
      "Episode 470\tLast reward: -13.00\tAverage reward: -12.03\n",
      "Episode 471\tLast reward: -12.00\tAverage reward: -12.03\n",
      "Episode 472\tLast reward: -5.00\tAverage reward: -11.68\n",
      "Episode 473\tLast reward: -11.00\tAverage reward: -11.64\n",
      "Episode 474\tLast reward: -17.00\tAverage reward: -11.91\n",
      "Episode 475\tLast reward: -11.00\tAverage reward: -11.87\n",
      "Episode 476\tLast reward: -13.00\tAverage reward: -11.92\n",
      "Episode 477\tLast reward: -10.00\tAverage reward: -11.83\n",
      "Episode 478\tLast reward: -15.00\tAverage reward: -11.99\n",
      "Episode 479\tLast reward: 3.00\tAverage reward: -11.24\n",
      "Episode 480\tLast reward: -13.00\tAverage reward: -11.32\n",
      "Episode 481\tLast reward: -16.00\tAverage reward: -11.56\n",
      "Episode 482\tLast reward: -18.00\tAverage reward: -11.88\n",
      "Episode 483\tLast reward: -6.00\tAverage reward: -11.59\n",
      "Episode 484\tLast reward: -13.00\tAverage reward: -11.66\n",
      "Episode 485\tLast reward: -9.00\tAverage reward: -11.52\n",
      "Episode 486\tLast reward: -10.00\tAverage reward: -11.45\n",
      "Episode 487\tLast reward: -16.00\tAverage reward: -11.68\n",
      "Episode 488\tLast reward: -15.00\tAverage reward: -11.84\n",
      "Episode 489\tLast reward: -13.00\tAverage reward: -11.90\n",
      "Episode 490\tLast reward: -10.00\tAverage reward: -11.80\n",
      "Episode 491\tLast reward: -17.00\tAverage reward: -12.06\n",
      "Episode 492\tLast reward: -13.00\tAverage reward: -12.11\n",
      "Episode 493\tLast reward: -13.00\tAverage reward: -12.16\n",
      "Episode 494\tLast reward: -16.00\tAverage reward: -12.35\n",
      "Episode 495\tLast reward: -11.00\tAverage reward: -12.28\n",
      "Episode 496\tLast reward: -9.00\tAverage reward: -12.12\n",
      "Episode 497\tLast reward: -8.00\tAverage reward: -11.91\n",
      "Episode 498\tLast reward: -9.00\tAverage reward: -11.77\n",
      "Episode 499\tLast reward: -17.00\tAverage reward: -12.03\n",
      "Episode 500\tLast reward: -14.00\tAverage reward: -12.13\n",
      "Episode 501\tLast reward: -3.00\tAverage reward: -11.67\n",
      "Episode 502\tLast reward: -9.00\tAverage reward: -11.54\n",
      "Episode 503\tLast reward: -13.00\tAverage reward: -11.61\n",
      "Episode 504\tLast reward: -10.00\tAverage reward: -11.53\n",
      "Episode 505\tLast reward: -10.00\tAverage reward: -11.45\n",
      "Episode 506\tLast reward: -11.00\tAverage reward: -11.43\n",
      "Episode 507\tLast reward: -13.00\tAverage reward: -11.51\n",
      "Episode 508\tLast reward: -9.00\tAverage reward: -11.38\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9753b40e3b8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mc_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mc_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     '''\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in count(1):\n",
    "    action_log_probs = list()\n",
    "    rewards = list()\n",
    "    values = list()\n",
    "    state = env.reset()\n",
    "    ##test = net(torch.FloatTensor(state).unsqueeze(0))\n",
    "    ##print(test)\n",
    "    prev_x = None\n",
    "    #print(state.shape)\n",
    "    ##counter = 0\n",
    "    for t in range(100000):\n",
    "    #while True:\n",
    "        #env.render()\n",
    "        \n",
    "        cur_x  = preprocess(state)\n",
    "        #print(\"cur_x = \", cur_x.shape)\n",
    "        state  = cur_x - prev_x if prev_x is not None else preprocess(prev_x)##np.zeros(input_shape)\n",
    "        #print(\"state = \", state.shape)\n",
    "        prev_x = cur_x\n",
    "        \n",
    "        ## so the tensor given to the model should be of shape [batch_size, 1, height, width].\n",
    "        \n",
    "        \n",
    "        ##state = torch.FloatTensor(state)\n",
    "        \n",
    "        state = state.reshape(1, 1, 80, 80)\n",
    "        \n",
    "        #env.render()\n",
    "        ## take an action sampled from a categorical distribution given the state\n",
    "        action_prob = pnet(torch.FloatTensor(state).to(device))\n",
    "        action = action_prob.sample()\n",
    "        action_log_probs.append(action_prob.log_prob(action))\n",
    "        \n",
    "        if action.item() == 0:\n",
    "            action = UP_ACTION\n",
    "        else:\n",
    "            action = DOWN_ACTION\n",
    "        \n",
    "        #print(entropy)\n",
    "        value = cnet(torch.FloatTensor(state).to(device))\n",
    "        values.append(value[0])\n",
    "        #print(action)\n",
    "        next_state, reward, is_done, _ = env.step(action) # take a random action\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        ## current state is next state now\n",
    "        state = next_state\n",
    "\n",
    "        if is_done:\n",
    "            #print(rewards)\n",
    "            #print(values)\n",
    "            break\n",
    "            \n",
    "       \n",
    "            \n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "                \n",
    "    ## Now we have the discounted reward + log_probs of the actions\n",
    "    returns = discounted_returns(rewards)\n",
    "    #print(returns)\n",
    "    action_losses = list()\n",
    "    critic_losses = list()\n",
    "    ## collect the action losses to a list\n",
    "    for ret, l_prob, v in zip(returns, action_log_probs, values):\n",
    "        advantage = ret - v\n",
    "        #print(advantage)\n",
    "        #print(-l_prob * ret)\n",
    "        action_losses.append(-l_prob * advantage.detach())\n",
    "        critic_losses.append(advantage.pow(2))\n",
    "\n",
    "    p_optimizer.zero_grad()\n",
    "    ## accumulate the action losses\n",
    "    action_loss = torch.cat(action_losses).sum()\n",
    "    action_loss.backward()\n",
    "    ## step the optimizer\n",
    "    p_optimizer.step()\n",
    "\n",
    "    c_optimizer.zero_grad()\n",
    "    critic_loss = torch.cat(critic_losses).mean()\n",
    "    critic_loss.backward()\n",
    "    c_optimizer.step()\n",
    "    '''\n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "    '''\n",
    "      \n",
    "    if running_reward >= MEAN_REWARD_BOUND:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n",
    "    \n",
    "env.close()\n",
    "\n",
    "\n",
    "            \n",
    "##:.2f\n",
    "##'Episode {}\\t Last reward: {}\\t Average reward: {}'.format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for the last part and introducing the next one \n",
    "\n",
    "The results was not that good, so we decided to decrease the learning rate more to be 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the environment\n",
    "## env = gym.make('PongNoFrameskip-v4')\n",
    "env = gym.make('Pong-v0')\n",
    "##input_shape = env.observation_space.shape[0]\n",
    "#action_size = env.action_space.n\n",
    "\n",
    "action_size = 2\n",
    "\n",
    "##print(\"Env reward threshold: {}\".format(env.spec.reward_threshold))\n",
    "reward_list = list()\n",
    "\n",
    "input_shape = [1,80,80]\n",
    "\n",
    "##input_shape = [STACK_SIZE, 84, 84]\n",
    "\n",
    "\n",
    "pnet = torch.load('../input/a2c500amsgrad/500_A2C_pnet.pt').to(device)##,map_location=torch.device('cpu'))\n",
    "pnet.eval()\n",
    "\n",
    "cnet = torch.load('../input/a2c500amsgrad/500_A2C_cnet.pt').to(device)##,map_location=torch.device('cpu'))\n",
    "cnet.eval()\n",
    "\n",
    "\n",
    "## initialize an optimizer\n",
    "p_optimizer = torch.optim.Adam(pnet.parameters(), lr=1e-6,eps=1e-3,amsgrad=True)\n",
    "c_optimizer = torch.optim.Adam(cnet.parameters(), lr=1e-6,eps=1e-3,amsgrad=True)\n",
    "\n",
    "\n",
    "\n",
    "running_reward  = None\n",
    "MEAN_REWARD_BOUND = 20\n",
    "\n",
    "UP_ACTION = 2\n",
    "\n",
    "DOWN_ACTION = 3\n",
    "Max_ep_rewared = -21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tLast reward: -15.00\tAverage reward: -15.00\n",
      "Episode 2\tLast reward: -13.00\tAverage reward: -14.90\n",
      "Episode 3\tLast reward: -17.00\tAverage reward: -15.00\n",
      "Episode 4\tLast reward: -13.00\tAverage reward: -14.90\n",
      "Episode 5\tLast reward: -15.00\tAverage reward: -14.91\n",
      "Episode 6\tLast reward: -4.00\tAverage reward: -14.36\n",
      "Episode 7\tLast reward: -11.00\tAverage reward: -14.20\n",
      "Episode 8\tLast reward: -11.00\tAverage reward: -14.04\n",
      "Episode 9\tLast reward: -8.00\tAverage reward: -13.73\n",
      "Episode 10\tLast reward: -12.00\tAverage reward: -13.65\n",
      "Episode 11\tLast reward: -5.00\tAverage reward: -13.22\n",
      "Episode 12\tLast reward: -8.00\tAverage reward: -12.95\n",
      "Episode 13\tLast reward: -15.00\tAverage reward: -13.06\n",
      "Episode 14\tLast reward: -13.00\tAverage reward: -13.05\n",
      "Episode 15\tLast reward: -11.00\tAverage reward: -12.95\n",
      "Episode 16\tLast reward: -12.00\tAverage reward: -12.90\n",
      "Episode 17\tLast reward: -12.00\tAverage reward: -12.86\n",
      "Episode 18\tLast reward: -11.00\tAverage reward: -12.77\n",
      "Episode 19\tLast reward: -1.00\tAverage reward: -12.18\n",
      "Episode 20\tLast reward: -9.00\tAverage reward: -12.02\n",
      "Episode 21\tLast reward: -17.00\tAverage reward: -12.27\n",
      "Episode 22\tLast reward: -15.00\tAverage reward: -12.40\n",
      "Episode 23\tLast reward: -4.00\tAverage reward: -11.98\n",
      "Episode 24\tLast reward: -8.00\tAverage reward: -11.78\n",
      "Episode 25\tLast reward: -21.00\tAverage reward: -12.25\n",
      "Episode 26\tLast reward: -13.00\tAverage reward: -12.28\n",
      "Episode 27\tLast reward: -15.00\tAverage reward: -12.42\n",
      "Episode 28\tLast reward: -13.00\tAverage reward: -12.45\n",
      "Episode 29\tLast reward: -9.00\tAverage reward: -12.28\n",
      "Episode 30\tLast reward: -6.00\tAverage reward: -11.96\n",
      "Episode 31\tLast reward: -11.00\tAverage reward: -11.91\n",
      "Episode 32\tLast reward: -15.00\tAverage reward: -12.07\n",
      "Episode 33\tLast reward: -15.00\tAverage reward: -12.21\n",
      "Episode 34\tLast reward: -11.00\tAverage reward: -12.15\n",
      "Episode 35\tLast reward: -6.00\tAverage reward: -11.85\n",
      "Episode 36\tLast reward: -2.00\tAverage reward: -11.35\n",
      "Episode 37\tLast reward: -15.00\tAverage reward: -11.54\n",
      "Episode 38\tLast reward: -7.00\tAverage reward: -11.31\n",
      "Episode 39\tLast reward: -15.00\tAverage reward: -11.49\n",
      "Episode 40\tLast reward: -14.00\tAverage reward: -11.62\n",
      "Episode 41\tLast reward: -15.00\tAverage reward: -11.79\n",
      "Episode 42\tLast reward: -9.00\tAverage reward: -11.65\n",
      "Episode 43\tLast reward: -11.00\tAverage reward: -11.62\n",
      "Episode 44\tLast reward: -7.00\tAverage reward: -11.39\n",
      "Episode 45\tLast reward: -11.00\tAverage reward: -11.37\n",
      "Episode 46\tLast reward: -7.00\tAverage reward: -11.15\n",
      "Episode 47\tLast reward: -8.00\tAverage reward: -10.99\n",
      "Episode 48\tLast reward: -8.00\tAverage reward: -10.84\n",
      "Episode 49\tLast reward: -11.00\tAverage reward: -10.85\n",
      "Episode 50\tLast reward: -11.00\tAverage reward: -10.86\n",
      "Episode 51\tLast reward: -11.00\tAverage reward: -10.86\n",
      "Episode 52\tLast reward: -9.00\tAverage reward: -10.77\n",
      "Episode 53\tLast reward: -3.00\tAverage reward: -10.38\n",
      "Episode 54\tLast reward: -13.00\tAverage reward: -10.51\n",
      "Episode 55\tLast reward: -2.00\tAverage reward: -10.09\n",
      "Episode 56\tLast reward: -15.00\tAverage reward: -10.33\n",
      "Episode 57\tLast reward: -12.00\tAverage reward: -10.42\n",
      "Episode 58\tLast reward: -11.00\tAverage reward: -10.45\n",
      "Episode 59\tLast reward: -10.00\tAverage reward: -10.42\n",
      "Episode 60\tLast reward: -12.00\tAverage reward: -10.50\n",
      "Episode 61\tLast reward: -12.00\tAverage reward: -10.58\n",
      "Episode 62\tLast reward: -13.00\tAverage reward: -10.70\n",
      "Episode 63\tLast reward: -15.00\tAverage reward: -10.91\n",
      "Episode 64\tLast reward: -12.00\tAverage reward: -10.97\n",
      "Episode 65\tLast reward: -15.00\tAverage reward: -11.17\n",
      "Episode 66\tLast reward: -15.00\tAverage reward: -11.36\n",
      "Episode 67\tLast reward: -14.00\tAverage reward: -11.49\n",
      "Episode 68\tLast reward: -17.00\tAverage reward: -11.77\n",
      "Episode 69\tLast reward: -10.00\tAverage reward: -11.68\n",
      "Episode 70\tLast reward: -12.00\tAverage reward: -11.70\n",
      "Episode 71\tLast reward: -6.00\tAverage reward: -11.41\n",
      "Episode 72\tLast reward: -5.00\tAverage reward: -11.09\n",
      "Episode 73\tLast reward: -17.00\tAverage reward: -11.39\n",
      "Episode 74\tLast reward: -8.00\tAverage reward: -11.22\n",
      "Episode 75\tLast reward: -14.00\tAverage reward: -11.36\n",
      "Episode 76\tLast reward: -7.00\tAverage reward: -11.14\n",
      "Episode 77\tLast reward: -17.00\tAverage reward: -11.43\n",
      "Episode 78\tLast reward: -2.00\tAverage reward: -10.96\n",
      "Episode 79\tLast reward: -8.00\tAverage reward: -10.81\n",
      "Episode 80\tLast reward: -14.00\tAverage reward: -10.97\n",
      "Episode 81\tLast reward: -9.00\tAverage reward: -10.87\n",
      "Episode 82\tLast reward: -8.00\tAverage reward: -10.73\n",
      "Episode 83\tLast reward: -16.00\tAverage reward: -10.99\n",
      "Episode 84\tLast reward: -6.00\tAverage reward: -10.74\n",
      "Episode 85\tLast reward: -11.00\tAverage reward: -10.76\n",
      "Episode 86\tLast reward: -15.00\tAverage reward: -10.97\n",
      "Episode 87\tLast reward: -4.00\tAverage reward: -10.62\n",
      "Episode 88\tLast reward: -15.00\tAverage reward: -10.84\n",
      "Episode 89\tLast reward: -16.00\tAverage reward: -11.10\n",
      "Episode 90\tLast reward: -9.00\tAverage reward: -10.99\n",
      "Episode 91\tLast reward: -15.00\tAverage reward: -11.19\n",
      "Episode 92\tLast reward: -12.00\tAverage reward: -11.23\n",
      "Episode 93\tLast reward: -9.00\tAverage reward: -11.12\n",
      "Episode 94\tLast reward: -13.00\tAverage reward: -11.21\n",
      "Episode 95\tLast reward: -10.00\tAverage reward: -11.15\n",
      "Episode 96\tLast reward: -15.00\tAverage reward: -11.35\n",
      "Episode 97\tLast reward: -4.00\tAverage reward: -10.98\n",
      "Episode 98\tLast reward: -8.00\tAverage reward: -10.83\n",
      "Episode 99\tLast reward: -13.00\tAverage reward: -10.94\n",
      "Episode 100\tLast reward: -7.00\tAverage reward: -10.74\n",
      "Episode 101\tLast reward: -8.00\tAverage reward: -10.60\n",
      "Episode 102\tLast reward: -15.00\tAverage reward: -10.82\n",
      "Episode 103\tLast reward: -7.00\tAverage reward: -10.63\n",
      "Episode 104\tLast reward: -7.00\tAverage reward: -10.45\n",
      "Episode 105\tLast reward: -9.00\tAverage reward: -10.38\n",
      "Episode 106\tLast reward: -16.00\tAverage reward: -10.66\n",
      "Episode 107\tLast reward: -15.00\tAverage reward: -10.88\n",
      "Episode 108\tLast reward: -9.00\tAverage reward: -10.78\n",
      "Episode 109\tLast reward: -10.00\tAverage reward: -10.74\n",
      "Episode 110\tLast reward: -11.00\tAverage reward: -10.76\n",
      "Episode 111\tLast reward: -11.00\tAverage reward: -10.77\n",
      "Episode 112\tLast reward: -10.00\tAverage reward: -10.73\n",
      "Episode 113\tLast reward: -14.00\tAverage reward: -10.89\n",
      "Episode 114\tLast reward: -15.00\tAverage reward: -11.10\n",
      "Episode 115\tLast reward: -7.00\tAverage reward: -10.89\n",
      "Episode 116\tLast reward: -17.00\tAverage reward: -11.20\n",
      "Episode 117\tLast reward: -16.00\tAverage reward: -11.44\n",
      "Episode 118\tLast reward: -13.00\tAverage reward: -11.52\n",
      "Episode 119\tLast reward: -9.00\tAverage reward: -11.39\n",
      "Episode 120\tLast reward: -11.00\tAverage reward: -11.37\n",
      "Episode 121\tLast reward: -13.00\tAverage reward: -11.45\n",
      "Episode 122\tLast reward: -8.00\tAverage reward: -11.28\n",
      "Episode 123\tLast reward: -17.00\tAverage reward: -11.57\n",
      "Episode 124\tLast reward: -10.00\tAverage reward: -11.49\n",
      "Episode 125\tLast reward: -11.00\tAverage reward: -11.46\n",
      "Episode 126\tLast reward: -11.00\tAverage reward: -11.44\n",
      "Episode 127\tLast reward: -15.00\tAverage reward: -11.62\n",
      "Episode 128\tLast reward: -13.00\tAverage reward: -11.69\n",
      "Episode 129\tLast reward: -11.00\tAverage reward: -11.65\n",
      "Episode 130\tLast reward: -12.00\tAverage reward: -11.67\n",
      "Episode 131\tLast reward: -10.00\tAverage reward: -11.59\n",
      "Episode 132\tLast reward: -12.00\tAverage reward: -11.61\n",
      "Episode 133\tLast reward: -9.00\tAverage reward: -11.48\n",
      "Episode 134\tLast reward: -12.00\tAverage reward: -11.50\n",
      "Episode 135\tLast reward: -3.00\tAverage reward: -11.08\n",
      "Episode 136\tLast reward: -3.00\tAverage reward: -10.67\n",
      "Episode 137\tLast reward: -14.00\tAverage reward: -10.84\n",
      "Episode 138\tLast reward: -11.00\tAverage reward: -10.85\n",
      "Episode 139\tLast reward: -4.00\tAverage reward: -10.51\n",
      "Episode 140\tLast reward: -7.00\tAverage reward: -10.33\n",
      "Episode 141\tLast reward: -2.00\tAverage reward: -9.91\n",
      "Episode 142\tLast reward: -12.00\tAverage reward: -10.02\n",
      "Episode 143\tLast reward: -9.00\tAverage reward: -9.97\n",
      "Episode 144\tLast reward: -15.00\tAverage reward: -10.22\n",
      "Episode 145\tLast reward: -19.00\tAverage reward: -10.66\n",
      "Episode 146\tLast reward: -14.00\tAverage reward: -10.83\n",
      "Episode 147\tLast reward: -6.00\tAverage reward: -10.58\n",
      "Episode 148\tLast reward: -13.00\tAverage reward: -10.71\n",
      "Episode 149\tLast reward: -16.00\tAverage reward: -10.97\n",
      "Episode 150\tLast reward: -17.00\tAverage reward: -11.27\n",
      "Episode 151\tLast reward: -11.00\tAverage reward: -11.26\n",
      "Episode 152\tLast reward: -8.00\tAverage reward: -11.09\n",
      "Episode 153\tLast reward: -13.00\tAverage reward: -11.19\n",
      "Episode 154\tLast reward: -8.00\tAverage reward: -11.03\n",
      "Episode 155\tLast reward: -5.00\tAverage reward: -10.73\n",
      "Episode 156\tLast reward: -12.00\tAverage reward: -10.79\n",
      "Episode 157\tLast reward: -14.00\tAverage reward: -10.95\n",
      "Episode 158\tLast reward: -11.00\tAverage reward: -10.96\n",
      "Episode 159\tLast reward: -13.00\tAverage reward: -11.06\n",
      "Episode 160\tLast reward: -6.00\tAverage reward: -10.80\n",
      "Episode 161\tLast reward: -12.00\tAverage reward: -10.86\n",
      "Episode 162\tLast reward: -15.00\tAverage reward: -11.07\n",
      "Episode 163\tLast reward: -13.00\tAverage reward: -11.17\n",
      "Episode 164\tLast reward: -12.00\tAverage reward: -11.21\n",
      "Episode 165\tLast reward: -10.00\tAverage reward: -11.15\n",
      "Episode 166\tLast reward: -11.00\tAverage reward: -11.14\n",
      "Episode 167\tLast reward: -17.00\tAverage reward: -11.43\n",
      "Episode 168\tLast reward: -7.00\tAverage reward: -11.21\n",
      "Episode 169\tLast reward: -12.00\tAverage reward: -11.25\n",
      "Episode 170\tLast reward: -11.00\tAverage reward: -11.24\n",
      "Episode 171\tLast reward: -7.00\tAverage reward: -11.03\n",
      "Episode 172\tLast reward: -7.00\tAverage reward: -10.83\n",
      "Episode 173\tLast reward: -7.00\tAverage reward: -10.63\n",
      "Episode 174\tLast reward: -9.00\tAverage reward: -10.55\n",
      "Episode 175\tLast reward: -10.00\tAverage reward: -10.53\n",
      "Episode 176\tLast reward: -21.00\tAverage reward: -11.05\n",
      "Episode 177\tLast reward: -9.00\tAverage reward: -10.95\n",
      "Episode 178\tLast reward: -13.00\tAverage reward: -11.05\n",
      "Episode 179\tLast reward: -7.00\tAverage reward: -10.85\n",
      "Episode 180\tLast reward: -10.00\tAverage reward: -10.80\n",
      "Episode 181\tLast reward: -6.00\tAverage reward: -10.56\n",
      "Episode 182\tLast reward: -13.00\tAverage reward: -10.69\n",
      "Episode 183\tLast reward: -13.00\tAverage reward: -10.80\n",
      "Episode 184\tLast reward: -15.00\tAverage reward: -11.01\n",
      "Episode 185\tLast reward: -7.00\tAverage reward: -10.81\n",
      "Episode 186\tLast reward: -13.00\tAverage reward: -10.92\n",
      "Episode 187\tLast reward: -14.00\tAverage reward: -11.07\n",
      "Episode 188\tLast reward: -9.00\tAverage reward: -10.97\n",
      "Episode 189\tLast reward: -7.00\tAverage reward: -10.77\n",
      "Episode 190\tLast reward: -17.00\tAverage reward: -11.08\n",
      "Episode 191\tLast reward: -13.00\tAverage reward: -11.18\n",
      "Episode 192\tLast reward: -11.00\tAverage reward: -11.17\n",
      "Episode 193\tLast reward: -14.00\tAverage reward: -11.31\n",
      "Episode 194\tLast reward: -17.00\tAverage reward: -11.60\n",
      "Episode 195\tLast reward: -9.00\tAverage reward: -11.47\n",
      "Episode 196\tLast reward: -15.00\tAverage reward: -11.64\n",
      "Episode 197\tLast reward: -11.00\tAverage reward: -11.61\n",
      "Episode 198\tLast reward: -13.00\tAverage reward: -11.68\n",
      "Episode 199\tLast reward: -9.00\tAverage reward: -11.55\n",
      "Episode 200\tLast reward: -9.00\tAverage reward: -11.42\n",
      "Episode 201\tLast reward: -13.00\tAverage reward: -11.50\n",
      "Episode 202\tLast reward: -13.00\tAverage reward: -11.57\n",
      "Episode 203\tLast reward: -10.00\tAverage reward: -11.49\n",
      "Episode 204\tLast reward: -17.00\tAverage reward: -11.77\n",
      "Episode 205\tLast reward: -7.00\tAverage reward: -11.53\n",
      "Episode 206\tLast reward: -14.00\tAverage reward: -11.65\n",
      "Episode 207\tLast reward: -10.00\tAverage reward: -11.57\n",
      "Episode 208\tLast reward: -11.00\tAverage reward: -11.54\n",
      "Episode 209\tLast reward: -8.00\tAverage reward: -11.37\n",
      "Episode 210\tLast reward: -7.00\tAverage reward: -11.15\n",
      "Episode 211\tLast reward: -9.00\tAverage reward: -11.04\n",
      "Episode 212\tLast reward: -13.00\tAverage reward: -11.14\n",
      "Episode 213\tLast reward: -15.00\tAverage reward: -11.33\n",
      "Episode 214\tLast reward: -1.00\tAverage reward: -10.82\n",
      "Episode 215\tLast reward: -11.00\tAverage reward: -10.82\n",
      "Episode 216\tLast reward: -7.00\tAverage reward: -10.63\n",
      "Episode 217\tLast reward: -8.00\tAverage reward: -10.50\n",
      "Episode 218\tLast reward: -15.00\tAverage reward: -10.73\n",
      "Episode 219\tLast reward: -9.00\tAverage reward: -10.64\n",
      "Episode 220\tLast reward: -14.00\tAverage reward: -10.81\n",
      "Episode 221\tLast reward: -19.00\tAverage reward: -11.22\n",
      "Episode 222\tLast reward: -15.00\tAverage reward: -11.41\n",
      "Episode 223\tLast reward: -10.00\tAverage reward: -11.34\n",
      "Episode 224\tLast reward: -14.00\tAverage reward: -11.47\n",
      "Episode 225\tLast reward: -19.00\tAverage reward: -11.85\n",
      "Episode 226\tLast reward: -11.00\tAverage reward: -11.80\n",
      "Episode 227\tLast reward: -7.00\tAverage reward: -11.56\n",
      "Episode 228\tLast reward: -11.00\tAverage reward: -11.54\n",
      "Episode 229\tLast reward: -8.00\tAverage reward: -11.36\n",
      "Episode 230\tLast reward: -14.00\tAverage reward: -11.49\n",
      "Episode 231\tLast reward: -13.00\tAverage reward: -11.57\n",
      "Episode 232\tLast reward: -14.00\tAverage reward: -11.69\n",
      "Episode 233\tLast reward: -16.00\tAverage reward: -11.90\n",
      "Episode 234\tLast reward: -11.00\tAverage reward: -11.86\n",
      "Episode 235\tLast reward: -9.00\tAverage reward: -11.72\n",
      "Episode 236\tLast reward: -7.00\tAverage reward: -11.48\n",
      "Episode 237\tLast reward: -9.00\tAverage reward: -11.36\n",
      "Episode 238\tLast reward: -10.00\tAverage reward: -11.29\n",
      "Episode 239\tLast reward: -15.00\tAverage reward: -11.47\n",
      "Episode 240\tLast reward: -9.00\tAverage reward: -11.35\n",
      "Episode 241\tLast reward: -11.00\tAverage reward: -11.33\n",
      "Episode 242\tLast reward: -15.00\tAverage reward: -11.52\n",
      "Episode 243\tLast reward: -16.00\tAverage reward: -11.74\n",
      "Episode 244\tLast reward: -14.00\tAverage reward: -11.85\n",
      "Episode 245\tLast reward: -10.00\tAverage reward: -11.76\n",
      "Episode 246\tLast reward: -12.00\tAverage reward: -11.77\n",
      "Episode 247\tLast reward: -11.00\tAverage reward: -11.73\n",
      "Episode 248\tLast reward: -13.00\tAverage reward: -11.80\n",
      "Episode 249\tLast reward: -11.00\tAverage reward: -11.76\n",
      "Episode 250\tLast reward: -14.00\tAverage reward: -11.87\n",
      "Episode 251\tLast reward: -13.00\tAverage reward: -11.93\n",
      "Episode 252\tLast reward: -12.00\tAverage reward: -11.93\n",
      "Episode 253\tLast reward: -16.00\tAverage reward: -12.13\n",
      "Episode 254\tLast reward: -13.00\tAverage reward: -12.18\n",
      "Episode 255\tLast reward: -9.00\tAverage reward: -12.02\n",
      "Episode 256\tLast reward: -19.00\tAverage reward: -12.37\n",
      "Episode 257\tLast reward: -12.00\tAverage reward: -12.35\n",
      "Episode 258\tLast reward: -10.00\tAverage reward: -12.23\n",
      "Episode 259\tLast reward: -13.00\tAverage reward: -12.27\n",
      "Episode 260\tLast reward: -12.00\tAverage reward: -12.26\n",
      "Episode 261\tLast reward: -5.00\tAverage reward: -11.89\n",
      "Episode 262\tLast reward: -6.00\tAverage reward: -11.60\n",
      "Episode 263\tLast reward: -9.00\tAverage reward: -11.47\n",
      "Episode 264\tLast reward: -11.00\tAverage reward: -11.45\n",
      "Episode 265\tLast reward: -15.00\tAverage reward: -11.62\n",
      "Episode 266\tLast reward: -15.00\tAverage reward: -11.79\n",
      "Episode 267\tLast reward: -9.00\tAverage reward: -11.65\n",
      "Episode 268\tLast reward: -7.00\tAverage reward: -11.42\n",
      "Episode 269\tLast reward: -15.00\tAverage reward: -11.60\n",
      "Episode 270\tLast reward: -8.00\tAverage reward: -11.42\n",
      "Episode 271\tLast reward: -12.00\tAverage reward: -11.45\n",
      "Episode 272\tLast reward: -3.00\tAverage reward: -11.03\n",
      "Episode 273\tLast reward: -7.00\tAverage reward: -10.82\n",
      "Episode 274\tLast reward: -15.00\tAverage reward: -11.03\n",
      "Episode 275\tLast reward: -7.00\tAverage reward: -10.83\n",
      "Episode 276\tLast reward: -2.00\tAverage reward: -10.39\n",
      "Episode 277\tLast reward: -13.00\tAverage reward: -10.52\n",
      "Episode 278\tLast reward: -8.00\tAverage reward: -10.39\n",
      "Episode 279\tLast reward: -8.00\tAverage reward: -10.27\n",
      "Episode 280\tLast reward: -8.00\tAverage reward: -10.16\n",
      "Episode 281\tLast reward: -13.00\tAverage reward: -10.30\n",
      "Episode 282\tLast reward: -11.00\tAverage reward: -10.34\n",
      "Episode 283\tLast reward: -11.00\tAverage reward: -10.37\n",
      "Episode 284\tLast reward: -13.00\tAverage reward: -10.50\n",
      "Episode 285\tLast reward: -14.00\tAverage reward: -10.68\n",
      "Episode 286\tLast reward: -11.00\tAverage reward: -10.69\n",
      "Episode 287\tLast reward: -4.00\tAverage reward: -10.36\n",
      "Episode 288\tLast reward: -13.00\tAverage reward: -10.49\n",
      "Episode 289\tLast reward: -13.00\tAverage reward: -10.62\n",
      "Episode 290\tLast reward: 4.00\tAverage reward: -9.89\n",
      "Episode 291\tLast reward: -15.00\tAverage reward: -10.14\n",
      "Episode 292\tLast reward: -7.00\tAverage reward: -9.98\n",
      "Episode 293\tLast reward: -13.00\tAverage reward: -10.13\n",
      "Episode 294\tLast reward: -12.00\tAverage reward: -10.23\n",
      "Episode 295\tLast reward: -4.00\tAverage reward: -9.92\n",
      "Episode 296\tLast reward: -17.00\tAverage reward: -10.27\n",
      "Episode 297\tLast reward: -11.00\tAverage reward: -10.31\n",
      "Episode 298\tLast reward: -9.00\tAverage reward: -10.24\n",
      "Episode 299\tLast reward: -17.00\tAverage reward: -10.58\n",
      "Episode 300\tLast reward: -13.00\tAverage reward: -10.70\n",
      "Episode 301\tLast reward: -17.00\tAverage reward: -11.02\n",
      "Episode 302\tLast reward: -12.00\tAverage reward: -11.06\n",
      "Episode 303\tLast reward: -4.00\tAverage reward: -10.71\n",
      "Episode 304\tLast reward: -17.00\tAverage reward: -11.03\n",
      "Episode 305\tLast reward: -11.00\tAverage reward: -11.02\n",
      "Episode 306\tLast reward: -13.00\tAverage reward: -11.12\n",
      "Episode 307\tLast reward: -13.00\tAverage reward: -11.22\n",
      "Episode 308\tLast reward: -16.00\tAverage reward: -11.46\n",
      "Episode 309\tLast reward: -11.00\tAverage reward: -11.43\n",
      "Episode 310\tLast reward: -13.00\tAverage reward: -11.51\n",
      "Episode 311\tLast reward: -8.00\tAverage reward: -11.34\n",
      "Episode 312\tLast reward: -13.00\tAverage reward: -11.42\n",
      "Episode 313\tLast reward: -18.00\tAverage reward: -11.75\n",
      "Episode 314\tLast reward: -12.00\tAverage reward: -11.76\n",
      "Episode 315\tLast reward: -11.00\tAverage reward: -11.72\n",
      "Episode 316\tLast reward: -14.00\tAverage reward: -11.84\n",
      "Episode 317\tLast reward: -15.00\tAverage reward: -12.00\n",
      "Episode 318\tLast reward: -7.00\tAverage reward: -11.75\n",
      "Episode 319\tLast reward: -17.00\tAverage reward: -12.01\n",
      "Episode 320\tLast reward: -10.00\tAverage reward: -11.91\n",
      "Episode 321\tLast reward: -11.00\tAverage reward: -11.86\n",
      "Episode 322\tLast reward: -12.00\tAverage reward: -11.87\n",
      "Episode 323\tLast reward: -19.00\tAverage reward: -12.23\n",
      "Episode 324\tLast reward: -7.00\tAverage reward: -11.96\n",
      "Episode 325\tLast reward: -15.00\tAverage reward: -12.12\n",
      "Episode 326\tLast reward: -11.00\tAverage reward: -12.06\n",
      "Episode 327\tLast reward: -7.00\tAverage reward: -11.81\n",
      "Episode 328\tLast reward: -19.00\tAverage reward: -12.17\n",
      "Episode 329\tLast reward: -15.00\tAverage reward: -12.31\n",
      "Episode 330\tLast reward: -9.00\tAverage reward: -12.14\n",
      "Episode 331\tLast reward: -10.00\tAverage reward: -12.04\n",
      "Episode 332\tLast reward: -8.00\tAverage reward: -11.83\n",
      "Episode 333\tLast reward: -9.00\tAverage reward: -11.69\n",
      "Episode 334\tLast reward: -17.00\tAverage reward: -11.96\n",
      "Episode 335\tLast reward: -9.00\tAverage reward: -11.81\n",
      "Episode 336\tLast reward: -17.00\tAverage reward: -12.07\n",
      "Episode 337\tLast reward: -17.00\tAverage reward: -12.32\n",
      "Episode 338\tLast reward: -16.00\tAverage reward: -12.50\n",
      "Episode 339\tLast reward: -5.00\tAverage reward: -12.13\n",
      "Episode 340\tLast reward: -16.00\tAverage reward: -12.32\n",
      "Episode 341\tLast reward: 1.00\tAverage reward: -11.65\n",
      "Episode 342\tLast reward: -15.00\tAverage reward: -11.82\n",
      "Episode 343\tLast reward: -15.00\tAverage reward: -11.98\n",
      "Episode 344\tLast reward: -7.00\tAverage reward: -11.73\n",
      "Episode 345\tLast reward: -9.00\tAverage reward: -11.59\n",
      "Episode 346\tLast reward: -14.00\tAverage reward: -11.71\n",
      "Episode 347\tLast reward: -5.00\tAverage reward: -11.38\n",
      "Episode 348\tLast reward: -15.00\tAverage reward: -11.56\n",
      "Episode 349\tLast reward: -10.00\tAverage reward: -11.48\n",
      "Episode 350\tLast reward: -10.00\tAverage reward: -11.41\n",
      "Episode 351\tLast reward: -18.00\tAverage reward: -11.74\n",
      "Episode 352\tLast reward: -9.00\tAverage reward: -11.60\n",
      "Episode 353\tLast reward: -13.00\tAverage reward: -11.67\n",
      "Episode 354\tLast reward: -5.00\tAverage reward: -11.34\n",
      "Episode 355\tLast reward: -5.00\tAverage reward: -11.02\n",
      "Episode 356\tLast reward: -11.00\tAverage reward: -11.02\n",
      "Episode 357\tLast reward: -17.00\tAverage reward: -11.32\n",
      "Episode 358\tLast reward: -11.00\tAverage reward: -11.30\n",
      "Episode 359\tLast reward: -17.00\tAverage reward: -11.59\n",
      "Episode 360\tLast reward: -8.00\tAverage reward: -11.41\n",
      "Episode 361\tLast reward: -11.00\tAverage reward: -11.39\n",
      "Episode 362\tLast reward: -14.00\tAverage reward: -11.52\n",
      "Episode 363\tLast reward: -19.00\tAverage reward: -11.89\n",
      "Episode 364\tLast reward: -4.00\tAverage reward: -11.50\n",
      "Episode 365\tLast reward: -6.00\tAverage reward: -11.22\n",
      "Episode 366\tLast reward: -12.00\tAverage reward: -11.26\n",
      "Episode 367\tLast reward: -3.00\tAverage reward: -10.85\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9753b40e3b8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m## accumulate the action losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0maction_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0maction_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;31m## step the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mp_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in count(1):\n",
    "    action_log_probs = list()\n",
    "    rewards = list()\n",
    "    values = list()\n",
    "    state = env.reset()\n",
    "    ##test = net(torch.FloatTensor(state).unsqueeze(0))\n",
    "    ##print(test)\n",
    "    prev_x = None\n",
    "    #print(state.shape)\n",
    "    ##counter = 0\n",
    "    for t in range(100000):\n",
    "    #while True:\n",
    "        #env.render()\n",
    "        \n",
    "        cur_x  = preprocess(state)\n",
    "        #print(\"cur_x = \", cur_x.shape)\n",
    "        state  = cur_x - prev_x if prev_x is not None else preprocess(prev_x)##np.zeros(input_shape)\n",
    "        #print(\"state = \", state.shape)\n",
    "        prev_x = cur_x\n",
    "        \n",
    "        ## so the tensor given to the model should be of shape [batch_size, 1, height, width].\n",
    "        \n",
    "        \n",
    "        ##state = torch.FloatTensor(state)\n",
    "        \n",
    "        state = state.reshape(1, 1, 80, 80)\n",
    "        \n",
    "        #env.render()\n",
    "        ## take an action sampled from a categorical distribution given the state\n",
    "        action_prob = pnet(torch.FloatTensor(state).to(device))\n",
    "        action = action_prob.sample()\n",
    "        action_log_probs.append(action_prob.log_prob(action))\n",
    "        \n",
    "        if action.item() == 0:\n",
    "            action = UP_ACTION\n",
    "        else:\n",
    "            action = DOWN_ACTION\n",
    "        \n",
    "        #print(entropy)\n",
    "        value = cnet(torch.FloatTensor(state).to(device))\n",
    "        values.append(value[0])\n",
    "        #print(action)\n",
    "        next_state, reward, is_done, _ = env.step(action) # take a random action\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        ## current state is next state now\n",
    "        state = next_state\n",
    "\n",
    "        if is_done:\n",
    "            #print(rewards)\n",
    "            #print(values)\n",
    "            break\n",
    "            \n",
    "       \n",
    "            \n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "                \n",
    "    ## Now we have the discounted reward + log_probs of the actions\n",
    "    returns = discounted_returns(rewards)\n",
    "    #print(returns)\n",
    "    action_losses = list()\n",
    "    critic_losses = list()\n",
    "    ## collect the action losses to a list\n",
    "    for ret, l_prob, v in zip(returns, action_log_probs, values):\n",
    "        advantage = ret - v\n",
    "        #print(advantage)\n",
    "        #print(-l_prob * ret)\n",
    "        action_losses.append(-l_prob * advantage.detach())\n",
    "        critic_losses.append(advantage.pow(2))\n",
    "\n",
    "    p_optimizer.zero_grad()\n",
    "    ## accumulate the action losses\n",
    "    action_loss = torch.cat(action_losses).sum()\n",
    "    action_loss.backward()\n",
    "    ## step the optimizer\n",
    "    p_optimizer.step()\n",
    "\n",
    "    c_optimizer.zero_grad()\n",
    "    critic_loss = torch.cat(critic_losses).mean()\n",
    "    critic_loss.backward()\n",
    "    c_optimizer.step()\n",
    "    '''\n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "    '''\n",
    "      \n",
    "    if running_reward >= MEAN_REWARD_BOUND:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n",
    "    \n",
    "env.close()\n",
    "\n",
    "\n",
    "            \n",
    "##:.2f\n",
    "##'Episode {}\\t Last reward: {}\\t Average reward: {}'.format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for the last part and introducing the next one \n",
    "\n",
    "The results didn't improve that much and we change the learning rate several times as shown in the next traing until we introduce the Entropy in the training as we will see after some trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize an optimizer\n",
    "p_optimizer = torch.optim.Adam(pnet.parameters(), lr=1e-7,eps=1e-2,amsgrad=True)\n",
    "c_optimizer = torch.optim.Adam(cnet.parameters(), lr=1e-7,eps=1e-2,amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tLast reward: -11.00\tAverage reward: -10.86\n",
      "Episode 2\tLast reward: -9.00\tAverage reward: -10.76\n",
      "Episode 3\tLast reward: -12.00\tAverage reward: -10.82\n",
      "Episode 4\tLast reward: -17.00\tAverage reward: -11.13\n",
      "Episode 5\tLast reward: -14.00\tAverage reward: -11.28\n",
      "Episode 6\tLast reward: -5.00\tAverage reward: -10.96\n",
      "Episode 7\tLast reward: -2.00\tAverage reward: -10.52\n",
      "Episode 8\tLast reward: -14.00\tAverage reward: -10.69\n",
      "Episode 9\tLast reward: -7.00\tAverage reward: -10.50\n",
      "Episode 10\tLast reward: -12.00\tAverage reward: -10.58\n",
      "Episode 11\tLast reward: 2.00\tAverage reward: -9.95\n",
      "Episode 12\tLast reward: -15.00\tAverage reward: -10.20\n",
      "Episode 13\tLast reward: -9.00\tAverage reward: -10.14\n",
      "Episode 14\tLast reward: -12.00\tAverage reward: -10.24\n",
      "Episode 15\tLast reward: -9.00\tAverage reward: -10.17\n",
      "Episode 16\tLast reward: -13.00\tAverage reward: -10.32\n",
      "Episode 17\tLast reward: -10.00\tAverage reward: -10.30\n",
      "Episode 18\tLast reward: -5.00\tAverage reward: -10.03\n",
      "Episode 19\tLast reward: -13.00\tAverage reward: -10.18\n",
      "Episode 20\tLast reward: -21.00\tAverage reward: -10.72\n",
      "Episode 21\tLast reward: -11.00\tAverage reward: -10.74\n",
      "Episode 22\tLast reward: -19.00\tAverage reward: -11.15\n",
      "Episode 23\tLast reward: -7.00\tAverage reward: -10.94\n",
      "Episode 24\tLast reward: -4.00\tAverage reward: -10.60\n",
      "Episode 25\tLast reward: -9.00\tAverage reward: -10.52\n",
      "Episode 26\tLast reward: -15.00\tAverage reward: -10.74\n",
      "Episode 27\tLast reward: -5.00\tAverage reward: -10.45\n",
      "Episode 28\tLast reward: -3.00\tAverage reward: -10.08\n",
      "Episode 29\tLast reward: -13.00\tAverage reward: -10.23\n",
      "Episode 30\tLast reward: -5.00\tAverage reward: -9.97\n",
      "Episode 31\tLast reward: -17.00\tAverage reward: -10.32\n",
      "Episode 32\tLast reward: -15.00\tAverage reward: -10.55\n",
      "Episode 33\tLast reward: -7.00\tAverage reward: -10.37\n",
      "Episode 34\tLast reward: -14.00\tAverage reward: -10.55\n",
      "Episode 35\tLast reward: -13.00\tAverage reward: -10.68\n",
      "Episode 36\tLast reward: -13.00\tAverage reward: -10.79\n",
      "Episode 37\tLast reward: -13.00\tAverage reward: -10.90\n",
      "Episode 38\tLast reward: -11.00\tAverage reward: -10.91\n",
      "Episode 39\tLast reward: -13.00\tAverage reward: -11.01\n",
      "Episode 40\tLast reward: -15.00\tAverage reward: -11.21\n",
      "Episode 41\tLast reward: -15.00\tAverage reward: -11.40\n",
      "Episode 42\tLast reward: -17.00\tAverage reward: -11.68\n",
      "Episode 43\tLast reward: -6.00\tAverage reward: -11.40\n",
      "Episode 44\tLast reward: -13.00\tAverage reward: -11.48\n",
      "Episode 45\tLast reward: -9.00\tAverage reward: -11.35\n",
      "Episode 46\tLast reward: -11.00\tAverage reward: -11.34\n",
      "Episode 47\tLast reward: -15.00\tAverage reward: -11.52\n",
      "Episode 48\tLast reward: -11.00\tAverage reward: -11.49\n",
      "Episode 49\tLast reward: -16.00\tAverage reward: -11.72\n",
      "Episode 50\tLast reward: -14.00\tAverage reward: -11.83\n",
      "Episode 51\tLast reward: -17.00\tAverage reward: -12.09\n",
      "Episode 52\tLast reward: -12.00\tAverage reward: -12.09\n",
      "Episode 53\tLast reward: -9.00\tAverage reward: -11.93\n",
      "Episode 54\tLast reward: -7.00\tAverage reward: -11.69\n",
      "Episode 55\tLast reward: -4.00\tAverage reward: -11.30\n",
      "Episode 56\tLast reward: -8.00\tAverage reward: -11.14\n",
      "Episode 57\tLast reward: -13.00\tAverage reward: -11.23\n",
      "Episode 58\tLast reward: -12.00\tAverage reward: -11.27\n",
      "Episode 59\tLast reward: -13.00\tAverage reward: -11.35\n",
      "Episode 60\tLast reward: -15.00\tAverage reward: -11.54\n",
      "Episode 61\tLast reward: -5.00\tAverage reward: -11.21\n",
      "Episode 62\tLast reward: -9.00\tAverage reward: -11.10\n",
      "Episode 63\tLast reward: -11.00\tAverage reward: -11.09\n",
      "Episode 64\tLast reward: -9.00\tAverage reward: -10.99\n",
      "Episode 65\tLast reward: -13.00\tAverage reward: -11.09\n",
      "Episode 66\tLast reward: -12.00\tAverage reward: -11.14\n",
      "Episode 67\tLast reward: -9.00\tAverage reward: -11.03\n",
      "Episode 68\tLast reward: -15.00\tAverage reward: -11.23\n",
      "Episode 69\tLast reward: -13.00\tAverage reward: -11.32\n",
      "Episode 70\tLast reward: -14.00\tAverage reward: -11.45\n",
      "Episode 71\tLast reward: -15.00\tAverage reward: -11.63\n",
      "Episode 72\tLast reward: -13.00\tAverage reward: -11.70\n",
      "Episode 73\tLast reward: -8.00\tAverage reward: -11.51\n",
      "Episode 74\tLast reward: -10.00\tAverage reward: -11.44\n",
      "Episode 75\tLast reward: -14.00\tAverage reward: -11.56\n",
      "Episode 76\tLast reward: -15.00\tAverage reward: -11.74\n",
      "Episode 77\tLast reward: -12.00\tAverage reward: -11.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9753b40e3b8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mc_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mc_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     '''\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in count(1):\n",
    "    action_log_probs = list()\n",
    "    rewards = list()\n",
    "    values = list()\n",
    "    state = env.reset()\n",
    "    ##test = net(torch.FloatTensor(state).unsqueeze(0))\n",
    "    ##print(test)\n",
    "    prev_x = None\n",
    "    #print(state.shape)\n",
    "    ##counter = 0\n",
    "    for t in range(100000):\n",
    "    #while True:\n",
    "        #env.render()\n",
    "        \n",
    "        cur_x  = preprocess(state)\n",
    "        #print(\"cur_x = \", cur_x.shape)\n",
    "        state  = cur_x - prev_x if prev_x is not None else preprocess(prev_x)##np.zeros(input_shape)\n",
    "        #print(\"state = \", state.shape)\n",
    "        prev_x = cur_x\n",
    "        \n",
    "        ## so the tensor given to the model should be of shape [batch_size, 1, height, width].\n",
    "        \n",
    "        \n",
    "        ##state = torch.FloatTensor(state)\n",
    "        \n",
    "        state = state.reshape(1, 1, 80, 80)\n",
    "        \n",
    "        #env.render()\n",
    "        ## take an action sampled from a categorical distribution given the state\n",
    "        action_prob = pnet(torch.FloatTensor(state).to(device))\n",
    "        action = action_prob.sample()\n",
    "        action_log_probs.append(action_prob.log_prob(action))\n",
    "        \n",
    "        if action.item() == 0:\n",
    "            action = UP_ACTION\n",
    "        else:\n",
    "            action = DOWN_ACTION\n",
    "        \n",
    "        #print(entropy)\n",
    "        value = cnet(torch.FloatTensor(state).to(device))\n",
    "        values.append(value[0])\n",
    "        #print(action)\n",
    "        next_state, reward, is_done, _ = env.step(action) # take a random action\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        ## current state is next state now\n",
    "        state = next_state\n",
    "\n",
    "        if is_done:\n",
    "            #print(rewards)\n",
    "            #print(values)\n",
    "            break\n",
    "            \n",
    "       \n",
    "            \n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "                \n",
    "    ## Now we have the discounted reward + log_probs of the actions\n",
    "    returns = discounted_returns(rewards)\n",
    "    #print(returns)\n",
    "    action_losses = list()\n",
    "    critic_losses = list()\n",
    "    ## collect the action losses to a list\n",
    "    for ret, l_prob, v in zip(returns, action_log_probs, values):\n",
    "        advantage = ret - v\n",
    "        #print(advantage)\n",
    "        #print(-l_prob * ret)\n",
    "        action_losses.append(-l_prob * advantage.detach())\n",
    "        critic_losses.append(advantage.pow(2))\n",
    "\n",
    "    p_optimizer.zero_grad()\n",
    "    ## accumulate the action losses\n",
    "    action_loss = torch.cat(action_losses).sum()\n",
    "    action_loss.backward()\n",
    "    ## step the optimizer\n",
    "    p_optimizer.step()\n",
    "\n",
    "    c_optimizer.zero_grad()\n",
    "    critic_loss = torch.cat(critic_losses).mean()\n",
    "    critic_loss.backward()\n",
    "    c_optimizer.step()\n",
    "    '''\n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "    '''\n",
    "      \n",
    "    if running_reward >= MEAN_REWARD_BOUND:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n",
    "    \n",
    "env.close()\n",
    "\n",
    "\n",
    "            \n",
    "##:.2f\n",
    "##'Episode {}\\t Last reward: {}\\t Average reward: {}'.format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize an optimizer\n",
    "p_optimizer = torch.optim.Adam(pnet.parameters(), lr=1e-4,eps=1e-2)\n",
    "c_optimizer = torch.optim.Adam(cnet.parameters(), lr=1e-4,eps=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tLast reward: -10.00\tAverage reward: -11.66\n",
      "Episode 2\tLast reward: -15.00\tAverage reward: -11.83\n",
      "Episode 3\tLast reward: -13.00\tAverage reward: -11.89\n",
      "Episode 4\tLast reward: -8.00\tAverage reward: -11.69\n",
      "Episode 5\tLast reward: -14.00\tAverage reward: -11.81\n",
      "Episode 6\tLast reward: -8.00\tAverage reward: -11.62\n",
      "Episode 7\tLast reward: -15.00\tAverage reward: -11.79\n",
      "Episode 8\tLast reward: -11.00\tAverage reward: -11.75\n",
      "Episode 9\tLast reward: -13.00\tAverage reward: -11.81\n",
      "Episode 10\tLast reward: -13.00\tAverage reward: -11.87\n",
      "Episode 11\tLast reward: -11.00\tAverage reward: -11.83\n",
      "Episode 12\tLast reward: -14.00\tAverage reward: -11.93\n",
      "Episode 13\tLast reward: -10.00\tAverage reward: -11.84\n",
      "Episode 14\tLast reward: -15.00\tAverage reward: -12.00\n",
      "Episode 15\tLast reward: -18.00\tAverage reward: -12.30\n",
      "Episode 16\tLast reward: -20.00\tAverage reward: -12.68\n",
      "Episode 17\tLast reward: -9.00\tAverage reward: -12.50\n",
      "Episode 18\tLast reward: -16.00\tAverage reward: -12.67\n",
      "Episode 19\tLast reward: -15.00\tAverage reward: -12.79\n",
      "Episode 20\tLast reward: -13.00\tAverage reward: -12.80\n",
      "Episode 21\tLast reward: -6.00\tAverage reward: -12.46\n",
      "Episode 22\tLast reward: -19.00\tAverage reward: -12.79\n",
      "Episode 23\tLast reward: -12.00\tAverage reward: -12.75\n",
      "Episode 24\tLast reward: -15.00\tAverage reward: -12.86\n",
      "Episode 25\tLast reward: -9.00\tAverage reward: -12.67\n",
      "Episode 26\tLast reward: -5.00\tAverage reward: -12.28\n",
      "Episode 27\tLast reward: -10.00\tAverage reward: -12.17\n",
      "Episode 28\tLast reward: -13.00\tAverage reward: -12.21\n",
      "Episode 29\tLast reward: -5.00\tAverage reward: -11.85\n",
      "Episode 30\tLast reward: -15.00\tAverage reward: -12.01\n",
      "Episode 31\tLast reward: -13.00\tAverage reward: -12.06\n",
      "Episode 32\tLast reward: -13.00\tAverage reward: -12.10\n",
      "Episode 33\tLast reward: -10.00\tAverage reward: -12.00\n",
      "Episode 34\tLast reward: -6.00\tAverage reward: -11.70\n",
      "Episode 35\tLast reward: -14.00\tAverage reward: -11.81\n",
      "Episode 36\tLast reward: -8.00\tAverage reward: -11.62\n",
      "Episode 37\tLast reward: -12.00\tAverage reward: -11.64\n",
      "Episode 38\tLast reward: -12.00\tAverage reward: -11.66\n",
      "Episode 39\tLast reward: -12.00\tAverage reward: -11.68\n",
      "Episode 40\tLast reward: -11.00\tAverage reward: -11.64\n",
      "Episode 41\tLast reward: -11.00\tAverage reward: -11.61\n",
      "Episode 42\tLast reward: -11.00\tAverage reward: -11.58\n",
      "Episode 43\tLast reward: -13.00\tAverage reward: -11.65\n",
      "Episode 44\tLast reward: -12.00\tAverage reward: -11.67\n",
      "Episode 45\tLast reward: -5.00\tAverage reward: -11.34\n",
      "Episode 46\tLast reward: -5.00\tAverage reward: -11.02\n",
      "Episode 47\tLast reward: -8.00\tAverage reward: -10.87\n",
      "Episode 48\tLast reward: -12.00\tAverage reward: -10.92\n",
      "Episode 49\tLast reward: -15.00\tAverage reward: -11.13\n",
      "Episode 50\tLast reward: -5.00\tAverage reward: -10.82\n",
      "Episode 51\tLast reward: -14.00\tAverage reward: -10.98\n",
      "Episode 52\tLast reward: -5.00\tAverage reward: -10.68\n",
      "Episode 53\tLast reward: -15.00\tAverage reward: -10.90\n",
      "Episode 54\tLast reward: -11.00\tAverage reward: -10.90\n",
      "Episode 55\tLast reward: -11.00\tAverage reward: -10.91\n",
      "Episode 56\tLast reward: -10.00\tAverage reward: -10.86\n",
      "Episode 57\tLast reward: -17.00\tAverage reward: -11.17\n",
      "Episode 58\tLast reward: -2.00\tAverage reward: -10.71\n",
      "Episode 59\tLast reward: -11.00\tAverage reward: -10.73\n",
      "Episode 60\tLast reward: -13.00\tAverage reward: -10.84\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9753b40e3b8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m#print(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# take a random action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in count(1):\n",
    "    action_log_probs = list()\n",
    "    rewards = list()\n",
    "    values = list()\n",
    "    state = env.reset()\n",
    "    ##test = net(torch.FloatTensor(state).unsqueeze(0))\n",
    "    ##print(test)\n",
    "    prev_x = None\n",
    "    #print(state.shape)\n",
    "    ##counter = 0\n",
    "    for t in range(100000):\n",
    "    #while True:\n",
    "        #env.render()\n",
    "        \n",
    "        cur_x  = preprocess(state)\n",
    "        #print(\"cur_x = \", cur_x.shape)\n",
    "        state  = cur_x - prev_x if prev_x is not None else preprocess(prev_x)##np.zeros(input_shape)\n",
    "        #print(\"state = \", state.shape)\n",
    "        prev_x = cur_x\n",
    "        \n",
    "        ## so the tensor given to the model should be of shape [batch_size, 1, height, width].\n",
    "        \n",
    "        \n",
    "        ##state = torch.FloatTensor(state)\n",
    "        \n",
    "        state = state.reshape(1, 1, 80, 80)\n",
    "        \n",
    "        #env.render()\n",
    "        ## take an action sampled from a categorical distribution given the state\n",
    "        action_prob = pnet(torch.FloatTensor(state).to(device))\n",
    "        action = action_prob.sample()\n",
    "        action_log_probs.append(action_prob.log_prob(action))\n",
    "        \n",
    "        if action.item() == 0:\n",
    "            action = UP_ACTION\n",
    "        else:\n",
    "            action = DOWN_ACTION\n",
    "        \n",
    "        #print(entropy)\n",
    "        value = cnet(torch.FloatTensor(state).to(device))\n",
    "        values.append(value[0])\n",
    "        #print(action)\n",
    "        next_state, reward, is_done, _ = env.step(action) # take a random action\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        ## current state is next state now\n",
    "        state = next_state\n",
    "\n",
    "        if is_done:\n",
    "            #print(rewards)\n",
    "            #print(values)\n",
    "            break\n",
    "            \n",
    "       \n",
    "            \n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "                \n",
    "    ## Now we have the discounted reward + log_probs of the actions\n",
    "    returns = discounted_returns(rewards)\n",
    "    #print(returns)\n",
    "    action_losses = list()\n",
    "    critic_losses = list()\n",
    "    ## collect the action losses to a list\n",
    "    for ret, l_prob, v in zip(returns, action_log_probs, values):\n",
    "        advantage = ret - v\n",
    "        #print(advantage)\n",
    "        #print(-l_prob * ret)\n",
    "        action_losses.append(-l_prob * advantage.detach())\n",
    "        critic_losses.append(advantage.pow(2))\n",
    "\n",
    "    p_optimizer.zero_grad()\n",
    "    ## accumulate the action losses\n",
    "    action_loss = torch.cat(action_losses).sum()\n",
    "    action_loss.backward()\n",
    "    ## step the optimizer\n",
    "    p_optimizer.step()\n",
    "\n",
    "    c_optimizer.zero_grad()\n",
    "    critic_loss = torch.cat(critic_losses).mean()\n",
    "    critic_loss.backward()\n",
    "    c_optimizer.step()\n",
    "    '''\n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "    '''\n",
    "      \n",
    "    if running_reward >= MEAN_REWARD_BOUND:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n",
    "    \n",
    "env.close()\n",
    "\n",
    "\n",
    "            \n",
    "##:.2f\n",
    "##'Episode {}\\t Last reward: {}\\t Average reward: {}'.format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Entropy\n",
    "\n",
    "we will add entropy to the model to increase the exploatation of the env. and the other posibilites.\n",
    "\n",
    "Note: See The comments in the added part in the loop please \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the environment\n",
    "## env = gym.make('PongNoFrameskip-v4')\n",
    "env = gym.make('Pong-v0')\n",
    "##input_shape = env.observation_space.shape[0]\n",
    "#action_size = env.action_space.n\n",
    "\n",
    "action_size = 2\n",
    "\n",
    "##print(\"Env reward threshold: {}\".format(env.spec.reward_threshold))\n",
    "reward_list = list()\n",
    "\n",
    "input_shape = [1,80,80]\n",
    "\n",
    "##input_shape = [STACK_SIZE, 84, 84]\n",
    "\n",
    "\n",
    "pnet = torch.load('../input/a2c500amsgrad/500_A2C_pnet.pt').to(device)##,map_location=torch.device('cpu'))\n",
    "pnet.eval()\n",
    "\n",
    "cnet = torch.load('../input/a2c500amsgrad/500_A2C_cnet.pt').to(device)##,map_location=torch.device('cpu'))\n",
    "cnet.eval()\n",
    "\n",
    "\n",
    "## initialize an optimizer\n",
    "p_optimizer = torch.optim.Adam(pnet.parameters(), lr=1e-4,eps=1e-3,amsgrad=True)\n",
    "c_optimizer = torch.optim.Adam(cnet.parameters(), lr=1e-4,eps=1e-3,amsgrad=True)\n",
    "\n",
    "\n",
    "\n",
    "running_reward  = None\n",
    "MEAN_REWARD_BOUND = 20\n",
    "\n",
    "UP_ACTION = 2\n",
    "\n",
    "DOWN_ACTION = 3\n",
    "Max_ep_rewared = -21\n",
    "\n",
    "# Entropy coeff.\n",
    "ENTROPY_BETA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in count(1):\n",
    "    action_log_probs = list()\n",
    "    rewards = list()\n",
    "    values = list()\n",
    "    entropys = list()\n",
    "    state = env.reset()\n",
    "    ##test = net(torch.FloatTensor(state).unsqueeze(0))\n",
    "    ##print(test)\n",
    "    prev_x = None\n",
    "    #print(state.shape)\n",
    "    ##counter = 0\n",
    "    for t in range(100000):\n",
    "    #while True:\n",
    "        #env.render()\n",
    "        \n",
    "        cur_x  = preprocess(state)\n",
    "        #print(\"cur_x = \", cur_x.shape)\n",
    "        state  = cur_x - prev_x if prev_x is not None else preprocess(prev_x)##np.zeros(input_shape)\n",
    "        #print(\"state = \", state.shape)\n",
    "        prev_x = cur_x\n",
    "        \n",
    "        ## so the tensor given to the model should be of shape [batch_size, 1, height, width].\n",
    "        \n",
    "        \n",
    "        ##state = torch.FloatTensor(state)\n",
    "        \n",
    "        state = state.reshape(1, 1, 80, 80)\n",
    "        \n",
    "        #env.render()\n",
    "        ## take an action sampled from a categorical distribution given the state\n",
    "        action_prob = pnet(torch.FloatTensor(state).to(device))\n",
    "        action = action_prob.sample()\n",
    "        action_log_probs.append(action_prob.log_prob(action))\n",
    "        \n",
    "        # Getting the entropy for the action probabilities\n",
    "        entropy = ENTROPY_BETA*action_prob.entropy() ## action_prob.probs*action_prob.logits.sum(dim=1).mean()\n",
    "        entropys.append(entropy)\n",
    "        \n",
    "        if action.item() == 0:\n",
    "            action = UP_ACTION\n",
    "        else:\n",
    "            action = DOWN_ACTION\n",
    "        \n",
    "        #print(entropy)\n",
    "        value = cnet(torch.FloatTensor(state).to(device))\n",
    "        values.append(value[0])\n",
    "        #print(action)\n",
    "        next_state, reward, is_done, _ = env.step(action) # take a random action\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        ## current state is next state now\n",
    "        state = next_state\n",
    "\n",
    "        if is_done:\n",
    "            #print(rewards)\n",
    "            #print(values)\n",
    "            break\n",
    "            \n",
    "       \n",
    "            \n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "                \n",
    "    ## Now we have the discounted reward + log_probs of the actions\n",
    "    returns = discounted_returns(rewards)\n",
    "    #print(returns)\n",
    "    action_losses = list()\n",
    "    critic_losses = list()\n",
    "    ## collect the action losses to a list\n",
    "    for ret, l_prob, v in zip(returns, action_log_probs, values):\n",
    "        advantage = ret - v\n",
    "        #print(advantage)\n",
    "        #print(-l_prob * ret)\n",
    "        action_losses.append(-l_prob * advantage.detach())\n",
    "        critic_losses.append(advantage.pow(2))\n",
    "\n",
    "    \n",
    "    ## When trying to add entropy we had an error at the beginning\n",
    "    ## as the backward of one of the terms is earsed after calculating the step\n",
    "    ## so we need to add (retain_graph=True) to save the gradiens and not delete them after the step\n",
    "    ## then we have another error that the versions of the 2 backwards are different \n",
    "    ## to solve that we just flip the order of calling the backward \n",
    "    ## we call the critic loss 1st which have both values from policy (from entropy) and critic loss\n",
    "    ## Then we call the other one easily because we didn't delete the values of the backward from the first one\n",
    "    ## it was hard error to fix and need a lot of understanding\n",
    "    \n",
    "    \n",
    "    c_optimizer.zero_grad()\n",
    "    critic_loss = torch.cat(critic_losses).mean() - torch.cat(entropys).sum()\n",
    "    critic_loss.backward(retain_graph=True)\n",
    "    c_optimizer.step()\n",
    "    \n",
    "    \n",
    "    p_optimizer.zero_grad()\n",
    "    ## accumulate the action losses\n",
    "    action_loss = torch.cat(action_losses).sum()\n",
    "    action_loss.backward()\n",
    "    ## step the optimizer\n",
    "    p_optimizer.step()\n",
    "\n",
    "    \n",
    "    '''\n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "    '''\n",
    "      \n",
    "    if running_reward >= MEAN_REWARD_BOUND:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n",
    "    \n",
    "env.close()\n",
    "\n",
    "\n",
    "            \n",
    "##:.2f\n",
    "##'Episode {}\\t Last reward: {}\\t Average reward: {}'.format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The outputs are copied as the session of the GPU is ended and I couldn't save and download the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode 1\tLast reward: -13.00\tAverage reward: -13.00\n",
    "Episode 2\tLast reward: -8.00\tAverage reward: -12.75\n",
    "Episode 3\tLast reward: -17.00\tAverage reward: -12.96\n",
    "Episode 4\tLast reward: -13.00\tAverage reward: -12.96\n",
    "Episode 5\tLast reward: -10.00\tAverage reward: -12.82\n",
    "Episode 6\tLast reward: -13.00\tAverage reward: -12.83\n",
    "Episode 7\tLast reward: -11.00\tAverage reward: -12.73\n",
    "Episode 8\tLast reward: -6.00\tAverage reward: -12.40\n",
    "Episode 9\tLast reward: -10.00\tAverage reward: -12.28\n",
    "Episode 10\tLast reward: -10.00\tAverage reward: -12.16\n",
    "Episode 11\tLast reward: -6.00\tAverage reward: -11.86\n",
    "Episode 12\tLast reward: -8.00\tAverage reward: -11.66\n",
    "Episode 13\tLast reward: -11.00\tAverage reward: -11.63\n",
    "Episode 14\tLast reward: -13.00\tAverage reward: -11.70\n",
    "Episode 15\tLast reward: -10.00\tAverage reward: -11.61\n",
    "Episode 16\tLast reward: -4.00\tAverage reward: -11.23\n",
    "Episode 17\tLast reward: -12.00\tAverage reward: -11.27\n",
    "Episode 18\tLast reward: -8.00\tAverage reward: -11.11\n",
    "Episode 19\tLast reward: -15.00\tAverage reward: -11.30\n",
    "Episode 20\tLast reward: -17.00\tAverage reward: -11.59\n",
    "Episode 21\tLast reward: -11.00\tAverage reward: -11.56\n",
    "Episode 22\tLast reward: -14.00\tAverage reward: -11.68\n",
    "Episode 23\tLast reward: -9.00\tAverage reward: -11.55\n",
    "Episode 24\tLast reward: -11.00\tAverage reward: -11.52\n",
    "Episode 25\tLast reward: -7.00\tAverage reward: -11.29\n",
    "Episode 26\tLast reward: -11.00\tAverage reward: -11.28\n",
    "Episode 27\tLast reward: -17.00\tAverage reward: -11.56\n",
    "Episode 28\tLast reward: -10.00\tAverage reward: -11.49\n",
    "Episode 29\tLast reward: -9.00\tAverage reward: -11.36\n",
    "Episode 30\tLast reward: -15.00\tAverage reward: -11.54\n",
    "Episode 31\tLast reward: -10.00\tAverage reward: -11.47\n",
    "Episode 32\tLast reward: -4.00\tAverage reward: -11.09\n",
    "Episode 33\tLast reward: -13.00\tAverage reward: -11.19\n",
    "Episode 34\tLast reward: -13.00\tAverage reward: -11.28\n",
    "Episode 35\tLast reward: -11.00\tAverage reward: -11.26\n",
    "Episode 36\tLast reward: -7.00\tAverage reward: -11.05\n",
    "Episode 37\tLast reward: -7.00\tAverage reward: -10.85\n",
    "Episode 38\tLast reward: -7.00\tAverage reward: -10.66\n",
    "Episode 39\tLast reward: -16.00\tAverage reward: -10.92\n",
    "Episode 40\tLast reward: -9.00\tAverage reward: -10.83\n",
    "Episode 41\tLast reward: -5.00\tAverage reward: -10.54\n",
    "Episode 42\tLast reward: -8.00\tAverage reward: -10.41\n",
    "Episode 43\tLast reward: -15.00\tAverage reward: -10.64\n",
    "Episode 44\tLast reward: -9.00\tAverage reward: -10.56\n",
    "Episode 45\tLast reward: 3.00\tAverage reward: -9.88\n",
    "Episode 46\tLast reward: -16.00\tAverage reward: -10.19\n",
    "Episode 47\tLast reward: -15.00\tAverage reward: -10.43\n",
    "Episode 48\tLast reward: -15.00\tAverage reward: -10.65\n",
    "Episode 49\tLast reward: -15.00\tAverage reward: -10.87\n",
    "Episode 50\tLast reward: -8.00\tAverage reward: -10.73\n",
    "Episode 51\tLast reward: -19.00\tAverage reward: -11.14\n",
    "Episode 52\tLast reward: -5.00\tAverage reward: -10.83\n",
    "Episode 53\tLast reward: -15.00\tAverage reward: -11.04\n",
    "Episode 54\tLast reward: -10.00\tAverage reward: -10.99\n",
    "Episode 55\tLast reward: -13.00\tAverage reward: -11.09\n",
    "Episode 56\tLast reward: -11.00\tAverage reward: -11.09\n",
    "Episode 57\tLast reward: -16.00\tAverage reward: -11.33\n",
    "Episode 58\tLast reward: -8.00\tAverage reward: -11.17\n",
    "Episode 59\tLast reward: -11.00\tAverage reward: -11.16\n",
    "Episode 60\tLast reward: -17.00\tAverage reward: -11.45\n",
    "Episode 61\tLast reward: -14.00\tAverage reward: -11.58\n",
    "Episode 62\tLast reward: -17.00\tAverage reward: -11.85\n",
    "Episode 63\tLast reward: -11.00\tAverage reward: -11.81\n",
    "Episode 64\tLast reward: -8.00\tAverage reward: -11.62\n",
    "Episode 65\tLast reward: -8.00\tAverage reward: -11.43\n",
    "Episode 66\tLast reward: -13.00\tAverage reward: -11.51\n",
    "Episode 67\tLast reward: -15.00\tAverage reward: -11.69\n",
    "Episode 68\tLast reward: -13.00\tAverage reward: -11.75\n",
    "Episode 69\tLast reward: -15.00\tAverage reward: -11.92\n",
    "Episode 70\tLast reward: -16.00\tAverage reward: -12.12\n",
    "Episode 71\tLast reward: -13.00\tAverage reward: -12.16\n",
    "Episode 72\tLast reward: -13.00\tAverage reward: -12.21\n",
    "Episode 73\tLast reward: -13.00\tAverage reward: -12.25\n",
    "Episode 74\tLast reward: -16.00\tAverage reward: -12.43\n",
    "Episode 75\tLast reward: -11.00\tAverage reward: -12.36\n",
    "Episode 76\tLast reward: -8.00\tAverage reward: -12.14\n",
    "Episode 77\tLast reward: -14.00\tAverage reward: -12.24\n",
    "Episode 78\tLast reward: -11.00\tAverage reward: -12.17\n",
    "Episode 79\tLast reward: -15.00\tAverage reward: -12.32\n",
    "Episode 80\tLast reward: -13.00\tAverage reward: -12.35\n",
    "Episode 81\tLast reward: -10.00\tAverage reward: -12.23\n",
    "Episode 82\tLast reward: -10.00\tAverage reward: -12.12\n",
    "Episode 83\tLast reward: -8.00\tAverage reward: -11.91\n",
    "Episode 84\tLast reward: -13.00\tAverage reward: -11.97\n",
    "Episode 85\tLast reward: -13.00\tAverage reward: -12.02\n",
    "Episode 86\tLast reward: -9.00\tAverage reward: -11.87\n",
    "Episode 87\tLast reward: -1.00\tAverage reward: -11.33\n",
    "Episode 88\tLast reward: -13.00\tAverage reward: -11.41\n",
    "Episode 89\tLast reward: -8.00\tAverage reward: -11.24\n",
    "Episode 90\tLast reward: -11.00\tAverage reward: -11.23\n",
    "Episode 91\tLast reward: -11.00\tAverage reward: -11.22\n",
    "Episode 92\tLast reward: -13.00\tAverage reward: -11.31\n",
    "Episode 93\tLast reward: -11.00\tAverage reward: -11.29\n",
    "Episode 94\tLast reward: -19.00\tAverage reward: -11.68\n",
    "Episode 95\tLast reward: -15.00\tAverage reward: -11.84\n",
    "Episode 96\tLast reward: -8.00\tAverage reward: -11.65\n",
    "Episode 97\tLast reward: -8.00\tAverage reward: -11.47\n",
    "Episode 98\tLast reward: -10.00\tAverage reward: -11.39\n",
    "Episode 99\tLast reward: -18.00\tAverage reward: -11.72\n",
    "Episode 100\tLast reward: -17.00\tAverage reward: -11.99\n",
    "Episode 101\tLast reward: -15.00\tAverage reward: -12.14\n",
    "Episode 102\tLast reward: -9.00\tAverage reward: -11.98\n",
    "Episode 103\tLast reward: -2.00\tAverage reward: -11.48\n",
    "Episode 104\tLast reward: -15.00\tAverage reward: -11.66\n",
    "Episode 105\tLast reward: -15.00\tAverage reward: -11.83\n",
    "Episode 106\tLast reward: -12.00\tAverage reward: -11.83\n",
    "Episode 107\tLast reward: -9.00\tAverage reward: -11.69\n",
    "Episode 108\tLast reward: -6.00\tAverage reward: -11.41\n",
    "Episode 109\tLast reward: -16.00\tAverage reward: -11.64\n",
    "Episode 110\tLast reward: -1.00\tAverage reward: -11.11\n",
    "Episode 111\tLast reward: -12.00\tAverage reward: -11.15\n",
    "Episode 112\tLast reward: -15.00\tAverage reward: -11.34\n",
    "Episode 113\tLast reward: -13.00\tAverage reward: -11.43\n",
    "Episode 114\tLast reward: -9.00\tAverage reward: -11.30\n",
    "Episode 115\tLast reward: -11.00\tAverage reward: -11.29\n",
    "Episode 116\tLast reward: -9.00\tAverage reward: -11.17\n",
    "Episode 117\tLast reward: -11.00\tAverage reward: -11.17\n",
    "Episode 118\tLast reward: -12.00\tAverage reward: -11.21\n",
    "Episode 119\tLast reward: -9.00\tAverage reward: -11.10\n",
    "Episode 120\tLast reward: -11.00\tAverage reward: -11.09\n",
    "Episode 121\tLast reward: -8.00\tAverage reward: -10.94\n",
    "Episode 122\tLast reward: -13.00\tAverage reward: -11.04\n",
    "Episode 123\tLast reward: -17.00\tAverage reward: -11.34\n",
    "Episode 124\tLast reward: -14.00\tAverage reward: -11.47\n",
    "Episode 125\tLast reward: -11.00\tAverage reward: -11.45\n",
    "Episode 126\tLast reward: -13.00\tAverage reward: -11.53\n",
    "Episode 127\tLast reward: -15.00\tAverage reward: -11.70\n",
    "Episode 128\tLast reward: -13.00\tAverage reward: -11.76\n",
    "Episode 129\tLast reward: -16.00\tAverage reward: -11.98\n",
    "Episode 130\tLast reward: -7.00\tAverage reward: -11.73\n",
    "Episode 131\tLast reward: -13.00\tAverage reward: -11.79\n",
    "Episode 132\tLast reward: -13.00\tAverage reward: -11.85\n",
    "Episode 133\tLast reward: -6.00\tAverage reward: -11.56\n",
    "Episode 134\tLast reward: -17.00\tAverage reward: -11.83\n",
    "Episode 135\tLast reward: -3.00\tAverage reward: -11.39\n",
    "Episode 136\tLast reward: -9.00\tAverage reward: -11.27\n",
    "Episode 137\tLast reward: -15.00\tAverage reward: -11.46\n",
    "Episode 138\tLast reward: -17.00\tAverage reward: -11.73\n",
    "Episode 139\tLast reward: -14.00\tAverage reward: -11.85\n",
    "Episode 140\tLast reward: -11.00\tAverage reward: -11.80\n",
    "Episode 141\tLast reward: -15.00\tAverage reward: -11.96\n",
    "Episode 142\tLast reward: -8.00\tAverage reward: -11.77\n",
    "Episode 143\tLast reward: -19.00\tAverage reward: -12.13\n",
    "Episode 144\tLast reward: -14.00\tAverage reward: -12.22\n",
    "Episode 145\tLast reward: -15.00\tAverage reward: -12.36\n",
    "Episode 146\tLast reward: -8.00\tAverage reward: -12.14\n",
    "Episode 147\tLast reward: -8.00\tAverage reward: -11.94\n",
    "Episode 148\tLast reward: -2.00\tAverage reward: -11.44\n",
    "Episode 149\tLast reward: -9.00\tAverage reward: -11.32\n",
    "Episode 150\tLast reward: -15.00\tAverage reward: -11.50\n",
    "Episode 151\tLast reward: -3.00\tAverage reward: -11.08\n",
    "Episode 152\tLast reward: -8.00\tAverage reward: -10.92\n",
    "Episode 153\tLast reward: -11.00\tAverage reward: -10.93\n",
    "Episode 154\tLast reward: -16.00\tAverage reward: -11.18\n",
    "Episode 155\tLast reward: -1.00\tAverage reward: -10.67\n",
    "Episode 156\tLast reward: -10.00\tAverage reward: -10.64\n",
    "Episode 157\tLast reward: -2.00\tAverage reward: -10.21\n",
    "Episode 158\tLast reward: -13.00\tAverage reward: -10.34\n",
    "Episode 159\tLast reward: -8.00\tAverage reward: -10.23\n",
    "Episode 160\tLast reward: -5.00\tAverage reward: -9.97\n",
    "Episode 161\tLast reward: -11.00\tAverage reward: -10.02\n",
    "Episode 162\tLast reward: -5.00\tAverage reward: -9.77\n",
    "Episode 163\tLast reward: -12.00\tAverage reward: -9.88\n",
    "Episode 164\tLast reward: -13.00\tAverage reward: -10.03\n",
    "Episode 165\tLast reward: -11.00\tAverage reward: -10.08\n",
    "Episode 166\tLast reward: -9.00\tAverage reward: -10.03\n",
    "Episode 167\tLast reward: -9.00\tAverage reward: -9.98\n",
    "Episode 168\tLast reward: -9.00\tAverage reward: -9.93\n",
    "Episode 169\tLast reward: -10.00\tAverage reward: -9.93\n",
    "Episode 170\tLast reward: -17.00\tAverage reward: -10.29\n",
    "Episode 171\tLast reward: -13.00\tAverage reward: -10.42\n",
    "Episode 172\tLast reward: -7.00\tAverage reward: -10.25\n",
    "Episode 173\tLast reward: -3.00\tAverage reward: -9.89\n",
    "Episode 174\tLast reward: -15.00\tAverage reward: -10.14\n",
    "Episode 175\tLast reward: -17.00\tAverage reward: -10.49\n",
    "Episode 176\tLast reward: -13.00\tAverage reward: -10.61\n",
    "Episode 177\tLast reward: -12.00\tAverage reward: -10.68\n",
    "Episode 178\tLast reward: -3.00\tAverage reward: -10.30\n",
    "Episode 179\tLast reward: -6.00\tAverage reward: -10.08\n",
    "Episode 180\tLast reward: -10.00\tAverage reward: -10.08\n",
    "Episode 181\tLast reward: -14.00\tAverage reward: -10.27\n",
    "Episode 182\tLast reward: -6.00\tAverage reward: -10.06\n",
    "Episode 183\tLast reward: -11.00\tAverage reward: -10.11\n",
    "Episode 184\tLast reward: -5.00\tAverage reward: -9.85\n",
    "Episode 185\tLast reward: -19.00\tAverage reward: -10.31\n",
    "Episode 186\tLast reward: -13.00\tAverage reward: -10.44\n",
    "Episode 187\tLast reward: -12.00\tAverage reward: -10.52\n",
    "Episode 188\tLast reward: -14.00\tAverage reward: -10.70\n",
    "Episode 189\tLast reward: -14.00\tAverage reward: -10.86\n",
    "Episode 190\tLast reward: -16.00\tAverage reward: -11.12\n",
    "Episode 191\tLast reward: -11.00\tAverage reward: -11.11\n",
    "Episode 192\tLast reward: -11.00\tAverage reward: -11.11\n",
    "Episode 193\tLast reward: -14.00\tAverage reward: -11.25\n",
    "Episode 194\tLast reward: -11.00\tAverage reward: -11.24\n",
    "Episode 195\tLast reward: -7.00\tAverage reward: -11.03\n",
    "Episode 196\tLast reward: -15.00\tAverage reward: -11.23\n",
    "Episode 197\tLast reward: -12.00\tAverage reward: -11.26\n",
    "Episode 198\tLast reward: -13.00\tAverage reward: -11.35\n",
    "Episode 199\tLast reward: -7.00\tAverage reward: -11.13\n",
    "Episode 200\tLast reward: -13.00\tAverage reward: -11.23\n",
    "Episode 201\tLast reward: -8.00\tAverage reward: -11.07\n",
    "Episode 202\tLast reward: -11.00\tAverage reward: -11.06\n",
    "Episode 203\tLast reward: -15.00\tAverage reward: -11.26\n",
    "Episode 204\tLast reward: -9.00\tAverage reward: -11.15\n",
    "Episode 205\tLast reward: -13.00\tAverage reward: -11.24\n",
    "Episode 206\tLast reward: -15.00\tAverage reward: -11.43\n",
    "Episode 207\tLast reward: -2.00\tAverage reward: -10.96\n",
    "Episode 208\tLast reward: -13.00\tAverage reward: -11.06\n",
    "Episode 209\tLast reward: -10.00\tAverage reward: -11.00\n",
    "Episode 210\tLast reward: -9.00\tAverage reward: -10.90\n",
    "Episode 211\tLast reward: -17.00\tAverage reward: -11.21\n",
    "Episode 212\tLast reward: -6.00\tAverage reward: -10.95\n",
    "Episode 213\tLast reward: -11.00\tAverage reward: -10.95\n",
    "Episode 214\tLast reward: -17.00\tAverage reward: -11.25\n",
    "Episode 215\tLast reward: -11.00\tAverage reward: -11.24\n",
    "Episode 216\tLast reward: -11.00\tAverage reward: -11.23\n",
    "Episode 217\tLast reward: -13.00\tAverage reward: -11.32\n",
    "Episode 218\tLast reward: -19.00\tAverage reward: -11.70\n",
    "Episode 219\tLast reward: -12.00\tAverage reward: -11.72\n",
    "Episode 220\tLast reward: -10.00\tAverage reward: -11.63\n",
    "Episode 221\tLast reward: -8.00\tAverage reward: -11.45\n",
    "Episode 222\tLast reward: -11.00\tAverage reward: -11.43\n",
    "Episode 223\tLast reward: -17.00\tAverage reward: -11.71\n",
    "Episode 224\tLast reward: -3.00\tAverage reward: -11.27\n",
    "Episode 225\tLast reward: -15.00\tAverage reward: -11.46\n",
    "Episode 226\tLast reward: 7.00\tAverage reward: -10.53\n",
    "Episode 227\tLast reward: -11.00\tAverage reward: -10.56\n",
    "Episode 228\tLast reward: -17.00\tAverage reward: -10.88\n",
    "Episode 229\tLast reward: -11.00\tAverage reward: -10.89\n",
    "Episode 230\tLast reward: -17.00\tAverage reward: -11.19\n",
    "Episode 231\tLast reward: 5.00\tAverage reward: -10.38\n",
    "Episode 232\tLast reward: -11.00\tAverage reward: -10.41\n",
    "Episode 233\tLast reward: -14.00\tAverage reward: -10.59\n",
    "Episode 234\tLast reward: -9.00\tAverage reward: -10.51\n",
    "Episode 235\tLast reward: -8.00\tAverage reward: -10.39\n",
    "Episode 236\tLast reward: -8.00\tAverage reward: -10.27\n",
    "Episode 237\tLast reward: -11.00\tAverage reward: -10.30\n",
    "Episode 238\tLast reward: -5.00\tAverage reward: -10.04\n",
    "Episode 239\tLast reward: -8.00\tAverage reward: -9.94\n",
    "Episode 240\tLast reward: -17.00\tAverage reward: -10.29\n",
    "Episode 241\tLast reward: -13.00\tAverage reward: -10.43\n",
    "Episode 242\tLast reward: -15.00\tAverage reward: -10.65\n",
    "Episode 243\tLast reward: -21.00\tAverage reward: -11.17\n",
    "Episode 244\tLast reward: -7.00\tAverage reward: -10.96\n",
    "Episode 245\tLast reward: -12.00\tAverage reward: -11.01\n",
    "Episode 246\tLast reward: -10.00\tAverage reward: -10.96\n",
    "Episode 247\tLast reward: -3.00\tAverage reward: -10.57\n",
    "Episode 248\tLast reward: -16.00\tAverage reward: -10.84\n",
    "Episode 249\tLast reward: -8.00\tAverage reward: -10.70\n",
    "Episode 250\tLast reward: -15.00\tAverage reward: -10.91\n",
    "Episode 251\tLast reward: -5.00\tAverage reward: -10.62\n",
    "Episode 252\tLast reward: -12.00\tAverage reward: -10.68\n",
    "Episode 253\tLast reward: -10.00\tAverage reward: -10.65\n",
    "Episode 254\tLast reward: -1.00\tAverage reward: -10.17\n",
    "Episode 255\tLast reward: -9.00\tAverage reward: -10.11\n",
    "Episode 256\tLast reward: -11.00\tAverage reward: -10.15\n",
    "Episode 257\tLast reward: -12.00\tAverage reward: -10.25\n",
    "Episode 258\tLast reward: -9.00\tAverage reward: -10.18\n",
    "Episode 259\tLast reward: 4.00\tAverage reward: -9.47\n",
    "Episode 260\tLast reward: -1.00\tAverage reward: -9.05\n",
    "Episode 261\tLast reward: -10.00\tAverage reward: -9.10\n",
    "Episode 262\tLast reward: -9.00\tAverage reward: -9.09\n",
    "Episode 263\tLast reward: -7.00\tAverage reward: -8.99\n",
    "Episode 264\tLast reward: -17.00\tAverage reward: -9.39\n",
    "Episode 265\tLast reward: -6.00\tAverage reward: -9.22\n",
    "Episode 266\tLast reward: -2.00\tAverage reward: -8.86\n",
    "Episode 267\tLast reward: 0.00\tAverage reward: -8.42\n",
    "Episode 268\tLast reward: -9.00\tAverage reward: -8.45\n",
    "Episode 269\tLast reward: -1.00\tAverage reward: -8.07\n",
    "Episode 270\tLast reward: -5.00\tAverage reward: -7.92\n",
    "Episode 271\tLast reward: -12.00\tAverage reward: -8.12\n",
    "Episode 272\tLast reward: -8.00\tAverage reward: -8.12\n",
    "Episode 273\tLast reward: -13.00\tAverage reward: -8.36\n",
    "Episode 274\tLast reward: -9.00\tAverage reward: -8.39\n",
    "Episode 275\tLast reward: -11.00\tAverage reward: -8.52\n",
    "Episode 276\tLast reward: -11.00\tAverage reward: -8.65\n",
    "Episode 277\tLast reward: -7.00\tAverage reward: -8.57\n",
    "Episode 278\tLast reward: -12.00\tAverage reward: -8.74\n",
    "Episode 279\tLast reward: -11.00\tAverage reward: -8.85\n",
    "Episode 280\tLast reward: -10.00\tAverage reward: -8.91\n",
    "Episode 281\tLast reward: -10.00\tAverage reward: -8.96\n",
    "Episode 282\tLast reward: -13.00\tAverage reward: -9.16\n",
    "Episode 283\tLast reward: -14.00\tAverage reward: -9.41\n",
    "Episode 284\tLast reward: -18.00\tAverage reward: -9.84\n",
    "Episode 285\tLast reward: -8.00\tAverage reward: -9.74\n",
    "Episode 286\tLast reward: -7.00\tAverage reward: -9.61\n",
    "Episode 287\tLast reward: -9.00\tAverage reward: -9.58\n",
    "Episode 288\tLast reward: -19.00\tAverage reward: -10.05\n",
    "Episode 289\tLast reward: -5.00\tAverage reward: -9.80\n",
    "Episode 290\tLast reward: -9.00\tAverage reward: -9.76\n",
    "Episode 291\tLast reward: -8.00\tAverage reward: -9.67\n",
    "Episode 292\tLast reward: -13.00\tAverage reward: -9.83\n",
    "Episode 293\tLast reward: -17.00\tAverage reward: -10.19\n",
    "Episode 294\tLast reward: -19.00\tAverage reward: -10.63\n",
    "Episode 295\tLast reward: -4.00\tAverage reward: -10.30\n",
    "Episode 296\tLast reward: -10.00\tAverage reward: -10.29\n",
    "Episode 297\tLast reward: -17.00\tAverage reward: -10.62\n",
    "Episode 298\tLast reward: -9.00\tAverage reward: -10.54\n",
    "Episode 299\tLast reward: -16.00\tAverage reward: -10.81\n",
    "Episode 300\tLast reward: -12.00\tAverage reward: -10.87\n",
    "Episode 301\tLast reward: -5.00\tAverage reward: -10.58\n",
    "Episode 302\tLast reward: -1.00\tAverage reward: -10.10\n",
    "Episode 303\tLast reward: -9.00\tAverage reward: -10.05\n",
    "Episode 304\tLast reward: -12.00\tAverage reward: -10.14\n",
    "Episode 305\tLast reward: -12.00\tAverage reward: -10.24\n",
    "Episode 306\tLast reward: -11.00\tAverage reward: -10.27\n",
    "Episode 307\tLast reward: -13.00\tAverage reward: -10.41\n",
    "Episode 308\tLast reward: -14.00\tAverage reward: -10.59\n",
    "Episode 309\tLast reward: -13.00\tAverage reward: -10.71\n",
    "Episode 310\tLast reward: -17.00\tAverage reward: -11.02\n",
    "Episode 311\tLast reward: -13.00\tAverage reward: -11.12\n",
    "Episode 312\tLast reward: -11.00\tAverage reward: -11.12\n",
    "Episode 313\tLast reward: -11.00\tAverage reward: -11.11\n",
    "Episode 314\tLast reward: -15.00\tAverage reward: -11.31\n",
    "Episode 315\tLast reward: -15.00\tAverage reward: -11.49\n",
    "Episode 316\tLast reward: -13.00\tAverage reward: -11.57\n",
    "Episode 317\tLast reward: -17.00\tAverage reward: -11.84\n",
    "Episode 318\tLast reward: -12.00\tAverage reward: -11.85\n",
    "Episode 319\tLast reward: -15.00\tAverage reward: -12.00\n",
    "Episode 320\tLast reward: -13.00\tAverage reward: -12.05\n",
    "Episode 321\tLast reward: -14.00\tAverage reward: -12.15\n",
    "Episode 322\tLast reward: -11.00\tAverage reward: -12.09\n",
    "Episode 323\tLast reward: -7.00\tAverage reward: -11.84\n",
    "Episode 324\tLast reward: -15.00\tAverage reward: -12.00\n",
    "Episode 325\tLast reward: -7.00\tAverage reward: -11.75\n",
    "Episode 326\tLast reward: -9.00\tAverage reward: -11.61\n",
    "Episode 327\tLast reward: -9.00\tAverage reward: -11.48\n",
    "Episode 328\tLast reward: -9.00\tAverage reward: -11.36\n",
    "Episode 329\tLast reward: -11.00\tAverage reward: -11.34\n",
    "Episode 330\tLast reward: -11.00\tAverage reward: -11.32\n",
    "Episode 331\tLast reward: -10.00\tAverage reward: -11.25\n",
    "Episode 332\tLast reward: -7.00\tAverage reward: -11.04\n",
    "Episode 333\tLast reward: -9.00\tAverage reward: -10.94\n",
    "Episode 334\tLast reward: -11.00\tAverage reward: -10.94\n",
    "Episode 335\tLast reward: -13.00\tAverage reward: -11.05\n",
    "Episode 336\tLast reward: -12.00\tAverage reward: -11.09\n",
    "Episode 337\tLast reward: -4.00\tAverage reward: -10.74\n",
    "Episode 338\tLast reward: -16.00\tAverage reward: -11.00\n",
    "Episode 339\tLast reward: -8.00\tAverage reward: -10.85\n",
    "Episode 340\tLast reward: -3.00\tAverage reward: -10.46\n",
    "Episode 341\tLast reward: -8.00\tAverage reward: -10.34\n",
    "Episode 342\tLast reward: -9.00\tAverage reward: -10.27\n",
    "Episode 343\tLast reward: -9.00\tAverage reward: -10.21\n",
    "Episode 344\tLast reward: -7.00\tAverage reward: -10.05\n",
    "Episode 345\tLast reward: -3.00\tAverage reward: -9.69\n",
    "Episode 346\tLast reward: -14.00\tAverage reward: -9.91\n",
    "Episode 347\tLast reward: -13.00\tAverage reward: -10.06\n",
    "Episode 348\tLast reward: -9.00\tAverage reward: -10.01\n",
    "Episode 349\tLast reward: -3.00\tAverage reward: -9.66\n",
    "Episode 350\tLast reward: 7.00\tAverage reward: -8.83\n",
    "Episode 351\tLast reward: -15.00\tAverage reward: -9.14\n",
    "Episode 352\tLast reward: -5.00\tAverage reward: -8.93\n",
    "Episode 353\tLast reward: -11.00\tAverage reward: -9.03\n",
    "Episode 354\tLast reward: -6.00\tAverage reward: -8.88\n",
    "Episode 355\tLast reward: -1.00\tAverage reward: -8.49\n",
    "Episode 356\tLast reward: -11.00\tAverage reward: -8.61\n",
    "Episode 357\tLast reward: -9.00\tAverage reward: -8.63\n",
    "Episode 358\tLast reward: -14.00\tAverage reward: -8.90\n",
    "Episode 359\tLast reward: -12.00\tAverage reward: -9.05\n",
    "Episode 360\tLast reward: -4.00\tAverage reward: -8.80\n",
    "Episode 361\tLast reward: -11.00\tAverage reward: -8.91\n",
    "Episode 362\tLast reward: -12.00\tAverage reward: -9.07\n",
    "Episode 363\tLast reward: -10.00\tAverage reward: -9.11\n",
    "Episode 364\tLast reward: -5.00\tAverage reward: -8.91\n",
    "Episode 365\tLast reward: -7.00\tAverage reward: -8.81\n",
    "Episode 366\tLast reward: -9.00\tAverage reward: -8.82\n",
    "Episode 367\tLast reward: -13.00\tAverage reward: -9.03\n",
    "Episode 368\tLast reward: -6.00\tAverage reward: -8.88\n",
    "Episode 369\tLast reward: -1.00\tAverage reward: -8.48\n",
    "Episode 370\tLast reward: -9.00\tAverage reward: -8.51\n",
    "Episode 371\tLast reward: -3.00\tAverage reward: -8.24\n",
    "Episode 372\tLast reward: -7.00\tAverage reward: -8.17\n",
    "Episode 373\tLast reward: -15.00\tAverage reward: -8.51\n",
    "Episode 374\tLast reward: -12.00\tAverage reward: -8.69\n",
    "Episode 375\tLast reward: -6.00\tAverage reward: -8.55\n",
    "Episode 376\tLast reward: -10.00\tAverage reward: -8.63\n",
    "Episode 377\tLast reward: -3.00\tAverage reward: -8.35\n",
    "Episode 378\tLast reward: -7.00\tAverage reward: -8.28\n",
    "Episode 379\tLast reward: -10.00\tAverage reward: -8.36\n",
    "Episode 380\tLast reward: -3.00\tAverage reward: -8.10\n",
    "Episode 381\tLast reward: -7.00\tAverage reward: -8.04\n",
    "Episode 382\tLast reward: -7.00\tAverage reward: -7.99\n",
    "Episode 383\tLast reward: -9.00\tAverage reward: -8.04\n",
    "Episode 384\tLast reward: -10.00\tAverage reward: -8.14\n",
    "Episode 385\tLast reward: -10.00\tAverage reward: -8.23\n",
    "Episode 386\tLast reward: -4.00\tAverage reward: -8.02\n",
    "Episode 387\tLast reward: -11.00\tAverage reward: -8.17\n",
    "Episode 388\tLast reward: -17.00\tAverage reward: -8.61\n",
    "Episode 389\tLast reward: -10.00\tAverage reward: -8.68\n",
    "Episode 390\tLast reward: -11.00\tAverage reward: -8.80\n",
    "Episode 391\tLast reward: -11.00\tAverage reward: -8.91\n",
    "Episode 392\tLast reward: -13.00\tAverage reward: -9.11\n",
    "Episode 393\tLast reward: -17.00\tAverage reward: -9.50\n",
    "Episode 394\tLast reward: -14.00\tAverage reward: -9.73\n",
    "Episode 395\tLast reward: -11.00\tAverage reward: -9.79\n",
    "Episode 396\tLast reward: -13.00\tAverage reward: -9.95\n",
    "Episode 397\tLast reward: -11.00\tAverage reward: -10.01\n",
    "Episode 398\tLast reward: -9.00\tAverage reward: -9.96\n",
    "Episode 399\tLast reward: -3.00\tAverage reward: -9.61\n",
    "Episode 400\tLast reward: -8.00\tAverage reward: -9.53\n",
    "Episode 401\tLast reward: -9.00\tAverage reward: -9.50\n",
    "Episode 402\tLast reward: -19.00\tAverage reward: -9.98\n",
    "Episode 403\tLast reward: -7.00\tAverage reward: -9.83\n",
    "Episode 404\tLast reward: -10.00\tAverage reward: -9.84\n",
    "Episode 405\tLast reward: -15.00\tAverage reward: -10.09\n",
    "Episode 406\tLast reward: -17.00\tAverage reward: -10.44\n",
    "Episode 407\tLast reward: -8.00\tAverage reward: -10.32\n",
    "Episode 408\tLast reward: -7.00\tAverage reward: -10.15\n",
    "Episode 409\tLast reward: -1.00\tAverage reward: -9.69\n",
    "Episode 410\tLast reward: -13.00\tAverage reward: -9.86\n",
    "Episode 411\tLast reward: -12.00\tAverage reward: -9.97\n",
    "Episode 412\tLast reward: -9.00\tAverage reward: -9.92\n",
    "Episode 413\tLast reward: -7.00\tAverage reward: -9.77\n",
    "Episode 414\tLast reward: -13.00\tAverage reward: -9.93\n",
    "Episode 415\tLast reward: -14.00\tAverage reward: -10.14\n",
    "Episode 416\tLast reward: -9.00\tAverage reward: -10.08\n",
    "Episode 417\tLast reward: -15.00\tAverage reward: -10.33\n",
    "Episode 418\tLast reward: -16.00\tAverage reward: -10.61\n",
    "Episode 419\tLast reward: -10.00\tAverage reward: -10.58\n",
    "Episode 420\tLast reward: -11.00\tAverage reward: -10.60\n",
    "Episode 421\tLast reward: -9.00\tAverage reward: -10.52\n",
    "Episode 422\tLast reward: -9.00\tAverage reward: -10.44\n",
    "Episode 423\tLast reward: -15.00\tAverage reward: -10.67\n",
    "Episode 424\tLast reward: -15.00\tAverage reward: -10.89\n",
    "Episode 425\tLast reward: -11.00\tAverage reward: -10.89\n",
    "Episode 426\tLast reward: -3.00\tAverage reward: -10.50\n",
    "Episode 427\tLast reward: -5.00\tAverage reward: -10.22\n",
    "Episode 428\tLast reward: -15.00\tAverage reward: -10.46\n",
    "Episode 429\tLast reward: -4.00\tAverage reward: -10.14\n",
    "Episode 430\tLast reward: -4.00\tAverage reward: -9.83\n",
    "Episode 431\tLast reward: -11.00\tAverage reward: -9.89\n",
    "Episode 432\tLast reward: -11.00\tAverage reward: -9.95\n",
    "Episode 433\tLast reward: -16.00\tAverage reward: -10.25\n",
    "Episode 434\tLast reward: -11.00\tAverage reward: -10.29\n",
    "Episode 435\tLast reward: -13.00\tAverage reward: -10.42\n",
    "Episode 436\tLast reward: -15.00\tAverage reward: -10.65\n",
    "Episode 437\tLast reward: -13.00\tAverage reward: -10.77\n",
    "Episode 438\tLast reward: -10.00\tAverage reward: -10.73\n",
    "Episode 439\tLast reward: -13.00\tAverage reward: -10.84\n",
    "Episode 440\tLast reward: -11.00\tAverage reward: -10.85\n",
    "Episode 441\tLast reward: -9.00\tAverage reward: -10.76\n",
    "Episode 442\tLast reward: 2.00\tAverage reward: -10.12\n",
    "Episode 443\tLast reward: -9.00\tAverage reward: -10.07\n",
    "Episode 444\tLast reward: 1.00\tAverage reward: -9.51\n",
    "Episode 445\tLast reward: -12.00\tAverage reward: -9.64\n",
    "Episode 446\tLast reward: -6.00\tAverage reward: -9.45\n",
    "Episode 447\tLast reward: -9.00\tAverage reward: -9.43\n",
    "Episode 448\tLast reward: -14.00\tAverage reward: -9.66\n",
    "Episode 449\tLast reward: -13.00\tAverage reward: -9.83\n",
    "Episode 450\tLast reward: -11.00\tAverage reward: -9.89\n",
    "Episode 451\tLast reward: -15.00\tAverage reward: -10.14\n",
    "Episode 452\tLast reward: -11.00\tAverage reward: -10.18\n",
    "Episode 453\tLast reward: -3.00\tAverage reward: -9.83\n",
    "Episode 454\tLast reward: -11.00\tAverage reward: -9.88\n",
    "Episode 455\tLast reward: -10.00\tAverage reward: -9.89\n",
    "Episode 456\tLast reward: -9.00\tAverage reward: -9.85\n",
    "Episode 457\tLast reward: -11.00\tAverage reward: -9.90\n",
    "Episode 458\tLast reward: -15.00\tAverage reward: -10.16\n",
    "Episode 459\tLast reward: -4.00\tAverage reward: -9.85\n",
    "Episode 460\tLast reward: -12.00\tAverage reward: -9.96\n",
    "Episode 461\tLast reward: -7.00\tAverage reward: -9.81\n",
    "Episode 462\tLast reward: -12.00\tAverage reward: -9.92\n",
    "Episode 463\tLast reward: -2.00\tAverage reward: -9.52\n",
    "Episode 464\tLast reward: -16.00\tAverage reward: -9.85\n",
    "Episode 465\tLast reward: -17.00\tAverage reward: -10.20\n",
    "Episode 466\tLast reward: -9.00\tAverage reward: -10.14\n",
    "Episode 467\tLast reward: -15.00\tAverage reward: -10.39\n",
    "Episode 468\tLast reward: -12.00\tAverage reward: -10.47\n",
    "Episode 469\tLast reward: -13.00\tAverage reward: -10.59\n",
    "Episode 470\tLast reward: -5.00\tAverage reward: -10.31\n",
    "Episode 471\tLast reward: -13.00\tAverage reward: -10.45\n",
    "Episode 472\tLast reward: -9.00\tAverage reward: -10.38\n",
    "Episode 473\tLast reward: -8.00\tAverage reward: -10.26\n",
    "Episode 474\tLast reward: -14.00\tAverage reward: -10.44\n",
    "Episode 475\tLast reward: -12.00\tAverage reward: -10.52\n",
    "Episode 476\tLast reward: -10.00\tAverage reward: -10.50\n",
    "Episode 477\tLast reward: -10.00\tAverage reward: -10.47\n",
    "Episode 478\tLast reward: -11.00\tAverage reward: -10.50\n",
    "Episode 479\tLast reward: -7.00\tAverage reward: -10.32\n",
    "Episode 480\tLast reward: -10.00\tAverage reward: -10.31\n",
    "Episode 481\tLast reward: -13.00\tAverage reward: -10.44\n",
    "Episode 482\tLast reward: -5.00\tAverage reward: -10.17\n",
    "Episode 483\tLast reward: -5.00\tAverage reward: -9.91\n",
    "Episode 484\tLast reward: -3.00\tAverage reward: -9.57\n",
    "Episode 485\tLast reward: -10.00\tAverage reward: -9.59\n",
    "Episode 486\tLast reward: -9.00\tAverage reward: -9.56\n",
    "Episode 487\tLast reward: -7.00\tAverage reward: -9.43\n",
    "Episode 488\tLast reward: -3.00\tAverage reward: -9.11\n",
    "Episode 489\tLast reward: -9.00\tAverage reward: -9.10\n",
    "Episode 490\tLast reward: -12.00\tAverage reward: -9.25\n",
    "Episode 491\tLast reward: -5.00\tAverage reward: -9.04\n",
    "Episode 492\tLast reward: -8.00\tAverage reward: -8.98\n",
    "Episode 493\tLast reward: -9.00\tAverage reward: -8.98\n",
    "Episode 494\tLast reward: -15.00\tAverage reward: -9.29\n",
    "Episode 495\tLast reward: -13.00\tAverage reward: -9.47\n",
    "Episode 496\tLast reward: -9.00\tAverage reward: -9.45\n",
    "Episode 497\tLast reward: -4.00\tAverage reward: -9.18\n",
    "Episode 498\tLast reward: -12.00\tAverage reward: -9.32\n",
    "Episode 499\tLast reward: -6.00\tAverage reward: -9.15\n",
    "Episode 500\tLast reward: -9.00\tAverage reward: -9.14\n",
    "Episode 501\tLast reward: -6.00\tAverage reward: -8.99\n",
    "Episode 502\tLast reward: -7.00\tAverage reward: -8.89\n",
    "Episode 503\tLast reward: -13.00\tAverage reward: -9.09\n",
    "Episode 504\tLast reward: -13.00\tAverage reward: -9.29\n",
    "Episode 505\tLast reward: -9.00\tAverage reward: -9.27\n",
    "Episode 506\tLast reward: -7.00\tAverage reward: -9.16\n",
    "Episode 507\tLast reward: -14.00\tAverage reward: -9.40\n",
    "Episode 508\tLast reward: -16.00\tAverage reward: -9.73\n",
    "Episode 509\tLast reward: -7.00\tAverage reward: -9.59\n",
    "Episode 510\tLast reward: -9.00\tAverage reward: -9.57\n",
    "Episode 511\tLast reward: -10.00\tAverage reward: -9.59\n",
    "Episode 512\tLast reward: -11.00\tAverage reward: -9.66\n",
    "Episode 513\tLast reward: 0.00\tAverage reward: -9.17\n",
    "Episode 514\tLast reward: -13.00\tAverage reward: -9.37\n",
    "Episode 515\tLast reward: -6.00\tAverage reward: -9.20\n",
    "Episode 516\tLast reward: -15.00\tAverage reward: -9.49\n",
    "Episode 517\tLast reward: -2.00\tAverage reward: -9.11\n",
    "Episode 518\tLast reward: -7.00\tAverage reward: -9.01\n",
    "Episode 519\tLast reward: -14.00\tAverage reward: -9.26\n",
    "Episode 520\tLast reward: -9.00\tAverage reward: -9.24\n",
    "Episode 521\tLast reward: -13.00\tAverage reward: -9.43\n",
    "Episode 522\tLast reward: -9.00\tAverage reward: -9.41\n",
    "Episode 523\tLast reward: -11.00\tAverage reward: -9.49\n",
    "Episode 524\tLast reward: -13.00\tAverage reward: -9.67\n",
    "Episode 525\tLast reward: -8.00\tAverage reward: -9.58\n",
    "Episode 526\tLast reward: -12.00\tAverage reward: -9.70\n",
    "Episode 527\tLast reward: -12.00\tAverage reward: -9.82\n",
    "Episode 528\tLast reward: -15.00\tAverage reward: -10.08\n",
    "Episode 529\tLast reward: -11.00\tAverage reward: -10.12\n",
    "Episode 530\tLast reward: -13.00\tAverage reward: -10.27\n",
    "Episode 531\tLast reward: -8.00\tAverage reward: -10.15\n",
    "Episode 532\tLast reward: -10.00\tAverage reward: -10.15\n",
    "Episode 533\tLast reward: -3.00\tAverage reward: -9.79\n",
    "Episode 534\tLast reward: 0.00\tAverage reward: -9.30\n",
    "Episode 535\tLast reward: -15.00\tAverage reward: -9.58\n",
    "Episode 536\tLast reward: -5.00\tAverage reward: -9.36\n",
    "Episode 537\tLast reward: -6.00\tAverage reward: -9.19\n",
    "Episode 538\tLast reward: -2.00\tAverage reward: -8.83\n",
    "Episode 539\tLast reward: -13.00\tAverage reward: -9.04\n",
    "Episode 540\tLast reward: -6.00\tAverage reward: -8.88\n",
    "Episode 541\tLast reward: -11.00\tAverage reward: -8.99\n",
    "Episode 542\tLast reward: -13.00\tAverage reward: -9.19\n",
    "Episode 543\tLast reward: -4.00\tAverage reward: -8.93\n",
    "Episode 544\tLast reward: -10.00\tAverage reward: -8.98\n",
    "Episode 545\tLast reward: -17.00\tAverage reward: -9.39\n",
    "Episode 546\tLast reward: 2.00\tAverage reward: -8.82\n",
    "Episode 547\tLast reward: 0.00\tAverage reward: -8.38\n",
    "Episode 548\tLast reward: -9.00\tAverage reward: -8.41\n",
    "Episode 549\tLast reward: -9.00\tAverage reward: -8.44\n",
    "Episode 550\tLast reward: -14.00\tAverage reward: -8.71\n",
    "Episode 551\tLast reward: -2.00\tAverage reward: -8.38\n",
    "Episode 552\tLast reward: -7.00\tAverage reward: -8.31\n",
    "Episode 553\tLast reward: -7.00\tAverage reward: -8.24\n",
    "Episode 554\tLast reward: -12.00\tAverage reward: -8.43\n",
    "Episode 555\tLast reward: -6.00\tAverage reward: -8.31\n",
    "Episode 556\tLast reward: -16.00\tAverage reward: -8.70\n",
    "Episode 557\tLast reward: -13.00\tAverage reward: -8.91\n",
    "Episode 558\tLast reward: -14.00\tAverage reward: -9.16\n",
    "Episode 559\tLast reward: -13.00\tAverage reward: -9.36\n",
    "Episode 560\tLast reward: -13.00\tAverage reward: -9.54\n",
    "Episode 561\tLast reward: -13.00\tAverage reward: -9.71\n",
    "Episode 562\tLast reward: -5.00\tAverage reward: -9.48\n",
    "Episode 563\tLast reward: -11.00\tAverage reward: -9.55\n",
    "Episode 564\tLast reward: -1.00\tAverage reward: -9.12\n",
    "Episode 565\tLast reward: -14.00\tAverage reward: -9.37\n",
    "Episode 566\tLast reward: -12.00\tAverage reward: -9.50\n",
    "Episode 567\tLast reward: -15.00\tAverage reward: -9.78\n",
    "Episode 568\tLast reward: -10.00\tAverage reward: -9.79\n",
    "Episode 569\tLast reward: -13.00\tAverage reward: -9.95\n",
    "Episode 570\tLast reward: -10.00\tAverage reward: -9.95\n",
    "Episode 571\tLast reward: -11.00\tAverage reward: -10.00\n",
    "Episode 572\tLast reward: -11.00\tAverage reward: -10.05\n",
    "Episode 573\tLast reward: -11.00\tAverage reward: -10.10\n",
    "Episode 574\tLast reward: -10.00\tAverage reward: -10.09\n",
    "Episode 575\tLast reward: -7.00\tAverage reward: -9.94\n",
    "Episode 576\tLast reward: -9.00\tAverage reward: -9.89\n",
    "Episode 577\tLast reward: -15.00\tAverage reward: -10.15\n",
    "Episode 578\tLast reward: -7.00\tAverage reward: -9.99\n",
    "Episode 579\tLast reward: -13.00\tAverage reward: -10.14\n",
    "Episode 580\tLast reward: -8.00\tAverage reward: -10.03\n",
    "Episode 581\tLast reward: -10.00\tAverage reward: -10.03\n",
    "Episode 582\tLast reward: -12.00\tAverage reward: -10.13\n",
    "Episode 583\tLast reward: -6.00\tAverage reward: -9.92\n",
    "Episode 584\tLast reward: -17.00\tAverage reward: -10.28\n",
    "Episode 585\tLast reward: -11.00\tAverage reward: -10.31\n",
    "Episode 586\tLast reward: -3.00\tAverage reward: -9.95\n",
    "Episode 587\tLast reward: 2.00\tAverage reward: -9.35\n",
    "Episode 588\tLast reward: -17.00\tAverage reward: -9.73\n",
    "Episode 589\tLast reward: -12.00\tAverage reward: -9.85\n",
    "Episode 590\tLast reward: -7.00\tAverage reward: -9.70\n",
    "Episode 591\tLast reward: -11.00\tAverage reward: -9.77\n",
    "Episode 592\tLast reward: -15.00\tAverage reward: -10.03\n",
    "Episode 593\tLast reward: -13.00\tAverage reward: -10.18\n",
    "Episode 594\tLast reward: -7.00\tAverage reward: -10.02\n",
    "Episode 595\tLast reward: -8.00\tAverage reward: -9.92\n",
    "Episode 596\tLast reward: -7.00\tAverage reward: -9.77\n",
    "Episode 597\tLast reward: -10.00\tAverage reward: -9.78\n",
    "Episode 598\tLast reward: -11.00\tAverage reward: -9.85\n",
    "Episode 599\tLast reward: -7.00\tAverage reward: -9.70\n",
    "Episode 600\tLast reward: -5.00\tAverage reward: -9.47\n",
    "Episode 601\tLast reward: -6.00\tAverage reward: -9.29\n",
    "Episode 602\tLast reward: -8.00\tAverage reward: -9.23\n",
    "Episode 603\tLast reward: -9.00\tAverage reward: -9.22\n",
    "Episode 604\tLast reward: -2.00\tAverage reward: -8.86\n",
    "Episode 605\tLast reward: -7.00\tAverage reward: -8.76\n",
    "Episode 606\tLast reward: -17.00\tAverage reward: -9.18\n",
    "Episode 607\tLast reward: -17.00\tAverage reward: -9.57\n",
    "Episode 608\tLast reward: -13.00\tAverage reward: -9.74\n",
    "Episode 609\tLast reward: -8.00\tAverage reward: -9.65\n",
    "Episode 610\tLast reward: -5.00\tAverage reward: -9.42\n",
    "Episode 611\tLast reward: -7.00\tAverage reward: -9.30\n",
    "Episode 612\tLast reward: -11.00\tAverage reward: -9.38\n",
    "Episode 613\tLast reward: -14.00\tAverage reward: -9.61\n",
    "Episode 614\tLast reward: -2.00\tAverage reward: -9.23\n",
    "Episode 615\tLast reward: -11.00\tAverage reward: -9.32\n",
    "Episode 616\tLast reward: -9.00\tAverage reward: -9.31\n",
    "Episode 617\tLast reward: -9.00\tAverage reward: -9.29\n",
    "Episode 618\tLast reward: -5.00\tAverage reward: -9.08\n",
    "Episode 619\tLast reward: -9.00\tAverage reward: -9.07\n",
    "Episode 620\tLast reward: 0.00\tAverage reward: -8.62\n",
    "Episode 621\tLast reward: -2.00\tAverage reward: -8.29\n",
    "Episode 622\tLast reward: -3.00\tAverage reward: -8.02\n",
    "Episode 623\tLast reward: -6.00\tAverage reward: -7.92\n",
    "Episode 624\tLast reward: -4.00\tAverage reward: -7.73\n",
    "Episode 625\tLast reward: -3.00\tAverage reward: -7.49\n",
    "Episode 626\tLast reward: -9.00\tAverage reward: -7.57\n",
    "Episode 627\tLast reward: -8.00\tAverage reward: -7.59\n",
    "Episode 628\tLast reward: -11.00\tAverage reward: -7.76\n",
    "Episode 629\tLast reward: -5.00\tAverage reward: -7.62\n",
    "Episode 630\tLast reward: -10.00\tAverage reward: -7.74\n",
    "Episode 631\tLast reward: -13.00\tAverage reward: -8.00\n",
    "Episode 632\tLast reward: -9.00\tAverage reward: -8.05\n",
    "Episode 633\tLast reward: -9.00\tAverage reward: -8.10\n",
    "Episode 634\tLast reward: -17.00\tAverage reward: -8.54\n",
    "Episode 635\tLast reward: -2.00\tAverage reward: -8.22\n",
    "Episode 636\tLast reward: -7.00\tAverage reward: -8.16\n",
    "Episode 637\tLast reward: -9.00\tAverage reward: -8.20\n",
    "Episode 638\tLast reward: -12.00\tAverage reward: -8.39\n",
    "Episode 639\tLast reward: -7.00\tAverage reward: -8.32\n",
    "Episode 640\tLast reward: -9.00\tAverage reward: -8.35\n",
    "Episode 641\tLast reward: -12.00\tAverage reward: -8.54\n",
    "Episode 642\tLast reward: -13.00\tAverage reward: -8.76\n",
    "Episode 643\tLast reward: -8.00\tAverage reward: -8.72\n",
    "Episode 644\tLast reward: -11.00\tAverage reward: -8.83\n",
    "Episode 645\tLast reward: -14.00\tAverage reward: -9.09\n",
    "Episode 646\tLast reward: -13.00\tAverage reward: -9.29\n",
    "Episode 647\tLast reward: -3.00\tAverage reward: -8.97\n",
    "Episode 648\tLast reward: -5.00\tAverage reward: -8.78\n",
    "Episode 649\tLast reward: -9.00\tAverage reward: -8.79\n",
    "Episode 650\tLast reward: -11.00\tAverage reward: -8.90\n",
    "Episode 651\tLast reward: -9.00\tAverage reward: -8.90\n",
    "Episode 652\tLast reward: -13.00\tAverage reward: -9.11\n",
    "Episode 653\tLast reward: -14.00\tAverage reward: -9.35\n",
    "Episode 654\tLast reward: -9.00\tAverage reward: -9.33\n",
    "Episode 655\tLast reward: -9.00\tAverage reward: -9.32\n",
    "Episode 656\tLast reward: -7.00\tAverage reward: -9.20\n",
    "Episode 657\tLast reward: -11.00\tAverage reward: -9.29\n",
    "Episode 658\tLast reward: -13.00\tAverage reward: -9.48\n",
    "Episode 659\tLast reward: -3.00\tAverage reward: -9.15\n",
    "Episode 660\tLast reward: -9.00\tAverage reward: -9.15\n",
    "Episode 661\tLast reward: -5.00\tAverage reward: -8.94\n",
    "Episode 662\tLast reward: -15.00\tAverage reward: -9.24\n",
    "Episode 663\tLast reward: -9.00\tAverage reward: -9.23\n",
    "Episode 664\tLast reward: -2.00\tAverage reward: -8.87\n",
    "Episode 665\tLast reward: -1.00\tAverage reward: -8.47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for the last part and introducing the next one \n",
    "\n",
    "Adding the entropy is really helped the results to converge to reach sometimes -7 in the averege reward, and reach some positive values in the last rewards\n",
    "\n",
    "Now we will increase the value of the entropy coeff. to be (ENTROPY_BETA = 0.02) , as shown in the book this will give better results as they test it.\n",
    "\n",
    "Also we will add the CLIP_GRAD = 0.1, which clip the gradients when excedding some value to prevent them from getting very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the environment\n",
    "## env = gym.make('PongNoFrameskip-v4')\n",
    "env = gym.make('Pong-v0')\n",
    "##input_shape = env.observation_space.shape[0]\n",
    "#action_size = env.action_space.n\n",
    "\n",
    "action_size = 2\n",
    "\n",
    "##print(\"Env reward threshold: {}\".format(env.spec.reward_threshold))\n",
    "reward_list = list()\n",
    "\n",
    "input_shape = [1,80,80]\n",
    "\n",
    "##input_shape = [STACK_SIZE, 84, 84]\n",
    "\n",
    "\n",
    "pnet = torch.load('/content/drive/MyDrive/codes/cnn-entropy/600_A2C_pnet.pt').to(device)##,map_location=torch.device('cpu'))\n",
    "pnet.eval()\n",
    "\n",
    "cnet = torch.load('/content/drive/MyDrive/codes/cnn-entropy/600_A2C_cnet.pt').to(device)##,map_location=torch.device('cpu'))\n",
    "cnet.eval()\n",
    "\n",
    "\n",
    "## initialize an optimizer\n",
    "p_optimizer = torch.optim.Adam(pnet.parameters(), lr=1e-4,eps=1e-3,amsgrad=True)\n",
    "c_optimizer = torch.optim.Adam(cnet.parameters(), lr=1e-4,eps=1e-3,amsgrad=True)\n",
    "\n",
    "ENTROPY_BETA = 0.02\n",
    "\n",
    "\n",
    "\n",
    "running_reward  = None\n",
    "MEAN_REWARD_BOUND = 20\n",
    "\n",
    "UP_ACTION = 2\n",
    "\n",
    "DOWN_ACTION = 3\n",
    "Max_ep_rewared = -21\n",
    "#ENTROPY_BETA = 0.01\n",
    "import torch.nn.utils as nn_utils\n",
    "CLIP_GRAD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoLGnGeNrIbj",
    "outputId": "8754dfa7-624d-4d4d-bd83-8ebedf573d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tLast reward: -7.00\tAverage reward: -7.00\n",
      "Episode 2\tLast reward: -11.00\tAverage reward: -7.20\n",
      "Episode 3\tLast reward: -2.00\tAverage reward: -6.94\n",
      "Episode 4\tLast reward: -11.00\tAverage reward: -7.14\n",
      "Episode 5\tLast reward: -2.00\tAverage reward: -6.89\n",
      "Episode 6\tLast reward: -8.00\tAverage reward: -6.94\n",
      "Episode 7\tLast reward: -10.00\tAverage reward: -7.09\n",
      "Episode 8\tLast reward: -18.00\tAverage reward: -7.64\n",
      "Episode 9\tLast reward: -13.00\tAverage reward: -7.91\n",
      "Episode 10\tLast reward: -3.00\tAverage reward: -7.66\n",
      "Episode 11\tLast reward: -9.00\tAverage reward: -7.73\n",
      "Episode 12\tLast reward: -11.00\tAverage reward: -7.89\n",
      "Episode 13\tLast reward: -13.00\tAverage reward: -8.15\n",
      "Episode 14\tLast reward: -11.00\tAverage reward: -8.29\n",
      "Episode 15\tLast reward: -11.00\tAverage reward: -8.43\n",
      "Episode 16\tLast reward: -7.00\tAverage reward: -8.35\n",
      "Episode 17\tLast reward: -11.00\tAverage reward: -8.49\n",
      "Episode 18\tLast reward: -12.00\tAverage reward: -8.66\n",
      "Episode 19\tLast reward: -7.00\tAverage reward: -8.58\n",
      "Episode 20\tLast reward: -4.00\tAverage reward: -8.35\n",
      "Episode 21\tLast reward: -12.00\tAverage reward: -8.53\n",
      "Episode 22\tLast reward: -4.00\tAverage reward: -8.31\n",
      "Episode 23\tLast reward: -10.00\tAverage reward: -8.39\n",
      "Episode 24\tLast reward: -15.00\tAverage reward: -8.72\n",
      "Episode 25\tLast reward: 1.00\tAverage reward: -8.24\n",
      "Episode 26\tLast reward: -9.00\tAverage reward: -8.27\n",
      "Episode 27\tLast reward: 5.00\tAverage reward: -7.61\n",
      "Episode 28\tLast reward: -11.00\tAverage reward: -7.78\n",
      "Episode 29\tLast reward: -13.00\tAverage reward: -8.04\n",
      "Episode 30\tLast reward: -4.00\tAverage reward: -7.84\n",
      "Episode 31\tLast reward: -7.00\tAverage reward: -7.80\n",
      "Episode 32\tLast reward: -9.00\tAverage reward: -7.86\n",
      "Episode 33\tLast reward: -12.00\tAverage reward: -8.06\n",
      "Episode 34\tLast reward: -8.00\tAverage reward: -8.06\n",
      "Episode 35\tLast reward: -11.00\tAverage reward: -8.21\n",
      "Episode 36\tLast reward: -13.00\tAverage reward: -8.45\n",
      "Episode 37\tLast reward: -3.00\tAverage reward: -8.17\n",
      "Episode 38\tLast reward: -13.00\tAverage reward: -8.42\n",
      "Episode 39\tLast reward: -9.00\tAverage reward: -8.45\n",
      "Episode 40\tLast reward: -15.00\tAverage reward: -8.77\n",
      "Episode 41\tLast reward: -9.00\tAverage reward: -8.78\n",
      "Episode 42\tLast reward: -1.00\tAverage reward: -8.40\n",
      "Episode 43\tLast reward: 0.00\tAverage reward: -7.98\n",
      "Episode 44\tLast reward: -19.00\tAverage reward: -8.53\n",
      "Episode 45\tLast reward: -5.00\tAverage reward: -8.35\n",
      "Episode 46\tLast reward: -16.00\tAverage reward: -8.73\n",
      "Episode 47\tLast reward: -8.00\tAverage reward: -8.70\n",
      "Episode 48\tLast reward: -7.00\tAverage reward: -8.61\n",
      "Episode 49\tLast reward: -7.00\tAverage reward: -8.53\n",
      "Episode 50\tLast reward: -10.00\tAverage reward: -8.60\n",
      "Episode 51\tLast reward: -7.00\tAverage reward: -8.52\n",
      "Episode 52\tLast reward: -13.00\tAverage reward: -8.75\n",
      "Episode 53\tLast reward: -15.00\tAverage reward: -9.06\n",
      "Episode 54\tLast reward: -8.00\tAverage reward: -9.01\n",
      "Episode 55\tLast reward: -15.00\tAverage reward: -9.31\n",
      "Episode 56\tLast reward: -2.00\tAverage reward: -8.94\n",
      "Episode 57\tLast reward: -11.00\tAverage reward: -9.04\n",
      "Episode 58\tLast reward: -12.00\tAverage reward: -9.19\n",
      "Episode 59\tLast reward: -12.00\tAverage reward: -9.33\n",
      "Episode 60\tLast reward: -11.00\tAverage reward: -9.42\n",
      "Episode 61\tLast reward: -13.00\tAverage reward: -9.60\n",
      "Episode 62\tLast reward: -9.00\tAverage reward: -9.57\n",
      "Episode 63\tLast reward: -5.00\tAverage reward: -9.34\n",
      "Episode 64\tLast reward: -8.00\tAverage reward: -9.27\n",
      "Episode 65\tLast reward: 1.00\tAverage reward: -8.76\n",
      "Episode 66\tLast reward: -9.00\tAverage reward: -8.77\n",
      "Episode 67\tLast reward: -11.00\tAverage reward: -8.88\n",
      "Episode 68\tLast reward: -2.00\tAverage reward: -8.54\n",
      "Episode 69\tLast reward: -7.00\tAverage reward: -8.46\n",
      "Episode 70\tLast reward: -10.00\tAverage reward: -8.54\n",
      "Episode 71\tLast reward: -11.00\tAverage reward: -8.66\n",
      "Episode 72\tLast reward: -10.00\tAverage reward: -8.73\n",
      "Episode 73\tLast reward: -10.00\tAverage reward: -8.79\n",
      "Episode 74\tLast reward: -13.00\tAverage reward: -9.00\n",
      "Episode 75\tLast reward: -11.00\tAverage reward: -9.10\n",
      "Episode 76\tLast reward: -15.00\tAverage reward: -9.40\n",
      "Episode 77\tLast reward: -2.00\tAverage reward: -9.03\n",
      "Episode 78\tLast reward: -12.00\tAverage reward: -9.17\n",
      "Episode 79\tLast reward: -6.00\tAverage reward: -9.02\n",
      "Episode 80\tLast reward: -14.00\tAverage reward: -9.27\n",
      "Episode 81\tLast reward: -4.00\tAverage reward: -9.00\n",
      "Episode 82\tLast reward: -11.00\tAverage reward: -9.10\n",
      "Episode 83\tLast reward: -3.00\tAverage reward: -8.80\n",
      "Episode 84\tLast reward: 2.00\tAverage reward: -8.26\n",
      "Episode 85\tLast reward: -9.00\tAverage reward: -8.29\n",
      "Episode 86\tLast reward: -10.00\tAverage reward: -8.38\n",
      "Episode 87\tLast reward: -13.00\tAverage reward: -8.61\n",
      "Episode 88\tLast reward: -7.00\tAverage reward: -8.53\n",
      "Episode 89\tLast reward: -7.00\tAverage reward: -8.45\n",
      "Episode 90\tLast reward: -11.00\tAverage reward: -8.58\n",
      "Episode 91\tLast reward: -9.00\tAverage reward: -8.60\n",
      "Episode 92\tLast reward: -13.00\tAverage reward: -8.82\n",
      "Episode 93\tLast reward: -3.00\tAverage reward: -8.53\n",
      "Episode 94\tLast reward: -15.00\tAverage reward: -8.85\n",
      "Episode 95\tLast reward: -17.00\tAverage reward: -9.26\n",
      "Episode 96\tLast reward: -11.00\tAverage reward: -9.35\n",
      "Episode 97\tLast reward: -1.00\tAverage reward: -8.93\n",
      "Episode 98\tLast reward: -5.00\tAverage reward: -8.73\n",
      "Episode 99\tLast reward: -6.00\tAverage reward: -8.60\n",
      "Episode 100\tLast reward: -11.00\tAverage reward: -8.72\n",
      "Episode 101\tLast reward: -4.00\tAverage reward: -8.48\n",
      "Episode 102\tLast reward: -15.00\tAverage reward: -8.81\n",
      "Episode 103\tLast reward: -11.00\tAverage reward: -8.92\n",
      "Episode 104\tLast reward: -5.00\tAverage reward: -8.72\n",
      "Episode 105\tLast reward: -17.00\tAverage reward: -9.14\n",
      "Episode 106\tLast reward: -15.00\tAverage reward: -9.43\n",
      "Episode 107\tLast reward: -13.00\tAverage reward: -9.61\n",
      "Episode 108\tLast reward: -13.00\tAverage reward: -9.78\n",
      "Episode 109\tLast reward: 4.00\tAverage reward: -9.09\n",
      "Episode 110\tLast reward: -2.00\tAverage reward: -8.73\n",
      "Episode 111\tLast reward: -5.00\tAverage reward: -8.55\n",
      "Episode 112\tLast reward: -13.00\tAverage reward: -8.77\n",
      "Episode 113\tLast reward: -14.00\tAverage reward: -9.03\n",
      "Episode 114\tLast reward: -1.00\tAverage reward: -8.63\n",
      "Episode 115\tLast reward: 1.00\tAverage reward: -8.15\n",
      "Episode 116\tLast reward: -9.00\tAverage reward: -8.19\n",
      "Episode 117\tLast reward: -9.00\tAverage reward: -8.23\n",
      "Episode 118\tLast reward: -4.00\tAverage reward: -8.02\n",
      "Episode 119\tLast reward: -13.00\tAverage reward: -8.27\n",
      "Episode 120\tLast reward: -10.00\tAverage reward: -8.36\n",
      "Episode 121\tLast reward: -10.00\tAverage reward: -8.44\n",
      "Episode 122\tLast reward: -4.00\tAverage reward: -8.22\n",
      "Episode 123\tLast reward: -8.00\tAverage reward: -8.20\n",
      "Episode 124\tLast reward: -7.00\tAverage reward: -8.14\n",
      "Episode 125\tLast reward: -12.00\tAverage reward: -8.34\n",
      "Episode 126\tLast reward: -2.00\tAverage reward: -8.02\n",
      "Episode 127\tLast reward: -14.00\tAverage reward: -8.32\n",
      "Episode 128\tLast reward: 1.00\tAverage reward: -7.85\n",
      "Episode 129\tLast reward: -7.00\tAverage reward: -7.81\n",
      "Episode 130\tLast reward: -10.00\tAverage reward: -7.92\n",
      "Episode 131\tLast reward: -9.00\tAverage reward: -7.97\n",
      "Episode 132\tLast reward: -11.00\tAverage reward: -8.13\n",
      "Episode 133\tLast reward: -9.00\tAverage reward: -8.17\n",
      "Episode 134\tLast reward: -10.00\tAverage reward: -8.26\n",
      "Episode 135\tLast reward: -8.00\tAverage reward: -8.25\n",
      "Episode 136\tLast reward: -7.00\tAverage reward: -8.19\n",
      "Episode 137\tLast reward: -12.00\tAverage reward: -8.38\n",
      "Episode 138\tLast reward: -3.00\tAverage reward: -8.11\n",
      "Episode 139\tLast reward: 3.00\tAverage reward: -7.55\n",
      "Episode 140\tLast reward: -7.00\tAverage reward: -7.52\n",
      "Episode 141\tLast reward: -6.00\tAverage reward: -7.45\n",
      "Episode 142\tLast reward: -15.00\tAverage reward: -7.83\n",
      "Episode 143\tLast reward: -8.00\tAverage reward: -7.83\n",
      "Episode 144\tLast reward: -5.00\tAverage reward: -7.69\n",
      "Episode 145\tLast reward: -5.00\tAverage reward: -7.56\n",
      "Episode 146\tLast reward: -13.00\tAverage reward: -7.83\n",
      "Episode 147\tLast reward: -10.00\tAverage reward: -7.94\n",
      "Episode 148\tLast reward: -3.00\tAverage reward: -7.69\n",
      "Episode 149\tLast reward: -6.00\tAverage reward: -7.61\n",
      "Episode 150\tLast reward: -11.00\tAverage reward: -7.78\n",
      "Episode 151\tLast reward: -4.00\tAverage reward: -7.59\n",
      "Episode 152\tLast reward: -16.00\tAverage reward: -8.01\n",
      "Episode 153\tLast reward: -3.00\tAverage reward: -7.76\n",
      "Episode 154\tLast reward: -9.00\tAverage reward: -7.82\n",
      "Episode 155\tLast reward: -13.00\tAverage reward: -8.08\n",
      "Episode 156\tLast reward: -6.00\tAverage reward: -7.98\n",
      "Episode 157\tLast reward: -8.00\tAverage reward: -7.98\n",
      "Episode 158\tLast reward: -9.00\tAverage reward: -8.03\n",
      "Episode 159\tLast reward: -9.00\tAverage reward: -8.08\n",
      "Episode 160\tLast reward: -13.00\tAverage reward: -8.32\n",
      "Episode 161\tLast reward: -4.00\tAverage reward: -8.11\n",
      "Episode 162\tLast reward: -10.00\tAverage reward: -8.20\n",
      "Episode 163\tLast reward: -4.00\tAverage reward: -7.99\n",
      "Episode 164\tLast reward: -1.00\tAverage reward: -7.64\n",
      "Episode 165\tLast reward: -2.00\tAverage reward: -7.36\n",
      "Episode 166\tLast reward: -5.00\tAverage reward: -7.24\n",
      "Episode 167\tLast reward: -7.00\tAverage reward: -7.23\n",
      "Episode 168\tLast reward: 3.00\tAverage reward: -6.72\n",
      "Episode 169\tLast reward: -4.00\tAverage reward: -6.58\n",
      "Episode 170\tLast reward: -11.00\tAverage reward: -6.80\n",
      "Episode 171\tLast reward: -15.00\tAverage reward: -7.21\n",
      "Episode 172\tLast reward: -15.00\tAverage reward: -7.60\n",
      "Episode 173\tLast reward: -14.00\tAverage reward: -7.92\n",
      "Episode 174\tLast reward: 4.00\tAverage reward: -7.33\n",
      "Episode 175\tLast reward: -11.00\tAverage reward: -7.51\n",
      "Episode 176\tLast reward: -12.00\tAverage reward: -7.73\n",
      "Episode 177\tLast reward: -14.00\tAverage reward: -8.05\n",
      "Episode 178\tLast reward: -5.00\tAverage reward: -7.90\n",
      "Episode 179\tLast reward: 1.00\tAverage reward: -7.45\n",
      "Episode 180\tLast reward: -4.00\tAverage reward: -7.28\n",
      "Episode 181\tLast reward: -2.00\tAverage reward: -7.01\n",
      "Episode 182\tLast reward: -7.00\tAverage reward: -7.01\n",
      "Episode 183\tLast reward: 0.00\tAverage reward: -6.66\n",
      "Episode 184\tLast reward: -4.00\tAverage reward: -6.53\n",
      "Episode 185\tLast reward: -8.00\tAverage reward: -6.60\n",
      "Episode 186\tLast reward: -2.00\tAverage reward: -6.37\n",
      "Episode 187\tLast reward: -14.00\tAverage reward: -6.75\n",
      "Episode 188\tLast reward: -5.00\tAverage reward: -6.67\n",
      "Episode 189\tLast reward: -13.00\tAverage reward: -6.98\n",
      "Episode 190\tLast reward: -15.00\tAverage reward: -7.38\n",
      "Episode 191\tLast reward: -11.00\tAverage reward: -7.56\n",
      "Episode 192\tLast reward: -9.00\tAverage reward: -7.64\n",
      "Episode 193\tLast reward: -2.00\tAverage reward: -7.35\n",
      "Episode 194\tLast reward: -10.00\tAverage reward: -7.49\n",
      "Episode 195\tLast reward: 3.00\tAverage reward: -6.96\n",
      "Episode 196\tLast reward: -7.00\tAverage reward: -6.96\n",
      "Episode 197\tLast reward: -11.00\tAverage reward: -7.17\n",
      "Episode 198\tLast reward: -9.00\tAverage reward: -7.26\n",
      "Episode 199\tLast reward: -13.00\tAverage reward: -7.55\n",
      "Episode 200\tLast reward: 1.00\tAverage reward: -7.12\n",
      "Episode 201\tLast reward: -1.00\tAverage reward: -6.81\n",
      "Episode 202\tLast reward: 3.00\tAverage reward: -6.32\n",
      "Episode 203\tLast reward: -15.00\tAverage reward: -6.76\n",
      "Episode 204\tLast reward: -11.00\tAverage reward: -6.97\n",
      "Episode 205\tLast reward: -5.00\tAverage reward: -6.87\n",
      "Episode 206\tLast reward: -7.00\tAverage reward: -6.88\n",
      "Episode 207\tLast reward: -10.00\tAverage reward: -7.03\n",
      "Episode 208\tLast reward: 5.00\tAverage reward: -6.43\n",
      "Episode 209\tLast reward: -1.00\tAverage reward: -6.16\n",
      "Episode 210\tLast reward: -9.00\tAverage reward: -6.30\n",
      "Episode 211\tLast reward: -5.00\tAverage reward: -6.24\n",
      "Episode 212\tLast reward: -7.00\tAverage reward: -6.27\n",
      "Episode 213\tLast reward: -10.00\tAverage reward: -6.46\n",
      "Episode 214\tLast reward: -14.00\tAverage reward: -6.84\n",
      "Episode 215\tLast reward: -2.00\tAverage reward: -6.60\n",
      "Episode 216\tLast reward: -9.00\tAverage reward: -6.72\n",
      "Episode 217\tLast reward: -13.00\tAverage reward: -7.03\n",
      "Episode 218\tLast reward: 1.00\tAverage reward: -6.63\n",
      "Episode 219\tLast reward: -6.00\tAverage reward: -6.60\n",
      "Episode 220\tLast reward: -7.00\tAverage reward: -6.62\n",
      "Episode 221\tLast reward: -7.00\tAverage reward: -6.64\n",
      "Episode 222\tLast reward: -11.00\tAverage reward: -6.85\n",
      "Episode 223\tLast reward: -3.00\tAverage reward: -6.66\n",
      "Episode 224\tLast reward: -3.00\tAverage reward: -6.48\n",
      "Episode 225\tLast reward: -10.00\tAverage reward: -6.65\n",
      "Episode 226\tLast reward: -9.00\tAverage reward: -6.77\n",
      "Episode 227\tLast reward: -7.00\tAverage reward: -6.78\n",
      "Episode 228\tLast reward: -3.00\tAverage reward: -6.59\n",
      "Episode 229\tLast reward: 1.00\tAverage reward: -6.21\n",
      "Episode 230\tLast reward: -4.00\tAverage reward: -6.10\n",
      "Episode 231\tLast reward: -1.00\tAverage reward: -5.85\n",
      "Episode 232\tLast reward: -9.00\tAverage reward: -6.01\n",
      "Episode 233\tLast reward: -11.00\tAverage reward: -6.26\n",
      "Episode 234\tLast reward: -10.00\tAverage reward: -6.44\n",
      "Episode 235\tLast reward: -3.00\tAverage reward: -6.27\n",
      "Episode 236\tLast reward: -8.00\tAverage reward: -6.36\n",
      "Episode 237\tLast reward: -17.00\tAverage reward: -6.89\n",
      "Episode 238\tLast reward: -8.00\tAverage reward: -6.95\n",
      "Episode 239\tLast reward: -14.00\tAverage reward: -7.30\n",
      "Episode 240\tLast reward: -9.00\tAverage reward: -7.38\n",
      "Episode 241\tLast reward: -2.00\tAverage reward: -7.11\n",
      "Episode 242\tLast reward: -2.00\tAverage reward: -6.86\n",
      "Episode 243\tLast reward: 4.00\tAverage reward: -6.32\n",
      "Episode 244\tLast reward: -2.00\tAverage reward: -6.10\n",
      "Episode 245\tLast reward: -16.00\tAverage reward: -6.59\n",
      "Episode 246\tLast reward: -9.00\tAverage reward: -6.71\n",
      "Episode 247\tLast reward: -11.00\tAverage reward: -6.93\n",
      "Episode 248\tLast reward: -11.00\tAverage reward: -7.13\n",
      "Episode 249\tLast reward: -13.00\tAverage reward: -7.43\n",
      "Episode 250\tLast reward: -2.00\tAverage reward: -7.15\n",
      "Episode 251\tLast reward: -9.00\tAverage reward: -7.25\n",
      "Episode 252\tLast reward: -1.00\tAverage reward: -6.93\n",
      "Episode 253\tLast reward: -5.00\tAverage reward: -6.84\n",
      "Episode 254\tLast reward: 0.00\tAverage reward: -6.50\n",
      "Episode 255\tLast reward: -14.00\tAverage reward: -6.87\n",
      "Episode 256\tLast reward: 3.00\tAverage reward: -6.38\n",
      "Episode 257\tLast reward: -10.00\tAverage reward: -6.56\n",
      "Episode 258\tLast reward: -9.00\tAverage reward: -6.68\n",
      "Episode 259\tLast reward: -7.00\tAverage reward: -6.70\n",
      "Episode 260\tLast reward: -5.00\tAverage reward: -6.61\n",
      "Episode 261\tLast reward: -7.00\tAverage reward: -6.63\n",
      "Episode 262\tLast reward: -11.00\tAverage reward: -6.85\n",
      "Episode 263\tLast reward: -8.00\tAverage reward: -6.91\n",
      "Episode 264\tLast reward: -12.00\tAverage reward: -7.16\n",
      "Episode 265\tLast reward: -3.00\tAverage reward: -6.95\n",
      "Episode 266\tLast reward: 2.00\tAverage reward: -6.51\n",
      "Episode 267\tLast reward: -3.00\tAverage reward: -6.33\n",
      "Episode 268\tLast reward: -11.00\tAverage reward: -6.56\n",
      "Episode 269\tLast reward: 3.00\tAverage reward: -6.09\n",
      "Episode 270\tLast reward: -9.00\tAverage reward: -6.23\n",
      "Episode 271\tLast reward: -5.00\tAverage reward: -6.17\n",
      "Episode 272\tLast reward: -8.00\tAverage reward: -6.26\n",
      "Episode 273\tLast reward: -3.00\tAverage reward: -6.10\n",
      "Episode 274\tLast reward: -9.00\tAverage reward: -6.24\n",
      "Episode 275\tLast reward: -8.00\tAverage reward: -6.33\n",
      "Episode 276\tLast reward: 2.00\tAverage reward: -5.91\n",
      "Episode 277\tLast reward: -3.00\tAverage reward: -5.77\n",
      "Episode 278\tLast reward: -7.00\tAverage reward: -5.83\n",
      "Episode 279\tLast reward: -10.00\tAverage reward: -6.04\n",
      "Episode 280\tLast reward: -15.00\tAverage reward: -6.49\n",
      "Episode 281\tLast reward: 4.00\tAverage reward: -5.96\n",
      "Episode 282\tLast reward: -11.00\tAverage reward: -6.21\n",
      "Episode 283\tLast reward: -3.00\tAverage reward: -6.05\n",
      "Episode 284\tLast reward: 3.00\tAverage reward: -5.60\n",
      "Episode 285\tLast reward: 1.00\tAverage reward: -5.27\n",
      "Episode 286\tLast reward: -7.00\tAverage reward: -5.36\n",
      "Episode 287\tLast reward: -12.00\tAverage reward: -5.69\n",
      "Episode 288\tLast reward: -14.00\tAverage reward: -6.11\n",
      "Episode 289\tLast reward: -2.00\tAverage reward: -5.90\n",
      "Episode 290\tLast reward: -3.00\tAverage reward: -5.75\n",
      "Episode 291\tLast reward: -6.00\tAverage reward: -5.77\n",
      "Episode 292\tLast reward: -4.00\tAverage reward: -5.68\n",
      "Episode 293\tLast reward: -4.00\tAverage reward: -5.59\n",
      "Episode 294\tLast reward: -6.00\tAverage reward: -5.62\n",
      "Episode 295\tLast reward: -8.00\tAverage reward: -5.73\n",
      "Episode 296\tLast reward: -12.00\tAverage reward: -6.05\n",
      "Episode 297\tLast reward: -7.00\tAverage reward: -6.10\n",
      "Episode 298\tLast reward: -9.00\tAverage reward: -6.24\n",
      "Episode 299\tLast reward: -7.00\tAverage reward: -6.28\n",
      "Episode 300\tLast reward: -1.00\tAverage reward: -6.01\n",
      "Episode 301\tLast reward: -13.00\tAverage reward: -6.36\n",
      "Episode 302\tLast reward: -2.00\tAverage reward: -6.15\n",
      "Episode 303\tLast reward: -6.00\tAverage reward: -6.14\n",
      "Episode 304\tLast reward: -15.00\tAverage reward: -6.58\n",
      "Episode 305\tLast reward: -13.00\tAverage reward: -6.90\n",
      "Episode 306\tLast reward: -7.00\tAverage reward: -6.91\n",
      "Episode 307\tLast reward: -8.00\tAverage reward: -6.96\n",
      "Episode 308\tLast reward: 0.00\tAverage reward: -6.61\n",
      "Episode 309\tLast reward: -1.00\tAverage reward: -6.33\n",
      "Episode 310\tLast reward: -13.00\tAverage reward: -6.67\n",
      "Episode 311\tLast reward: -15.00\tAverage reward: -7.08\n",
      "Episode 312\tLast reward: 3.00\tAverage reward: -6.58\n",
      "Episode 313\tLast reward: -6.00\tAverage reward: -6.55\n",
      "Episode 314\tLast reward: -8.00\tAverage reward: -6.62\n",
      "Episode 315\tLast reward: -9.00\tAverage reward: -6.74\n",
      "Episode 316\tLast reward: -3.00\tAverage reward: -6.55\n",
      "Episode 317\tLast reward: -4.00\tAverage reward: -6.43\n",
      "Episode 318\tLast reward: -10.00\tAverage reward: -6.61\n",
      "Episode 319\tLast reward: -6.00\tAverage reward: -6.58\n",
      "Episode 320\tLast reward: -9.00\tAverage reward: -6.70\n",
      "Episode 321\tLast reward: -7.00\tAverage reward: -6.71\n",
      "Episode 322\tLast reward: -10.00\tAverage reward: -6.88\n",
      "Episode 323\tLast reward: -10.00\tAverage reward: -7.03\n",
      "Episode 324\tLast reward: -13.00\tAverage reward: -7.33\n",
      "Episode 325\tLast reward: -13.00\tAverage reward: -7.61\n",
      "Episode 326\tLast reward: -9.00\tAverage reward: -7.68\n",
      "Episode 327\tLast reward: -2.00\tAverage reward: -7.40\n",
      "Episode 328\tLast reward: -5.00\tAverage reward: -7.28\n",
      "Episode 329\tLast reward: -8.00\tAverage reward: -7.32\n",
      "Episode 330\tLast reward: -15.00\tAverage reward: -7.70\n",
      "Episode 331\tLast reward: -11.00\tAverage reward: -7.86\n",
      "Episode 332\tLast reward: -6.00\tAverage reward: -7.77\n",
      "Episode 333\tLast reward: 4.00\tAverage reward: -7.18\n",
      "Episode 334\tLast reward: 4.00\tAverage reward: -6.62\n",
      "Episode 335\tLast reward: -9.00\tAverage reward: -6.74\n",
      "Episode 336\tLast reward: -7.00\tAverage reward: -6.76\n",
      "Episode 337\tLast reward: -13.00\tAverage reward: -7.07\n",
      "Episode 338\tLast reward: -5.00\tAverage reward: -6.96\n",
      "Episode 339\tLast reward: -3.00\tAverage reward: -6.77\n",
      "Episode 340\tLast reward: -9.00\tAverage reward: -6.88\n",
      "Episode 341\tLast reward: -16.00\tAverage reward: -7.33\n",
      "Episode 342\tLast reward: -11.00\tAverage reward: -7.52\n",
      "Episode 343\tLast reward: 4.00\tAverage reward: -6.94\n",
      "Episode 344\tLast reward: 0.00\tAverage reward: -6.59\n",
      "Episode 345\tLast reward: -2.00\tAverage reward: -6.36\n",
      "Episode 346\tLast reward: -10.00\tAverage reward: -6.55\n",
      "Episode 347\tLast reward: -8.00\tAverage reward: -6.62\n",
      "Episode 348\tLast reward: -11.00\tAverage reward: -6.84\n",
      "Episode 349\tLast reward: 2.00\tAverage reward: -6.40\n",
      "Episode 350\tLast reward: -10.00\tAverage reward: -6.58\n",
      "Episode 351\tLast reward: -9.00\tAverage reward: -6.70\n",
      "Episode 352\tLast reward: -9.00\tAverage reward: -6.81\n",
      "Episode 353\tLast reward: -5.00\tAverage reward: -6.72\n",
      "Episode 354\tLast reward: -11.00\tAverage reward: -6.94\n",
      "Episode 355\tLast reward: -12.00\tAverage reward: -7.19\n",
      "Episode 356\tLast reward: -15.00\tAverage reward: -7.58\n",
      "Episode 357\tLast reward: -1.00\tAverage reward: -7.25\n",
      "Episode 358\tLast reward: -6.00\tAverage reward: -7.19\n",
      "Episode 359\tLast reward: -13.00\tAverage reward: -7.48\n",
      "Episode 360\tLast reward: -16.00\tAverage reward: -7.90\n",
      "Episode 361\tLast reward: -8.00\tAverage reward: -7.91\n",
      "Episode 362\tLast reward: 0.00\tAverage reward: -7.51\n",
      "Episode 363\tLast reward: -3.00\tAverage reward: -7.29\n",
      "Episode 364\tLast reward: -7.00\tAverage reward: -7.27\n",
      "Episode 365\tLast reward: -17.00\tAverage reward: -7.76\n",
      "Episode 366\tLast reward: -10.00\tAverage reward: -7.87\n",
      "Episode 367\tLast reward: -4.00\tAverage reward: -7.68\n",
      "Episode 368\tLast reward: -7.00\tAverage reward: -7.64\n",
      "Episode 369\tLast reward: -11.00\tAverage reward: -7.81\n",
      "Episode 370\tLast reward: -6.00\tAverage reward: -7.72\n",
      "Episode 371\tLast reward: -11.00\tAverage reward: -7.89\n",
      "Episode 372\tLast reward: -14.00\tAverage reward: -8.19\n",
      "Episode 373\tLast reward: -13.00\tAverage reward: -8.43\n",
      "Episode 374\tLast reward: -2.00\tAverage reward: -8.11\n",
      "Episode 375\tLast reward: -13.00\tAverage reward: -8.35\n",
      "Episode 376\tLast reward: -4.00\tAverage reward: -8.14\n",
      "Episode 377\tLast reward: -3.00\tAverage reward: -7.88\n",
      "Episode 378\tLast reward: -13.00\tAverage reward: -8.14\n",
      "Episode 379\tLast reward: -8.00\tAverage reward: -8.13\n",
      "Episode 380\tLast reward: -9.00\tAverage reward: -8.17\n",
      "Episode 381\tLast reward: -9.00\tAverage reward: -8.21\n",
      "Episode 382\tLast reward: -1.00\tAverage reward: -7.85\n",
      "Episode 383\tLast reward: -5.00\tAverage reward: -7.71\n",
      "Episode 384\tLast reward: -12.00\tAverage reward: -7.93\n",
      "Episode 385\tLast reward: -11.00\tAverage reward: -8.08\n",
      "Episode 386\tLast reward: -7.00\tAverage reward: -8.03\n",
      "Episode 387\tLast reward: -6.00\tAverage reward: -7.92\n",
      "Episode 388\tLast reward: 0.00\tAverage reward: -7.53\n",
      "Episode 389\tLast reward: -3.00\tAverage reward: -7.30\n",
      "Episode 390\tLast reward: -11.00\tAverage reward: -7.49\n",
      "Episode 391\tLast reward: -15.00\tAverage reward: -7.86\n",
      "Episode 392\tLast reward: -14.00\tAverage reward: -8.17\n",
      "Episode 393\tLast reward: -11.00\tAverage reward: -8.31\n",
      "Episode 394\tLast reward: -1.00\tAverage reward: -7.94\n",
      "Episode 395\tLast reward: -10.00\tAverage reward: -8.05\n",
      "Episode 396\tLast reward: -11.00\tAverage reward: -8.20\n",
      "Episode 397\tLast reward: -9.00\tAverage reward: -8.24\n",
      "Episode 398\tLast reward: -8.00\tAverage reward: -8.22\n",
      "Episode 399\tLast reward: -9.00\tAverage reward: -8.26\n",
      "Episode 400\tLast reward: -13.00\tAverage reward: -8.50\n",
      "Episode 401\tLast reward: -11.00\tAverage reward: -8.62\n",
      "Episode 402\tLast reward: -2.00\tAverage reward: -8.29\n",
      "Episode 403\tLast reward: -9.00\tAverage reward: -8.33\n",
      "Episode 404\tLast reward: -7.00\tAverage reward: -8.26\n",
      "Episode 405\tLast reward: -10.00\tAverage reward: -8.35\n",
      "Episode 406\tLast reward: -8.00\tAverage reward: -8.33\n",
      "Episode 407\tLast reward: -5.00\tAverage reward: -8.16\n",
      "Episode 408\tLast reward: -9.00\tAverage reward: -8.21\n",
      "Episode 409\tLast reward: 5.00\tAverage reward: -7.55\n",
      "Episode 410\tLast reward: -8.00\tAverage reward: -7.57\n",
      "Episode 411\tLast reward: -7.00\tAverage reward: -7.54\n",
      "Episode 412\tLast reward: -13.00\tAverage reward: -7.81\n",
      "Episode 413\tLast reward: 6.00\tAverage reward: -7.12\n",
      "Episode 414\tLast reward: -7.00\tAverage reward: -7.12\n",
      "Episode 415\tLast reward: 5.00\tAverage reward: -6.51\n",
      "Episode 416\tLast reward: -12.00\tAverage reward: -6.79\n",
      "Episode 417\tLast reward: -15.00\tAverage reward: -7.20\n",
      "Episode 418\tLast reward: -14.00\tAverage reward: -7.54\n",
      "Episode 419\tLast reward: -6.00\tAverage reward: -7.46\n",
      "Episode 420\tLast reward: -8.00\tAverage reward: -7.49\n",
      "Episode 421\tLast reward: 2.00\tAverage reward: -7.01\n",
      "Episode 422\tLast reward: -11.00\tAverage reward: -7.21\n",
      "Episode 423\tLast reward: 6.00\tAverage reward: -6.55\n",
      "Episode 424\tLast reward: -3.00\tAverage reward: -6.37\n",
      "Episode 425\tLast reward: -2.00\tAverage reward: -6.15\n",
      "Episode 426\tLast reward: -4.00\tAverage reward: -6.05\n",
      "Episode 427\tLast reward: -10.00\tAverage reward: -6.24\n",
      "Episode 428\tLast reward: -9.00\tAverage reward: -6.38\n",
      "Episode 429\tLast reward: -11.00\tAverage reward: -6.61\n",
      "Episode 430\tLast reward: -2.00\tAverage reward: -6.38\n",
      "Episode 431\tLast reward: -3.00\tAverage reward: -6.21\n",
      "Episode 432\tLast reward: -2.00\tAverage reward: -6.00\n",
      "Episode 433\tLast reward: -8.00\tAverage reward: -6.10\n",
      "Episode 434\tLast reward: -4.00\tAverage reward: -6.00\n",
      "Episode 435\tLast reward: 4.00\tAverage reward: -5.50\n",
      "Episode 436\tLast reward: -4.00\tAverage reward: -5.42\n",
      "Episode 437\tLast reward: -6.00\tAverage reward: -5.45\n",
      "Episode 438\tLast reward: 0.00\tAverage reward: -5.18\n",
      "Episode 439\tLast reward: -16.00\tAverage reward: -5.72\n",
      "Episode 440\tLast reward: -10.00\tAverage reward: -5.93\n",
      "Episode 441\tLast reward: -9.00\tAverage reward: -6.09\n",
      "Episode 442\tLast reward: -7.00\tAverage reward: -6.13\n",
      "Episode 443\tLast reward: -15.00\tAverage reward: -6.58\n",
      "Episode 444\tLast reward: -13.00\tAverage reward: -6.90\n",
      "Episode 445\tLast reward: -14.00\tAverage reward: -7.25\n",
      "Episode 446\tLast reward: -9.00\tAverage reward: -7.34\n",
      "Episode 447\tLast reward: -14.00\tAverage reward: -7.67\n",
      "Episode 448\tLast reward: -6.00\tAverage reward: -7.59\n",
      "Episode 449\tLast reward: -1.00\tAverage reward: -7.26\n",
      "Episode 450\tLast reward: -13.00\tAverage reward: -7.55\n",
      "Episode 451\tLast reward: -5.00\tAverage reward: -7.42\n",
      "Episode 452\tLast reward: -13.00\tAverage reward: -7.70\n",
      "Episode 453\tLast reward: 5.00\tAverage reward: -7.06\n",
      "Episode 454\tLast reward: -8.00\tAverage reward: -7.11\n",
      "Episode 455\tLast reward: -8.00\tAverage reward: -7.15\n",
      "Episode 456\tLast reward: -15.00\tAverage reward: -7.55\n",
      "Episode 457\tLast reward: -9.00\tAverage reward: -7.62\n",
      "Episode 458\tLast reward: -6.00\tAverage reward: -7.54\n",
      "Episode 459\tLast reward: -7.00\tAverage reward: -7.51\n",
      "Episode 460\tLast reward: -5.00\tAverage reward: -7.39\n",
      "Episode 461\tLast reward: -6.00\tAverage reward: -7.32\n",
      "Episode 462\tLast reward: -13.00\tAverage reward: -7.60\n",
      "Episode 463\tLast reward: -6.00\tAverage reward: -7.52\n",
      "Episode 464\tLast reward: -10.00\tAverage reward: -7.65\n",
      "Episode 465\tLast reward: -4.00\tAverage reward: -7.46\n",
      "Episode 466\tLast reward: -11.00\tAverage reward: -7.64\n",
      "Episode 467\tLast reward: -13.00\tAverage reward: -7.91\n",
      "Episode 468\tLast reward: -1.00\tAverage reward: -7.56\n",
      "Episode 469\tLast reward: -10.00\tAverage reward: -7.68\n",
      "Episode 470\tLast reward: -13.00\tAverage reward: -7.95\n",
      "Episode 471\tLast reward: 3.00\tAverage reward: -7.40\n",
      "Episode 472\tLast reward: -17.00\tAverage reward: -7.88\n",
      "Episode 473\tLast reward: -7.00\tAverage reward: -7.84\n",
      "Episode 474\tLast reward: -4.00\tAverage reward: -7.65\n",
      "Episode 475\tLast reward: -11.00\tAverage reward: -7.81\n",
      "Episode 476\tLast reward: -10.00\tAverage reward: -7.92\n",
      "Episode 477\tLast reward: -7.00\tAverage reward: -7.88\n",
      "Episode 478\tLast reward: -13.00\tAverage reward: -8.13\n",
      "Episode 479\tLast reward: -11.00\tAverage reward: -8.28\n",
      "Episode 480\tLast reward: -13.00\tAverage reward: -8.51\n",
      "Episode 481\tLast reward: -1.00\tAverage reward: -8.14\n",
      "Episode 482\tLast reward: -9.00\tAverage reward: -8.18\n",
      "Episode 483\tLast reward: -3.00\tAverage reward: -7.92\n",
      "Episode 484\tLast reward: -5.00\tAverage reward: -7.78\n",
      "Episode 485\tLast reward: 2.00\tAverage reward: -7.29\n",
      "Episode 486\tLast reward: 1.00\tAverage reward: -6.87\n",
      "Episode 487\tLast reward: -8.00\tAverage reward: -6.93\n",
      "Episode 488\tLast reward: -9.00\tAverage reward: -7.03\n",
      "Episode 489\tLast reward: -8.00\tAverage reward: -7.08\n",
      "Episode 490\tLast reward: 5.00\tAverage reward: -6.48\n",
      "Episode 491\tLast reward: -7.00\tAverage reward: -6.50\n",
      "Episode 492\tLast reward: -13.00\tAverage reward: -6.83\n",
      "Episode 493\tLast reward: -10.00\tAverage reward: -6.99\n",
      "Episode 494\tLast reward: -9.00\tAverage reward: -7.09\n",
      "Episode 495\tLast reward: -12.00\tAverage reward: -7.33\n",
      "Episode 496\tLast reward: -4.00\tAverage reward: -7.17\n",
      "Episode 497\tLast reward: -15.00\tAverage reward: -7.56\n",
      "Episode 498\tLast reward: -2.00\tAverage reward: -7.28\n",
      "Episode 499\tLast reward: -7.00\tAverage reward: -7.27\n",
      "Episode 500\tLast reward: -9.00\tAverage reward: -7.35\n",
      "Episode 501\tLast reward: -9.00\tAverage reward: -7.43\n",
      "Episode 502\tLast reward: -6.00\tAverage reward: -7.36\n",
      "Episode 503\tLast reward: -1.00\tAverage reward: -7.04\n",
      "Episode 504\tLast reward: 1.00\tAverage reward: -6.64\n",
      "Episode 505\tLast reward: -10.00\tAverage reward: -6.81\n",
      "Episode 506\tLast reward: 6.00\tAverage reward: -6.17\n",
      "Episode 507\tLast reward: -12.00\tAverage reward: -6.46\n",
      "Episode 508\tLast reward: -14.00\tAverage reward: -6.84\n",
      "Episode 509\tLast reward: -14.00\tAverage reward: -7.20\n",
      "Episode 510\tLast reward: -11.00\tAverage reward: -7.39\n",
      "Episode 511\tLast reward: 5.00\tAverage reward: -6.77\n",
      "Episode 512\tLast reward: -5.00\tAverage reward: -6.68\n",
      "Episode 513\tLast reward: -12.00\tAverage reward: -6.95\n",
      "Episode 514\tLast reward: -6.00\tAverage reward: -6.90\n",
      "Episode 515\tLast reward: -9.00\tAverage reward: -7.00\n",
      "Episode 516\tLast reward: -5.00\tAverage reward: -6.90\n",
      "Episode 517\tLast reward: -3.00\tAverage reward: -6.71\n",
      "Episode 518\tLast reward: -13.00\tAverage reward: -7.02\n",
      "Episode 519\tLast reward: -9.00\tAverage reward: -7.12\n",
      "Episode 520\tLast reward: -9.00\tAverage reward: -7.22\n",
      "Episode 521\tLast reward: -11.00\tAverage reward: -7.40\n",
      "Episode 522\tLast reward: -1.00\tAverage reward: -7.08\n",
      "Episode 523\tLast reward: -3.00\tAverage reward: -6.88\n",
      "Episode 524\tLast reward: -9.00\tAverage reward: -6.99\n",
      "Episode 525\tLast reward: -6.00\tAverage reward: -6.94\n",
      "Episode 526\tLast reward: -9.00\tAverage reward: -7.04\n",
      "Episode 527\tLast reward: 3.00\tAverage reward: -6.54\n",
      "Episode 528\tLast reward: -11.00\tAverage reward: -6.76\n",
      "Episode 529\tLast reward: -3.00\tAverage reward: -6.57\n",
      "Episode 530\tLast reward: -13.00\tAverage reward: -6.89\n",
      "Episode 531\tLast reward: -11.00\tAverage reward: -7.10\n",
      "Episode 532\tLast reward: -16.00\tAverage reward: -7.54\n",
      "Episode 533\tLast reward: -11.00\tAverage reward: -7.72\n",
      "Episode 534\tLast reward: -3.00\tAverage reward: -7.48\n"
     ]
    }
   ],
   "source": [
    "for e in count(1):\n",
    "    action_log_probs = list()\n",
    "    rewards = list()\n",
    "    values = list()\n",
    "    entropys = list()\n",
    "    state = env.reset()\n",
    "    ##test = net(torch.FloatTensor(state).unsqueeze(0))\n",
    "    ##print(test)\n",
    "    prev_x = None\n",
    "    #print(state.shape)\n",
    "    ##counter = 0\n",
    "    for t in range(100000):\n",
    "    #while True:\n",
    "        #env.render()\n",
    "        \n",
    "        cur_x  = preprocess(state)\n",
    "        #print(\"cur_x = \", cur_x.shape)\n",
    "        state  = cur_x - prev_x if prev_x is not None else preprocess(prev_x)##np.zeros(input_shape)\n",
    "        #print(\"state = \", state.shape)\n",
    "        prev_x = cur_x\n",
    "        \n",
    "        ## so the tensor given to the model should be of shape [batch_size, 1, height, width].\n",
    "        \n",
    "        \n",
    "        ##state = torch.FloatTensor(state)\n",
    "        \n",
    "        state = state.reshape(1, 1, 80, 80)\n",
    "        \n",
    "        #env.render()\n",
    "        ## take an action sampled from a categorical distribution given the state\n",
    "        action_prob = pnet(torch.FloatTensor(state).to(device))\n",
    "        action = action_prob.sample()\n",
    "        action_log_probs.append(action_prob.log_prob(action))\n",
    "        \n",
    "        entropy = ENTROPY_BETA*action_prob.entropy() ## action_prob.probs*action_prob.logits.sum(dim=1).mean()\n",
    "        entropys.append(entropy)\n",
    "        \n",
    "        if action.item() == 0:\n",
    "            action = UP_ACTION\n",
    "        else:\n",
    "            action = DOWN_ACTION\n",
    "        \n",
    "        #print(entropy)\n",
    "        value = cnet(torch.FloatTensor(state).to(device))\n",
    "        values.append(value[0])\n",
    "        #print(action)\n",
    "        next_state, reward, is_done, _ = env.step(action) # take a random action\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        ## current state is next state now\n",
    "        state = next_state\n",
    "\n",
    "        if is_done:\n",
    "            #print(rewards)\n",
    "            #print(values)\n",
    "            break\n",
    "            \n",
    "       \n",
    "            \n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 50 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "                \n",
    "    ## Now we have the discounted reward + log_probs of the actions\n",
    "    returns = discounted_returns(rewards)\n",
    "    #print(returns)\n",
    "    action_losses = list()\n",
    "    critic_losses = list()\n",
    "    ## collect the action losses to a list\n",
    "    for ret, l_prob, v in zip(returns, action_log_probs, values):\n",
    "        advantage = ret - v\n",
    "        #print(advantage)\n",
    "        #print(-l_prob * ret)\n",
    "        action_losses.append(-l_prob * advantage.detach())\n",
    "        critic_losses.append(advantage.pow(2))\n",
    "\n",
    "        \n",
    "        \n",
    "    c_optimizer.zero_grad()\n",
    "    critic_loss = torch.cat(critic_losses).mean() - torch.cat(entropys).sum()\n",
    "    critic_loss.backward(retain_graph=True)\n",
    "    nn_utils.clip_grad_norm_(cnet.parameters(),CLIP_GRAD)\n",
    "    c_optimizer.step()\n",
    "    \n",
    "    \n",
    "    p_optimizer.zero_grad()\n",
    "    ## accumulate the action losses\n",
    "    action_loss = torch.cat(action_losses).sum()\n",
    "    action_loss.backward()\n",
    "    nn_utils.clip_grad_norm_(pnet.parameters(),CLIP_GRAD)\n",
    "    ## step the optimizer\n",
    "    p_optimizer.step()\n",
    "\n",
    "    \n",
    "    '''\n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "    '''\n",
    "      \n",
    "    if running_reward >= MEAN_REWARD_BOUND:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n",
    "    \n",
    "env.close()\n",
    "\n",
    "\n",
    "            \n",
    "##:.2f\n",
    "##'Episode {}\\t Last reward: {}\\t Average reward: {}'.format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for the last part and introducing the next one \n",
    "\n",
    "Changing the value of the entropy coeff and ading the clip part helped the model to converge to -5 in the average rewards in some cases and we have some episodes with last rewards more than 5.\n",
    "\n",
    "Now we will decrese the learning rate and we will increase the value of entropy a bit to increase the exploration and learn a lot and take new experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q5JCfn1BrFWO"
   },
   "outputs": [],
   "source": [
    "# test the environment\n",
    "## env = gym.make('PongNoFrameskip-v4')\n",
    "env = gym.make('Pong-v0')\n",
    "##input_shape = env.observation_space.shape[0]\n",
    "#action_size = env.action_space.n\n",
    "\n",
    "action_size = 2\n",
    "\n",
    "##print(\"Env reward threshold: {}\".format(env.spec.reward_threshold))\n",
    "reward_list = list()\n",
    "\n",
    "input_shape = [1,80,80]\n",
    "\n",
    "##input_shape = [STACK_SIZE, 84, 84]\n",
    "\n",
    "\n",
    "pnet = torch.load('/content/drive/MyDrive/300_A2C_pnet.pt').to(device)##,map_location=torch.device('cpu'))\n",
    "pnet.eval()\n",
    "\n",
    "cnet = torch.load('/content/drive/MyDrive/300_A2C_cnet.pt').to(device)##,map_location=torch.device('cpu'))\n",
    "cnet.eval()\n",
    "\n",
    "## initialize the net\n",
    "#pnet = PolicyNet(input_shape, action_size).to(device)\n",
    "#cnet = CriticNet(input_shape).to(device)\n",
    "\n",
    "\n",
    "## initialize an optimizer\n",
    "p_optimizer = torch.optim.Adam(pnet.parameters(), lr=1e-5,eps=1e-3,amsgrad=True)\n",
    "c_optimizer = torch.optim.Adam(cnet.parameters(), lr=1e-5,eps=1e-3,amsgrad=True)\n",
    "\n",
    "\n",
    "\n",
    "running_reward  = None\n",
    "MEAN_REWARD_BOUND = 20\n",
    "\n",
    "UP_ACTION = 2\n",
    "\n",
    "DOWN_ACTION = 3\n",
    "Max_ep_rewared = -21\n",
    "ENTROPY_BETA = 0.03\n",
    "import torch.nn.utils as nn_utils\n",
    "CLIP_GRAD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoLGnGeNrIbj",
    "outputId": "7d96e15d-939b-40a9-cb9e-b20de1947acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tLast reward: -9.00\tAverage reward: -9.00\n",
      "Episode 2\tLast reward: -6.00\tAverage reward: -8.85\n",
      "Episode 3\tLast reward: -15.00\tAverage reward: -9.16\n",
      "Episode 4\tLast reward: 2.00\tAverage reward: -8.60\n",
      "Episode 5\tLast reward: -6.00\tAverage reward: -8.47\n",
      "Episode 6\tLast reward: -4.00\tAverage reward: -8.25\n",
      "Episode 7\tLast reward: -8.00\tAverage reward: -8.23\n",
      "Episode 8\tLast reward: -13.00\tAverage reward: -8.47\n",
      "Episode 9\tLast reward: -2.00\tAverage reward: -8.15\n",
      "Episode 10\tLast reward: 5.00\tAverage reward: -7.49\n",
      "Episode 11\tLast reward: -9.00\tAverage reward: -7.57\n",
      "Episode 12\tLast reward: -10.00\tAverage reward: -7.69\n",
      "Episode 13\tLast reward: -8.00\tAverage reward: -7.70\n",
      "Episode 14\tLast reward: -9.00\tAverage reward: -7.77\n",
      "Episode 15\tLast reward: 3.00\tAverage reward: -7.23\n",
      "Episode 16\tLast reward: -15.00\tAverage reward: -7.62\n",
      "Episode 17\tLast reward: -15.00\tAverage reward: -7.99\n",
      "Episode 18\tLast reward: 3.00\tAverage reward: -7.44\n",
      "Episode 19\tLast reward: 1.00\tAverage reward: -7.02\n",
      "Episode 20\tLast reward: -7.00\tAverage reward: -7.02\n",
      "Episode 21\tLast reward: 2.00\tAverage reward: -6.56\n",
      "Episode 22\tLast reward: -9.00\tAverage reward: -6.69\n",
      "Episode 23\tLast reward: -15.00\tAverage reward: -7.10\n",
      "Episode 24\tLast reward: -3.00\tAverage reward: -6.90\n",
      "Episode 25\tLast reward: -1.00\tAverage reward: -6.60\n",
      "Episode 26\tLast reward: -15.00\tAverage reward: -7.02\n",
      "Episode 27\tLast reward: -11.00\tAverage reward: -7.22\n",
      "Episode 28\tLast reward: -8.00\tAverage reward: -7.26\n",
      "Episode 29\tLast reward: -7.00\tAverage reward: -7.25\n",
      "Episode 30\tLast reward: -14.00\tAverage reward: -7.58\n",
      "Episode 31\tLast reward: -7.00\tAverage reward: -7.56\n",
      "Episode 32\tLast reward: -6.00\tAverage reward: -7.48\n",
      "Episode 33\tLast reward: -9.00\tAverage reward: -7.55\n",
      "Episode 34\tLast reward: -14.00\tAverage reward: -7.88\n",
      "Episode 35\tLast reward: -8.00\tAverage reward: -7.88\n",
      "Episode 36\tLast reward: -5.00\tAverage reward: -7.74\n",
      "Episode 37\tLast reward: -8.00\tAverage reward: -7.75\n",
      "Episode 38\tLast reward: -5.00\tAverage reward: -7.61\n",
      "Episode 39\tLast reward: -13.00\tAverage reward: -7.88\n",
      "Episode 40\tLast reward: -7.00\tAverage reward: -7.84\n",
      "Episode 41\tLast reward: -13.00\tAverage reward: -8.10\n",
      "Episode 42\tLast reward: -11.00\tAverage reward: -8.24\n",
      "Episode 43\tLast reward: -13.00\tAverage reward: -8.48\n",
      "Episode 44\tLast reward: 1.00\tAverage reward: -8.01\n",
      "Episode 45\tLast reward: -7.00\tAverage reward: -7.96\n",
      "Episode 46\tLast reward: -1.00\tAverage reward: -7.61\n",
      "Episode 47\tLast reward: -4.00\tAverage reward: -7.43\n",
      "Episode 48\tLast reward: 2.00\tAverage reward: -6.96\n",
      "Episode 49\tLast reward: -14.00\tAverage reward: -7.31\n",
      "Episode 50\tLast reward: -13.00\tAverage reward: -7.59\n",
      "Episode 51\tLast reward: -3.00\tAverage reward: -7.36\n",
      "Episode 52\tLast reward: -7.00\tAverage reward: -7.35\n",
      "Episode 53\tLast reward: -15.00\tAverage reward: -7.73\n",
      "Episode 54\tLast reward: -11.00\tAverage reward: -7.89\n",
      "Episode 55\tLast reward: -13.00\tAverage reward: -8.15\n",
      "Episode 56\tLast reward: -5.00\tAverage reward: -7.99\n",
      "Episode 57\tLast reward: -12.00\tAverage reward: -8.19\n",
      "Episode 58\tLast reward: -8.00\tAverage reward: -8.18\n",
      "Episode 59\tLast reward: -5.00\tAverage reward: -8.02\n",
      "Episode 60\tLast reward: -11.00\tAverage reward: -8.17\n",
      "Episode 61\tLast reward: -9.00\tAverage reward: -8.21\n",
      "Episode 62\tLast reward: -3.00\tAverage reward: -7.95\n",
      "Episode 63\tLast reward: -9.00\tAverage reward: -8.00\n",
      "Episode 64\tLast reward: -14.00\tAverage reward: -8.30\n",
      "Episode 65\tLast reward: -10.00\tAverage reward: -8.39\n",
      "Episode 66\tLast reward: -1.00\tAverage reward: -8.02\n",
      "Episode 67\tLast reward: -11.00\tAverage reward: -8.17\n",
      "Episode 68\tLast reward: -12.00\tAverage reward: -8.36\n",
      "Episode 69\tLast reward: -10.00\tAverage reward: -8.44\n",
      "Episode 70\tLast reward: -3.00\tAverage reward: -8.17\n",
      "Episode 71\tLast reward: 2.00\tAverage reward: -7.66\n",
      "Episode 72\tLast reward: -5.00\tAverage reward: -7.53\n",
      "Episode 73\tLast reward: -2.00\tAverage reward: -7.25\n",
      "Episode 74\tLast reward: -5.00\tAverage reward: -7.14\n",
      "Episode 75\tLast reward: 7.00\tAverage reward: -6.43\n",
      "Episode 76\tLast reward: -9.00\tAverage reward: -6.56\n",
      "Episode 77\tLast reward: -1.00\tAverage reward: -6.28\n",
      "Episode 78\tLast reward: -9.00\tAverage reward: -6.42\n",
      "Episode 79\tLast reward: -6.00\tAverage reward: -6.40\n",
      "Episode 80\tLast reward: -6.00\tAverage reward: -6.38\n",
      "Episode 81\tLast reward: -8.00\tAverage reward: -6.46\n",
      "Episode 82\tLast reward: -11.00\tAverage reward: -6.69\n",
      "Episode 83\tLast reward: 1.00\tAverage reward: -6.30\n",
      "Episode 84\tLast reward: -9.00\tAverage reward: -6.44\n",
      "Episode 85\tLast reward: -7.00\tAverage reward: -6.46\n",
      "Episode 86\tLast reward: -7.00\tAverage reward: -6.49\n",
      "Episode 87\tLast reward: -9.00\tAverage reward: -6.62\n",
      "Episode 88\tLast reward: -11.00\tAverage reward: -6.84\n",
      "Episode 89\tLast reward: -6.00\tAverage reward: -6.79\n",
      "Episode 90\tLast reward: -9.00\tAverage reward: -6.90\n",
      "Episode 91\tLast reward: -5.00\tAverage reward: -6.81\n",
      "Episode 92\tLast reward: -9.00\tAverage reward: -6.92\n",
      "Episode 93\tLast reward: -13.00\tAverage reward: -7.22\n",
      "Episode 94\tLast reward: 0.00\tAverage reward: -6.86\n",
      "Episode 95\tLast reward: 8.00\tAverage reward: -6.12\n",
      "Episode 96\tLast reward: -3.00\tAverage reward: -5.96\n",
      "Episode 97\tLast reward: -5.00\tAverage reward: -5.91\n",
      "Episode 98\tLast reward: -11.00\tAverage reward: -6.17\n",
      "Episode 99\tLast reward: -5.00\tAverage reward: -6.11\n",
      "Episode 100\tLast reward: -3.00\tAverage reward: -5.95\n",
      "Episode 101\tLast reward: -11.00\tAverage reward: -6.21\n",
      "Episode 102\tLast reward: -8.00\tAverage reward: -6.30\n",
      "Episode 103\tLast reward: 4.00\tAverage reward: -5.78\n",
      "Episode 104\tLast reward: -5.00\tAverage reward: -5.74\n",
      "Episode 105\tLast reward: -15.00\tAverage reward: -6.21\n"
     ]
    }
   ],
   "source": [
    "for e in count(1):\n",
    "    action_log_probs = list()\n",
    "    rewards = list()\n",
    "    values = list()\n",
    "    entropys = list()\n",
    "    state = env.reset()\n",
    "    ##test = net(torch.FloatTensor(state).unsqueeze(0))\n",
    "    ##print(test)\n",
    "    prev_x = None\n",
    "    #print(state.shape)\n",
    "    ##counter = 0\n",
    "    for t in range(100000):\n",
    "    #while True:\n",
    "        #env.render()\n",
    "        \n",
    "        cur_x  = preprocess(state)\n",
    "        #print(\"cur_x = \", cur_x.shape)\n",
    "        state  = cur_x - prev_x if prev_x is not None else preprocess(prev_x)##np.zeros(input_shape)\n",
    "        #print(\"state = \", state.shape)\n",
    "        prev_x = cur_x\n",
    "        \n",
    "        ## so the tensor given to the model should be of shape [batch_size, 1, height, width].\n",
    "        \n",
    "        \n",
    "        ##state = torch.FloatTensor(state)\n",
    "        \n",
    "        state = state.reshape(1, 1, 80, 80)\n",
    "        \n",
    "        #env.render()\n",
    "        ## take an action sampled from a categorical distribution given the state\n",
    "        action_prob = pnet(torch.FloatTensor(state).to(device))\n",
    "        action = action_prob.sample()\n",
    "        action_log_probs.append(action_prob.log_prob(action))\n",
    "        \n",
    "        entropy = ENTROPY_BETA*action_prob.entropy() ## action_prob.probs*action_prob.logits.sum(dim=1).mean()\n",
    "        entropys.append(entropy)\n",
    "        \n",
    "        if action.item() == 0:\n",
    "            action = UP_ACTION\n",
    "        else:\n",
    "            action = DOWN_ACTION\n",
    "        \n",
    "        #print(entropy)\n",
    "        value = cnet(torch.FloatTensor(state).to(device))\n",
    "        values.append(value[0])\n",
    "        #print(action)\n",
    "        next_state, reward, is_done, _ = env.step(action) # take a random action\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        ## current state is next state now\n",
    "        state = next_state\n",
    "\n",
    "        if is_done:\n",
    "            #print(rewards)\n",
    "            #print(values)\n",
    "            break\n",
    "            \n",
    "       \n",
    "            \n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 50 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "                \n",
    "    ## Now we have the discounted reward + log_probs of the actions\n",
    "    returns = discounted_returns(rewards)\n",
    "    #print(returns)\n",
    "    action_losses = list()\n",
    "    critic_losses = list()\n",
    "    ## collect the action losses to a list\n",
    "    for ret, l_prob, v in zip(returns, action_log_probs, values):\n",
    "        advantage = ret - v\n",
    "        #print(advantage)\n",
    "        #print(-l_prob * ret)\n",
    "        action_losses.append(-l_prob * advantage.detach())\n",
    "        critic_losses.append(advantage.pow(2))\n",
    "\n",
    "        \n",
    "        \n",
    "    c_optimizer.zero_grad()\n",
    "    critic_loss = torch.cat(critic_losses).mean() - torch.cat(entropys).sum()\n",
    "    critic_loss.backward(retain_graph=True)\n",
    "    nn_utils.clip_grad_norm_(cnet.parameters(),CLIP_GRAD)\n",
    "    c_optimizer.step()\n",
    "    \n",
    "    \n",
    "    p_optimizer.zero_grad()\n",
    "    ## accumulate the action losses\n",
    "    action_loss = torch.cat(action_losses).sum()\n",
    "    action_loss.backward()\n",
    "    nn_utils.clip_grad_norm_(pnet.parameters(),CLIP_GRAD)\n",
    "    ## step the optimizer\n",
    "    p_optimizer.step()\n",
    "\n",
    "    \n",
    "    '''\n",
    "    ## get stats\n",
    "    ep_reward = sum(rewards)\n",
    "    \n",
    "    running_reward = ep_reward if running_reward is None else (0.05 * ep_reward) + (0.95 * running_reward)\n",
    "    ##running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(e, ep_reward, running_reward))\n",
    "          \n",
    "        \n",
    "    if e % 100 == 0:\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(e)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "\n",
    "    if ep_reward > Max_ep_rewared:\n",
    "                Max_ep_rewared = ep_reward\n",
    "                ## save the model\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME1)  \n",
    "                torch.save(pnet, SAVE_PATH)\n",
    "                SAVE_PATH = os.path.join(MODEL_PATH, str(Max_ep_rewared)+'_'+MODEL_NAME2)  \n",
    "                torch.save(cnet, SAVE_PATH)\n",
    "                \n",
    "                \n",
    "    '''\n",
    "      \n",
    "    if running_reward >= MEAN_REWARD_BOUND:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break\n",
    "    \n",
    "env.close()\n",
    "\n",
    "\n",
    "            \n",
    "##:.2f\n",
    "##'Episode {}\\t Last reward: {}\\t Average reward: {}'.format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for the last part and introducing the next one \n",
    "\n",
    "The training was very good but it stopped suddenly in colab due to over usage.\n",
    "\n",
    "But we got one last reward with positive +8.00 \n",
    "\n",
    "I continue training in kaggle also for some period using this configration but it didn't improve much better it starts to fluctuate again but I had also last reward with 9 which also a lucky game because it didn't get 9 ever when played it is hard to get 0 also with it\n",
    "\n",
    "I will attach the file of playing which is compatible with my network and my configurations and also atached some trained model like +8 and +9 and +10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This file contained some cells which are copied from other notebooks to make all the results in just one notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
