{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "##import cPickle as pickle\n",
    "from torch.distributions import Categorical\n",
    "from itertools import count\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_shape, action_size):\n",
    "        super(PolicyNet, self).__init__()       \n",
    "       \n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            #nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            #nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            #nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            #nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        #2304\n",
    "        #print(conv_out_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_size)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        #print(conv_out.size())\n",
    "        #print(conv_out.size(0), np.prod(conv_out.size()[1:-1]))\n",
    "        ##fc_in = torch.reshape(conv_out, (conv_out.size(0), np.prod(conv_out.size()[1:])))\n",
    "        nn_out = self.fc(conv_out)\n",
    "        ##return Categorical(torch.sigmoid(nn_out))\n",
    "        \n",
    "        ##return Categorical(torch.softmax(nn_out, dim=1))\n",
    "        return torch.softmax(nn_out, dim=1)\n",
    "    \n",
    "                \n",
    "    \n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(CriticNet, self).__init__()       \n",
    "       \n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            #nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            #nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            #nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            #nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        #2304\n",
    "        #print(conv_out_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        #print(conv_out.size())\n",
    "        #print(conv_out.size(0), np.prod(conv_out.size()[1:-1]))\n",
    "        ##fc_in = torch.reshape(conv_out, (conv_out.size(0), np.prod(conv_out.size()[1:])))\n",
    "        nn_out = self.fc(conv_out)\n",
    "        ##return Categorical(torch.sigmoid(nn_out))\n",
    "        \n",
    "        return nn_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(I):\n",
    "  \"\"\" preprocess 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "\n",
    "  if I is None:\n",
    "     return torch.zeros(80,80)\n",
    "\n",
    "  I             = I[35:195]    # crop\n",
    "  I             = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144]   = 0            # erase background (background type 1)\n",
    "  I[I == 109]   = 0            # erase background (background type 2)\n",
    "  I[I != 0  ]   = 1            # everything else (paddles, ball) just set to 1\n",
    "  ##return torch.from_numpy(I.astype(np.float32).ravel()).unsqueeze(0)\n",
    "  return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "#device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: -5.0\n",
      "Number of actions: Counter({3: 6660, 2: 2874})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "import wrapper\n",
    "import dqn_model\n",
    "\n",
    "### Play the pong game with a trained dqn agent\n",
    "#LOAD_PATH = '17061_pong_policy_net.pt' \n",
    "RENDER = True\n",
    "FPS = 100\n",
    "\n",
    "## for playing first we initialize the env\n",
    "#env = gym.make('PongNoFrameskip-v4')\n",
    "\n",
    "UP_ACTION = 2\n",
    "\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "\n",
    "env = gym.make('Pong-v0')\n",
    "##input_shape = env.observation_space.shape[0]\n",
    "#action_size = env.action_space.n\n",
    "action_size = 2\n",
    "\n",
    "##print(\"Env reward threshold: {}\".format(env.spec.reward_threshold))\n",
    "reward_list = list()\n",
    "\n",
    "##input_shape = 6400\n",
    "## initialize a model\n",
    "#policy_net = PolicyNet(input_shape, action_size).eval().to(device)\n",
    "## load the trained modelinput_shape\n",
    "#print(torch.loPolicyNetd(LOAD_PATH))\n",
    "\n",
    "#policy_net.load_state_dict(torch.load(LOAD_PATH))\n",
    "\n",
    "\n",
    "pnet = torch.load('6.0_A2C_pnet.pt')\n",
    "pnet.eval()\n",
    "\n",
    "cnet = torch.load('6.0_A2C_cnet.pt')\n",
    "cnet.eval()\n",
    "\n",
    "\n",
    "## get the initial state\n",
    "state = env.reset()\n",
    "prev_x = None\n",
    "\n",
    "##action_prob = net(torch.FloatTensor(state).unsqueeze(0).to(device))\n",
    "rewards = []\n",
    "action_count = Counter()\n",
    "## play the game\n",
    "#for i in range(10):\n",
    "while True:\n",
    "    \n",
    "    cur_x  = preprocess(state)\n",
    "    #print(\"cur_x = \", cur_x.shape)\n",
    "    state  = cur_x - prev_x if prev_x is not None else preprocess(prev_x)##np.zeros(input_shape)\n",
    "    #print(\"state = \", state.shape)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    \n",
    "    \n",
    "    ## get start time\n",
    "    start_ts = time.time()\n",
    "    ## if render the game\n",
    "    if RENDER:\n",
    "        env.render()\n",
    "        \n",
    "        \n",
    "    state = state.reshape(1, 1, 80, 80)\n",
    "        \n",
    "    #env.render()\n",
    "    ## take an action sampled from a categorical distribution given the state\n",
    "    action = pnet(torch.FloatTensor(state).to(device)).max(1)[1].item()\n",
    "    \n",
    "    if action == 0:\n",
    "            action = UP_ACTION\n",
    "    else:\n",
    "            action = DOWN_ACTION\n",
    "    ##action_prob = pnet(torch.FloatTensor(state).to(device))\n",
    "    ##action = action_prob.sample()\n",
    "\n",
    "    #print(entropy)\n",
    "    #value = cnet(torch.FloatTensor(state).to(device))\n",
    "    #values.append(value[0])\n",
    "    #print(action)\n",
    "    next_state, reward, is_done, _ = env.step(action) # take a random action\n",
    "    rewards.append(reward)\n",
    "\n",
    "    ## current state is next state now\n",
    "    state = next_state        \n",
    "    \n",
    "    ##state = torch.FloatTensor(new_state).unsqueeze(0)\n",
    "    action_count[action] += 1\n",
    "\n",
    "    if is_done:\n",
    "        break\n",
    "\n",
    "    ep_reward = sum(rewards)    \n",
    "    if RENDER:\n",
    "        delta = 1/FPS - (time.time() - start_ts)\n",
    "        if delta > 0:\n",
    "            time.sleep(delta)\n",
    "\n",
    "print(\"Total Reward: {}\".format(ep_reward))\n",
    "print(\"Number of actions: {}\".format(action_count))\n",
    "\n",
    "##env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
